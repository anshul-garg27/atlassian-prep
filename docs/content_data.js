window.CONTENT_DATA = [
  {
    "type": "file",
    "name": "01_Karat_Screening_Round.md",
    "content": "# \ud83c\udfaf KARAT SCREENING ROUND - Complete Guide\n\n**Duration:** 60 minutes\n**Format:** 20 min System Design Rapid Fire + 40 min DSA Coding\n**Difficulty:** Medium\n**Can Retry:** Yes (One free retry if you fail)\n\n---\n\n## \ud83d\udccb Round Structure\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PART 1: SYSTEM DESIGN RAPID FIRE (20 minutes)  \u2502\n\u2502 \u251c\u2500 5 scenario-based questions                  \u2502\n\u2502 \u251c\u2500 ~4 minutes per question                     \u2502\n\u2502 \u2514\u2500 Focus: Quick thinking & trade-offs          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 PART 2: DSA CODING (40 minutes)                \u2502\n\u2502 \u251c\u2500 1-2 Medium level problems                   \u2502\n\u2502 \u251c\u2500 Must pass all test cases                    \u2502\n\u2502 \u2514\u2500 Follow-up questions expected                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udd25 PART 1: SYSTEM DESIGN RAPID FIRE QUESTIONS\n\n### Question Bank (Most Frequently Asked)\n\n---\n\n### \u2b50\u2b50\u2b50 **Q1: Music Streaming with Consistent Hashing**\n\n**Problem Statement:**\n> You're working on a music streaming and uploading service. The system uses consistent hashing to distribute load across servers. Load is equally distributed based on the number of files on each server. Do you see any concerns with this architecture?\n\n**Expected Discussion Points:**\n\n1. **Hot Files Problem**\n   - Popular songs get more requests than others\n   - Equal file distribution \u2260 Equal load distribution\n   - Some files might be accessed 1000x more than others\n\n2. **Storage vs Load Mismatch**\n   - Large files (high quality) vs small files (low quality)\n   - File count doesn't reflect actual load\n\n3. **Read vs Write Pattern**\n   - Most files are read-heavy (streaming)\n   - New uploads might cause rebalancing\n\n**Optimal Answer:**\n```\nConcerns:\n1. Hot content - Popular songs create hotspots on certain servers\n2. File size variation - Equal file count doesn't mean equal load\n3. Read/Write imbalance - Streaming is read-heavy\n\nImprovements:\n- Use request count or bandwidth for distribution, not file count\n- Implement caching layer (CDN) for hot content\n- Replicate popular content across multiple servers\n- Monitor per-server load and dynamically rebalance\n```\n\n**Follow-up:** How would you improve this system?\n\n---\n\n### \u2b50\u2b50\u2b50 **Q2: Crossword Puzzle Game - Hints Strategy**\n\n**Problem Statement:**\n> You're building a crossword puzzle gaming application that provides hints to users. What are the advantages and disadvantages of these two approaches:\n> 1. Fetching hints from server on-demand\n> 2. Preloading all hints on the device when game starts\n\n**Expected Analysis:**\n\n| Aspect | Server Fetch | Preload |\n|--------|-------------|---------|\n| **Network Usage** | \ud83d\udfe2 Low (only when needed) | \ud83d\udd34 High (all hints upfront) |\n| **Latency** | \ud83d\udd34 Network delay per hint | \ud83d\udfe2 Instant access |\n| **Storage** | \ud83d\udfe2 Minimal device storage | \ud83d\udd34 More device storage |\n| **Cheating** | \ud83d\udfe2 Can't see all hints | \ud83d\udd34 Easy to extract all hints |\n| **Offline Mode** | \ud83d\udd34 Requires internet | \ud83d\udfe2 Works offline |\n| **Updates** | \ud83d\udfe2 Easy to update hints | \ud83d\udd34 Need app update |\n| **Cost** | \ud83d\udd34 Server API costs | \ud83d\udfe2 One-time download |\n\n**Optimal Answer:**\n```\nServer Fetch Pros:\n- Low network usage (pay-as-you-go)\n- Better security (can't cheat easily)\n- Easy to update hints server-side\n- Analytics on which hints are used\n\nServer Fetch Cons:\n- Requires active internet connection\n- Latency for each hint request\n- Server costs for API calls\n\nPreload Pros:\n- Works offline\n- Instant hint access (better UX)\n- Fewer server API calls\n\nPreload Cons:\n- Large initial download\n- More device storage needed\n- Security issue - users can extract all hints\n- Hard to update hints\n\nBest Approach: Hybrid\n- Preload first 3 hints for each puzzle\n- Fetch additional hints on-demand\n- Cache fetched hints locally\n```\n\n---\n\n### \u2b50\u2b50 **Q3: Large XML File Processing**\n\n**Problem Statement:**\n> Your service needs to process a very large XML file. The default hardware doesn't have enough RAM to hold the entire file in memory. Give some approaches to optimize this.\n\n**Expected Solutions:**\n\n**Approach 1: Streaming Parser (SAX/StAX)**\n```python\n# Don't load entire file into memory\n# Process element by element\n\nimport xml.sax\n\nclass XMLHandler(xml.sax.ContentHandler):\n    def startElement(self, name, attrs):\n        # Process element\n        pass\n\n    def characters(self, content):\n        # Process content\n        pass\n\n# Reads file in chunks, never loads full file\nparser = xml.sax.make_parser()\nparser.setContentHandler(XMLHandler())\nparser.parse(\"large_file.xml\")\n```\n\n**Approach 2: Chunking with Parallel Processing**\n```\n1. Split XML into logical chunks (by top-level elements)\n2. Process each chunk separately\n3. Use distributed processing (MapReduce)\n4. Aggregate results at the end\n```\n\n**Approach 3: Database-Backed Processing**\n```\n1. Stream XML and store in database (insert as you read)\n2. Process data in database (SQL queries)\n3. Avoids keeping everything in memory\n```\n\n**Optimal Answer:**\n```\nSolutions:\n1. Use streaming XML parser (SAX, not DOM)\n   - Reads file sequentially\n   - Process element-by-element\n   - Memory usage: O(1) per element\n\n2. Split file into chunks\n   - Logical splitting at element boundaries\n   - Parallel processing with MapReduce\n   - Memory usage: O(chunk_size)\n\n3. External memory algorithm\n   - Stream to database/disk as you parse\n   - Query from disk instead of RAM\n   - Trade memory for I/O time\n\n4. Upgrade hardware (if budget allows)\n   - Increase RAM\n   - Use specialized parsing machines\n\nBest: Streaming parser with database backing\n```\n\n---\n\n### \u2b50\u2b50\u2b50 **Q4: Smart URL Engine - Budget Planning**\n\n**Problem Statement:**\n> You're building a smart engine service that takes URLs from users and processes them to extract useful data. You need to plan the budget for this project. What things will you take into consideration?\n\n**Expected Discussion:**\n\n**Capacity Estimation Parameters to Ask:**\n\n1. **Traffic Metrics**\n   - Expected number of users?\n   - URLs processed per day/month?\n   - Peak vs average traffic ratio?\n\n2. **Processing Metrics**\n   - Average URL processing time?\n   - Size of typical webpage?\n   - How much data extracted per URL?\n\n3. **Storage Requirements**\n   - Store original HTML? Just extracted data?\n   - Retention period for data?\n   - Growth rate?\n\n4. **Geographic Distribution**\n   - Single region or global?\n   - Latency requirements?\n\n**Budget Components:**\n\n```\n1. Compute Costs\n   - Server instances for processing\n   - Scaling requirements (auto-scaling)\n   - CPU/Memory requirements per URL\n\n2. Storage Costs\n   - Database (RDS, DynamoDB)\n   - Object storage (S3) for raw HTML\n   - Backup and archival\n\n3. Network Costs\n   - Bandwidth for fetching URLs\n   - Data transfer between services\n   - CDN if caching results\n\n4. Third-party Costs\n   - ML model API calls (if using external)\n   - Proxy services (to avoid IP blocking)\n   - Monitoring and logging tools\n\n5. Development & Maintenance\n   - Engineering hours\n   - DevOps and monitoring\n   - On-call support\n```\n\n**Sample Calculation:**\n```\nAssumptions:\n- 1M URLs/day\n- Average processing: 5 seconds/URL\n- Data extracted: 10KB/URL\n\nCompute:\n- Need: (1M URLs * 5 sec) / (24 * 3600) = ~58 parallel workers\n- Cost: 60 EC2 instances * $0.1/hour * 720 hours = $4,320/month\n\nStorage:\n- 1M * 10KB * 30 days = 300GB/month\n- Cost: 300GB * $0.023/GB = $7/month\n\nNetwork:\n- Fetching 1M pages * 500KB avg = 500GB/day\n- Cost: 15TB/month * $0.09/GB = $1,350/month\n\nTotal: ~$5,700/month\n```\n\n---\n\n### \u2b50\u2b50\u2b50 **Q5: Social Media App - Scaling Internationally**\n\n**Problem Statement:**\n> You have a social media app for college students that's successfully running in the US. How would you scale it to release worldwide?\n\n**Expected Discussion:**\n\n**Technical Challenges:**\n\n1. **Latency & Regional Distribution**\n```\nChallenge: Users in Asia accessing US servers = High latency\n\nSolution:\n- Deploy to multiple AWS/GCP regions\n- Route users to nearest region (GeoDNS)\n- CDN for static content (images, videos)\n- Edge caching for frequently accessed data\n```\n\n2. **Data Residency & Compliance**\n```\nChallenge: GDPR (Europe), data localization laws\n\nSolution:\n- Store EU user data in EU region\n- Implement data export/deletion APIs\n- Privacy-compliant analytics\n- Per-region encryption keys\n```\n\n3. **Database Strategy**\n```\nChallenge: Global data consistency vs availability\n\nOptions:\nA. Multi-region database with replication\n   - Write to primary, replicate globally\n   - Eventual consistency for reads\n\nB. Sharding by geography\n   - US users \u2192 US database\n   - EU users \u2192 EU database\n   - Cross-region queries when needed\n\nC. Hybrid approach\n   - User data sharded by region\n   - Global data (trending posts) replicated everywhere\n```\n\n4. **Content Moderation & Localization**\n```\n- Multiple languages (i18n)\n- Cultural sensitivity (content guidelines vary)\n- Local regulations (censorship in some countries)\n- Time zones for notifications\n```\n\n5. **Payment & Currency**\n```\n- Multiple payment gateways\n- Currency conversion\n- Tax compliance per country\n```\n\n**Optimal Answer:**\n```\nScaling Strategy:\n\n1. Infrastructure:\n   - Deploy to 3-5 major regions (US-East, EU-West, Asia-Pacific)\n   - Use CDN for static assets\n   - GeoDNS for intelligent routing\n\n2. Data Strategy:\n   - Shard user data by region\n   - Replicate global content (trending) with eventual consistency\n   - Local caching for frequently accessed data\n\n3. Compliance:\n   - GDPR compliance for Europe\n   - Data residency laws for China, Russia\n   - Privacy policies per region\n\n4. Application:\n   - Internationalization (i18n) for 10+ languages\n   - Localized content moderation policies\n   - Regional payment gateways\n\n5. Monitoring:\n   - Per-region performance metrics\n   - Multi-region alerting\n   - Cost optimization per region\n\nRollout:\nPhase 1: Canada, UK, Australia (similar regulations)\nPhase 2: Europe (GDPR compliance)\nPhase 3: Asia-Pacific\nPhase 4: Rest of world\n```\n\n---\n\n## \ud83d\udcbb PART 2: DSA CODING QUESTIONS\n\n---\n\n### \u2b50\u2b50\u2b50 **DSA Q1: Text Justification / Word Wrap**\n\n**Problem:** [LeetCode 68 - Text Justification](https://leetcode.com/problems/text-justification/)\n\n**Atlassian Variation:**\n> Given a list of words and an integer `maxLen`, wrap the words into lines separated by '-'. If line length exceeds `maxLen`, start a new line.\n\n**Example 1:**\n```python\nwords = [\"Hello\", \"Sir\", \"Please\", \"Upvote\", \"If\", \"You\", \"Like\", \"My\", \"Post\"]\nmaxLen = 10\n\nOutput = [\"Hello-Sir\", \"Please\", \"Upvote-If\", \"You-Like\", \"My-Post\"]\n\nExplanation:\n\"Hello-Sir\" = 5 + 1 + 3 = 9 \u2264 10 \u2713\n\"Please\" = 6 \u2264 10 \u2713\n\"Upvote-If\" = 6 + 1 + 2 = 9 \u2264 10 \u2713\n```\n\n**Solution:**\n```python\ndef word_wrap(words, maxLen):\n    result = []\n    current_line = []\n    current_length = 0\n\n    for word in words:\n        word_len = len(word)\n\n        # Check if adding this word exceeds maxLen\n        # Need to account for dashes between words\n        needed_length = current_length + word_len\n        if current_line:\n            needed_length += 1  # for the dash\n\n        if needed_length <= maxLen:\n            current_line.append(word)\n            current_length = needed_length\n        else:\n            # Start new line\n            result.append('-'.join(current_line))\n            current_line = [word]\n            current_length = word_len\n\n    # Add last line\n    if current_line:\n        result.append('-'.join(current_line))\n\n    return result\n\n# Time: O(n) where n = number of words\n# Space: O(n) for output\n```\n\n**Follow-up:** Justified text with exact length\n\n**Problem:**\n> Given sentences and `exactLen`, create lines of exactly `exactLen` by distributing extra spaces evenly. Last line doesn't need padding.\n\n**Example:**\n```python\nsentences = [\n    \"The day began as still as the\",\n    \"night abruptly lighted with\",\n    \"brilliant flame\"\n]\nexactLen = 24\n\nOutput = [\n    \"The--day--began-as-still\",  # 24 chars\n    \"as--the--night--abruptly\",  # 24 chars\n    \"lighted--with--brilliant\",  # 24 chars\n    \"flame\"                       # No padding (last line)\n]\n```\n\n**Solution:**\n```python\ndef justify_text(sentences, exactLen):\n    # First, extract all words\n    words = []\n    for sentence in sentences:\n        words.extend(sentence.split())\n\n    result = []\n    current_line_words = []\n    current_length = 0\n\n    for word in words:\n        needed = current_length + len(word)\n        if current_line_words:\n            needed += 1  # space/dash\n\n        if needed <= exactLen:\n            current_line_words.append(word)\n            current_length = needed\n        else:\n            # Justify current line\n            line = justify_line(current_line_words, exactLen)\n            result.append(line)\n\n            current_line_words = [word]\n            current_length = len(word)\n\n    # Last line - no justification\n    if current_line_words:\n        result.append('-'.join(current_line_words))\n\n    return result\n\ndef justify_line(words, exactLen):\n    if len(words) == 1:\n        # Single word - no padding\n        return words[0]\n\n    # Calculate total word length\n    total_word_len = sum(len(w) for w in words)\n    total_spaces = exactLen - total_word_len\n    gaps = len(words) - 1\n\n    # Distribute spaces evenly\n    spaces_per_gap = total_spaces // gaps\n    extra_spaces = total_spaces % gaps\n\n    result = []\n    for i, word in enumerate(words):\n        result.append(word)\n        if i < len(words) - 1:  # Not last word\n            # Add spaces\n            result.append('-' * spaces_per_gap)\n            if i < extra_spaces:\n                result.append('-')\n\n    return ''.join(result)\n\n# Time: O(n) where n = total words\n# Space: O(n)\n```\n\n**Test Cases:**\n```python\n# Test 1\nassert word_wrap([\"Hello\", \"World\"], 10) == [\"Hello\", \"World\"]\n\n# Test 2\nassert word_wrap([\"a\", \"b\", \"c\"], 3) == [\"a-b\", \"c\"]\n\n# Test 3\nassert word_wrap([\"ThisIsALongWord\"], 5) == [\"ThisIsALongWord\"]  # Exceeds maxLen\n\n# Edge cases to discuss:\n# - What if single word > maxLen?\n# - Empty input?\n# - maxLen = 0?\n```\n\n---\n\n### \u2b50\u2b50 **DSA Q2: Find Words That Can Be Formed**\n\n**Problem:** [LeetCode 1160](https://leetcode.com/problems/find-words-that-can-be-formed-by-characters/)\n\n**Atlassian Variation:**\n> Given a dictionary of words and a word with letters jumbled, check if any word in the dictionary can be formed from the jumbled letters.\n\n**Example:**\n```python\nwords = [\"cat\", \"dada\", \"dog\", \"baby\"]\njumbled = \"ctay\"\n\nOutput: \"cat\"  # Can form \"cat\" from \"ctay\"\n\njumbled = \"dad\"\nOutput: -1  # Cannot form any word\n```\n\n**Solution:**\n```python\nfrom collections import Counter\n\ndef find_formable_word(words, jumbled):\n    jumbled_count = Counter(jumbled)\n\n    for word in words:\n        word_count = Counter(word)\n\n        # Check if all characters in word are available\n        if all(word_count[ch] <= jumbled_count[ch] for ch in word_count):\n            return word\n\n    return -1\n\n# Time: O(n * m) where n = len(words), m = avg word length\n# Space: O(k) where k = alphabet size (26)\n\n# Better approach using Counter subtraction\ndef find_formable_word_v2(words, jumbled):\n    jumbled_count = Counter(jumbled)\n\n    for word in words:\n        word_count = Counter(word)\n\n        # Try subtracting - if any negative, not possible\n        remaining = jumbled_count.copy()\n        remaining.subtract(word_count)\n\n        if all(count >= 0 for count in remaining.values()):\n            return word\n\n    return -1\n```\n\n**Follow-up:** Return ALL formable words, not just first one\n\n```python\ndef find_all_formable_words(words, jumbled):\n    jumbled_count = Counter(jumbled)\n    result = []\n\n    for word in words:\n        word_count = Counter(word)\n        if all(word_count[ch] <= jumbled_count[ch] for ch in word_count):\n            result.append(word)\n\n    return result\n```\n\n---\n\n### \u2b50\u2b50 **DSA Q3: Badge In/Out Violations**\n\n**Problem:**\n> You have two lists:\n> - `entry`: timestamp-sorted list of names who badged IN\n> - `exit`: timestamp-sorted list of names who badged OUT\n>\n> Find people who forgot to badge in OR forgot to badge out.\n\n**Example:**\n```python\nentry = [\"Alice\", \"Bob\", \"Alice\", \"Charlie\"]\nexit = [\"Alice\", \"Alice\", \"Bob\"]\n\nOutput: {\n    \"forgot_badge_in\": [\"Alice\"],   # Exited but never entered first time\n    \"forgot_badge_out\": [\"Charlie\"]  # Entered but never exited\n}\n```\n\n**Solution:**\n```python\nfrom collections import defaultdict\n\ndef find_badge_violations(entry, exit):\n    # Track state: 0 = outside, 1 = inside\n    person_state = defaultdict(int)  # 0 by default\n\n    forgot_in = set()\n    forgot_out = set()\n\n    entry_idx = 0\n    exit_idx = 0\n\n    # Process in chronological order\n    # Since both are timestamp sorted, we need to merge\n\n    # Simplified: Process all entries, then exits\n    for person in entry:\n        if person_state[person] == 1:\n            # Already inside - forgot to badge out last time\n            forgot_out.add(person)\n        person_state[person] = 1  # Now inside\n\n    for person in exit:\n        if person_state[person] == 0:\n            # Outside, but exiting - forgot to badge in\n            forgot_in.add(person)\n        person_state[person] = 0  # Now outside\n\n    # After all events, anyone still inside forgot to badge out\n    for person, state in person_state.items():\n        if state == 1:\n            forgot_out.add(person)\n\n    return {\n        \"forgot_badge_in\": list(forgot_in),\n        \"forgot_badge_out\": list(forgot_out)\n    }\n\n# Time: O(n + m) where n = len(entry), m = len(exit)\n# Space: O(unique people)\n```\n\n**Better Solution with Timestamps:**\n```python\ndef find_violations_with_time(entries, exits):\n    # entries = [(timestamp, name), ...]\n    # exits = [(timestamp, name), ...]\n\n    # Merge both lists and sort by timestamp\n    events = []\n    for ts, name in entries:\n        events.append((ts, name, 'entry'))\n    for ts, name in exits:\n        events.append((ts, name, 'exit'))\n\n    events.sort()  # Sort by timestamp\n\n    person_state = {}\n    forgot_in = set()\n    forgot_out = set()\n\n    for ts, name, event_type in events:\n        if event_type == 'entry':\n            if name in person_state and person_state[name] == 'inside':\n                forgot_out.add(name)\n            person_state[name] = 'inside'\n        else:  # exit\n            if name not in person_state or person_state[name] == 'outside':\n                forgot_in.add(name)\n            person_state[name] = 'outside'\n\n    # Check final states\n    for name, state in person_state.items():\n        if state == 'inside':\n            forgot_out.add(name)\n\n    return list(forgot_in), list(forgot_out)\n```\n\n---\n\n### \u2b50\u2b50 **DSA Q4: Robot Parts Assembly**\n\n**Problem:**\n> Given available parts and robot requirements, return which robots can be fully built.\n\n**Example:**\n```python\nparts = [\n    \"Rosie_claw\", \"Rosie_sensors\", \"Rosie_case\", \"Rosie_wheels\",\n    \"Dustie_case\", \"Dustie_case\", \"Dustie_case\", \"Dustie_arms\",\n    \"Dustie_speaker\",\n    \"Optimus_sensors\", \"Optimus_speaker\", \"Optimus_case\",\n    \"Optimus_wheels\", \"Optimus_wheels\",\n    \"Rust_sensors\", \"Rust_case\", \"Rust_claw\", \"Rust_legs\"\n]\n\nrequirements = {\n    \"Rosie\": [\"claw\", \"sensors\", \"case\", \"wheels\"],\n    \"Dustie\": [\"case\", \"arms\", \"speaker\"],\n    \"Optimus\": [\"sensors\", \"speaker\", \"case\", \"wheels\"],\n    \"Rust\": [\"sensors\", \"case\", \"claw\", \"legs\"]\n}\n\nOutput: [\"Rosie\", \"Dustie\", \"Optimus\", \"Rust\"]\n```\n\n**Solution:**\n```python\nfrom collections import Counter\n\ndef find_buildable_robots(parts, requirements):\n    # Count available parts per robot\n    available = {}\n    for part in parts:\n        robot_name, part_name = part.split('_')\n        if robot_name not in available:\n            available[robot_name] = Counter()\n        available[robot_name][part_name] += 1\n\n    buildable = []\n    for robot, needed_parts in requirements.items():\n        if robot not in available:\n            continue\n\n        # Check if all required parts are available\n        needed_count = Counter(needed_parts)\n        can_build = True\n\n        for part, count in needed_count.items():\n            if available[robot][part] < count:\n                can_build = False\n                break\n\n        if can_build:\n            buildable.append(robot)\n\n    return buildable\n\n# Time: O(p + r*k) where p=parts, r=robots, k=parts per robot\n# Space: O(p + r)\n```\n\n---\n\n### \u2b50 **DSA Q5: Delivery Cart Routes (Graph)**\n\n**Problem:**\n> Given directed paths that carts take, identify all start locations and their possible end locations.\n\n**Example:**\n```python\npaths = [\n    [\"A\", \"B\"], [\"A\", \"C\"],\n    [\"B\", \"K\"], [\"C\", \"K\"], [\"C\", \"G\"],\n    [\"E\", \"F\"], [\"E\", \"L\"],\n    [\"F\", \"G\"],\n    [\"J\", \"M\"],\n    [\"G\", \"H\"], [\"G\", \"I\"]\n]\n\n\"\"\"\nGraph:\n   A          E      J\n  / \\        / \\      \\\n B   C      F   L      M\n  \\ / \\    /\n   K   G\n      / \\\n     H   I\n\"\"\"\n\nOutput: {\n    \"A\": [\"K\", \"H\", \"I\"],\n    \"E\": [\"H\", \"L\", \"I\"],\n    \"J\": [\"M\"]\n}\n```\n\n**Solution:**\n```python\nfrom collections import defaultdict, deque\n\ndef find_all_destinations(paths):\n    # Build adjacency list\n    graph = defaultdict(list)\n    all_nodes = set()\n    has_incoming = set()\n\n    for src, dest in paths:\n        graph[src].append(dest)\n        all_nodes.add(src)\n        all_nodes.add(dest)\n        has_incoming.add(dest)\n\n    # Find start nodes (no incoming edges)\n    start_nodes = all_nodes - has_incoming\n\n    result = {}\n\n    for start in start_nodes:\n        # BFS to find all reachable destinations\n        destinations = set()\n        queue = deque([start])\n        visited = {start}\n\n        while queue:\n            node = queue.popleft()\n\n            # If no outgoing edges, it's a destination\n            if node not in graph:\n                destinations.add(node)\n            else:\n                for neighbor in graph[node]:\n                    if neighbor not in visited:\n                        visited.add(neighbor)\n                        queue.append(neighbor)\n\n        result[start] = sorted(destinations)\n\n    return result\n\n# Time: O(V + E) for BFS from each start node\n# Space: O(V + E) for graph storage\n```\n\n---\n\n## \ud83c\udfaf Key Takeaways for Karat Round\n\n### \u2705 Success Tips\n\n1. **System Design Rapid Fire**\n   - Ask clarifying questions (even if time-limited)\n   - Think about trade-offs (not just one answer)\n   - Consider scale, cost, and latency\n   - Use real-world examples\n\n2. **DSA Coding**\n   - MUST pass all test cases\n   - Clean, readable code\n   - Handle edge cases\n   - Explain time/space complexity\n   - Be ready for follow-ups\n\n3. **Time Management**\n   - Don't spend > 5 min per SD question\n   - If stuck on DSA, ask for hints\n   - Test your code thoroughly\n\n### \u274c Common Mistakes\n\n1. **System Design**\n   - \u274c Not asking clarifying questions\n   - \u274c Giving only one solution without alternatives\n   - \u274c Ignoring scale/cost considerations\n\n2. **Coding**\n   - \u274c Not testing code before submitting\n   - \u274c Missing edge cases (empty input, single element)\n   - \u274c Poor variable naming\n   - \u274c Not explaining approach first\n\n### \ud83c\udf93 Preparation Strategy\n\n**Week 1-2: System Design**\n- [ ] Read \"Designing Data-Intensive Applications\"\n- [ ] Practice explaining trade-offs verbally\n- [ ] Study common patterns: caching, sharding, replication\n\n**Week 1-2: DSA**\n- [ ] Master these patterns:\n  - Two pointers\n  - HashMap/Counter\n  - Greedy algorithms\n  - Basic graph traversal (BFS)\n- [ ] Practice 20 medium LeetCode problems\n- [ ] Focus on string manipulation\n\n**Mock Practice:**\n- [ ] 5 rapid-fire system design questions (20 min total)\n- [ ] 2 DSA problems (40 min total)\n- [ ] Simulate real pressure\n\n---\n\n## \ud83d\udcda Additional Practice Problems\n\n### System Design Rapid Fire\n\n1. Design URL shortener - what are the scaling concerns?\n2. Video streaming service - caching strategy?\n3. Ride-sharing app - driver matching algorithm considerations?\n4. E-commerce - inventory management at scale?\n5. Chat application - message delivery guarantees?\n\n### DSA Problems (Similar Difficulty)\n\n1. [LeetCode 49 - Group Anagrams](https://leetcode.com/problems/group-anagrams/)\n2. [LeetCode 56 - Merge Intervals](https://leetcode.com/problems/merge-intervals/)\n3. [LeetCode 271 - Encode and Decode Strings](https://leetcode.com/problems/encode-and-decode-strings/)\n4. [LeetCode 347 - Top K Frequent Elements](https://leetcode.com/problems/top-k-frequent-elements/)\n\n---\n\n**Next:** [02_Data_Structures_Round.md](./02_Data_Structures_Round.md) - Deep dive into pure DSA round\n\n**Back to:** [README.md](./README.md)\n"
  },
  {
    "type": "file",
    "name": "02_Data_Structures_Round.md",
    "content": "# \ud83e\udde0 DATA STRUCTURES / ALGO ROUND - Complete Guide\n\n**Duration:** 60 minutes\n**Format:** 1-2 DSA problems with multiple follow-ups\n**Difficulty:** Medium to Hard\n**Pass Rate:** ~60% (hardest technical round)\n\n---\n\n## \ud83d\udccb Round Structure\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Main Problem (30-35 minutes)                     \u2502\n\u2502 \u251c\u2500 Problem statement + clarifications            \u2502\n\u2502 \u251c\u2500 Approach discussion                           \u2502\n\u2502 \u251c\u2500 Code implementation                           \u2502\n\u2502 \u2514\u2500 Test cases + complexity analysis              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Follow-ups (20-25 minutes)                       \u2502\n\u2502 \u251c\u2500 Extension 1: Add constraint                   \u2502\n\u2502 \u251c\u2500 Extension 2: Optimize further                 \u2502\n\u2502 \u2514\u2500 Extension 3: Handle edge cases / concurrency  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udcda Problem Collection\n\nThe questions have been organized into individual files for better readability.\n\n| # | Problem Name | Frequency | Key Concept | Link |\n|---|--------------|-----------|-------------|------|\n| 1 | **Employee Hierarchy** | \u2b50\u2b50\u2b50\u2b50\u2b50 (60%) | LCA, N-ary Tree | [View Problem](./Data_Structures/01_Employee_Hierarchy.md) |\n| 2 | **Stock Price Fluctuation** | \u2b50\u2b50\u2b50\u2b50 | SortedList, Heap, Map | [View Problem](./Data_Structures/02_Stock_Price_Fluctuation.md) |\n| 3 | **Content Popularity** | \u2b50\u2b50\u2b50\u2b50 (40%) | Doubly Linked List + Map | [View Problem](./Data_Structures/03_Content_Popularity.md) |\n| 4 | **Tennis Court Booking** | \u2b50\u2b50\u2b50 (30%) | Greedy, Heap, Intervals | [View Problem](./Data_Structures/04_Tennis_Court_Booking.md) |\n| 5 | **Router / Wildcards** | \u2b50\u2b50\u2b50 (25%) | Trie | [View Problem](./Data_Structures/05_Router_Wildcards.md) |\n| 6 | **Commodity Prices** | \u2b50\u2b50 | SortedMap, Segment Tree | [View Problem](./Data_Structures/06_Commodity_Prices.md) |\n| 7 | **File Collections** | \u2b50\u2b50 | Heap, HashMap | [View Problem](./Data_Structures/07_File_Collections.md) |\n| 8 | **Robot Parts** | \u2b50\u2b50 | Set, HashMap | [View Problem](./Data_Structures/08_Robot_Parts.md) |\n| 9 | **Vote Counting** | \u2b50\u2b50 | Sorting, Comparator | [View Problem](./Data_Structures/09_Vote_Counting.md) |\n| 10 | **Word Wrap** | \u2b50\u2b50\u2b50 | Greedy, Strings | [View Problem](./Data_Structures/10_Word_Wrap.md) |\n| 11 | **OA Problems** | \u2b50 | Math, DP | [View Problem](./Data_Structures/11_OA_Problems.md) |\n\n---\n\n## \ud83d\udcca SUMMARY & KEY TAKEAWAYS\n\n### \ud83c\udfaf Most Important Problems (Must Practice)\n\n1. **Employee Hierarchy (60% frequency)** \u2b50\u2b50\u2b50\u2b50\u2b50\n   - Master LCA algorithm\n   - Practice all follow-ups\n   - Know thread-safe implementation\n\n2. **Content Popularity (40% frequency)** \u2b50\u2b50\u2b50\u2b50\n   - Learn Doubly Linked List + HashMap pattern\n   - All O(1) operations\n   - Similar to LRU Cache design\n\n3. **Tennis Court Booking (30% frequency)** \u2b50\u2b50\u2b50\n   - Meeting Rooms II pattern\n   - Min-heap for greedy assignment\n\n### \u2705 Success Checklist\n\n**Before the Interview:**\n- [ ] Solve Employee Hierarchy 5+ times\n- [ ] Implement Content Popularity from scratch 3 times\n- [ ] Practice explaining time/space complexity\n- [ ] Review all follow-up variations\n- [ ] Review Robot Parts and File Collection problems\n\n**During the Interview:**\n- [ ] Ask clarifying questions\n- [ ] Discuss approach before coding\n- [ ] Write clean, modular code\n- [ ] Test with examples\n- [ ] Analyze complexity\n- [ ] Handle edge cases\n\n### \u274c Common Mistakes to Avoid\n\n1. **Not asking clarifying questions**\n   - \"Can employees be in multiple groups?\"\n   - \"Is the input sorted?\"\n   - \"What should I return if no solution?\"\n\n2. **Jumping to code too quickly**\n   - Discuss approach first\n   - Confirm with interviewer\n   - Then code\n\n3. **Ignoring edge cases**\n   - Employee doesn't exist\n   - Empty group\n   - Circular dependencies\n\n4. **Poor time complexity analysis**\n   - Be precise: O(n log n), not just \"O(n something)\"\n   - Explain which operations dominate\n\n5. **Not testing code**\n   - Walk through at least 2-3 examples\n   - Include edge case\n\n---\n\n**Next:** [03_Code_Design_LLD_Round.md](./03_Code_Design_LLD_Round.md) - Low-Level Design problems\n\n**Back to:** [README.md](./README.md)\n"
  },
  {
    "type": "file",
    "name": "03_Code_Design_LLD_Round.md",
    "content": "# \ud83c\udfa8 CODE DESIGN / LOW LEVEL DESIGN ROUND - Complete Guide\n\n**Duration:** 60 minutes\n**Format:** Object-Oriented Design + Implementation\n**Difficulty:** Medium to Hard\n**Expectations:** Clean, working code with good design patterns\n\n---\n\n## \ud83d\udccb Round Structure\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Problem Discussion (10 minutes)                  \u2502\n\u2502 \u251c\u2500 Understanding requirements                    \u2502\n\u2502 \u251c\u2500 Clarifying questions                          \u2502\n\u2502 \u2514\u2500 Discuss API/interface design                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Core Implementation (30-35 minutes)              \u2502\n\u2502 \u251c\u2500 Class design & relationships                  \u2502\n\u2502 \u251c\u2500 Code implementation                           \u2502\n\u2502 \u2514\u2500 Testing with examples                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Follow-ups & Extensions (15-20 minutes)          \u2502\n\u2502 \u251c\u2500 Add new features                              \u2502\n\u2502 \u251c\u2500 Handle edge cases                             \u2502\n\u2502 \u2514\u2500 Discuss improvements                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udc0d PROBLEM 1: SNAKE GAME (Most Popular!)\n\n### \u2b50\u2b50\u2b50\u2b50\u2b50 **Nokia Snake Game**\n\n**Frequency:** Appears in **50%** of Code Design rounds!\n\n**Problem Statement:**\n> Implement the classic Nokia Snake game:\n> - Snake moves on a 2D board\n> - Initial length: 3 units\n> - Grows by 1 unit every 5 moves\n> - Game ends when snake hits itself\n> - Snake can move up, down, left, right\n> - Board boundaries wrap around (optional)\n\n**Requirements:**\n1. `void moveSnake(Direction dir)` - Move snake in given direction\n2. `boolean isGameOver()` - Check if game has ended\n3. `Position getHeadPosition()` - Get current head position\n4. `int getScore()` - Get current score\n5. Working code with clean design\n\n**Visual Example:**\n```\nInitial (length 3):\n. . . . .\n. H B T .    H = Head, B = Body, T = Tail\n. . . . .\n\nAfter moveSnake(RIGHT):\n. . . . .\n. . H B T\n. . . . .\n\nAfter 5 moves (grows):\n. . . . .\n. . . H B\n. . . B T\n```\n\n---\n\n### \ud83d\udcbb **Complete Implementation**\n\n```python\nfrom enum import Enum\nfrom collections import deque\nfrom typing import List, Tuple, Optional\n\nclass Direction(Enum):\n    UP = (0, -1)\n    DOWN = (0, 1)\n    LEFT = (-1, 0)\n    RIGHT = (1, 0)\n\nclass Position:\n    def __init__(self, x: int, y: int):\n        self.x = x\n        self.y = y\n    \n    def __eq__(self, other):\n        return self.x == other.x and self.y == other.y\n    \n    def __hash__(self):\n        return hash((self.x, self.y))\n    \n    def __repr__(self):\n        return f\"({self.x}, {self.y})\"\n    \n    def move(self, direction: Direction) -> 'Position':\n        dx, dy = direction.value\n        return Position(self.x + dx, self.y + dy)\n\nclass Snake:\n    def __init__(self, start_pos: Position, initial_length: int = 3):\n        \"\"\"\n        Initialize snake at start position\n        \n        Args:\n            start_pos: Starting position of head\n            initial_length: Initial length of snake (default 3)\n        \"\"\"\n        self.body = deque()  # Deque for O(1) add/remove at both ends\n        \n        # Initialize snake horizontally\n        for i in range(initial_length):\n            self.body.append(Position(start_pos.x - i, start_pos.y))\n        \n        self.direction = Direction.RIGHT\n        self.moves_since_growth = 0\n        self.growth_interval = 5\n        \n    def get_head(self) -> Position:\n        return self.body[0]\n    \n    def get_tail(self) -> Position:\n        return self.body[-1]\n    \n    def move(self, new_direction: Direction) -> Position:\n        \"\"\"\n        Move snake in given direction\n        \n        Returns:\n            New head position after move\n        \"\"\"\n        # Prevent 180-degree turns (optional rule)\n        if self._is_opposite_direction(new_direction):\n            new_direction = self.direction\n        \n        self.direction = new_direction\n        \n        # Calculate new head position\n        current_head = self.get_head()\n        new_head = current_head.move(new_direction)\n        \n        # Add new head\n        self.body.appendleft(new_head)\n        \n        # Check if snake should grow\n        self.moves_since_growth += 1\n        \n        if self.moves_since_growth >= self.growth_interval:\n            # Grow: don't remove tail\n            self.moves_since_growth = 0\n        else:\n            # Don't grow: remove tail\n            self.body.pop()\n        \n        return new_head\n    \n    def check_self_collision(self) -> bool:\n        \"\"\"Check if head collides with body\"\"\"\n        head = self.get_head()\n        # Check if head position appears in body (excluding head itself)\n        return head in list(self.body)[1:]\n    \n    def _is_opposite_direction(self, new_dir: Direction) -> bool:\n        \"\"\"Check if new direction is opposite to current direction\"\"\"\n        if self.direction == Direction.UP and new_dir == Direction.DOWN:\n            return True\n        if self.direction == Direction.DOWN and new_dir == Direction.UP:\n            return True\n        if self.direction == Direction.LEFT and new_dir == Direction.RIGHT:\n            return True\n        if self.direction == Direction.RIGHT and new_dir == Direction.LEFT:\n            return True\n        return False\n    \n    def get_length(self) -> int:\n        return len(self.body)\n    \n    def get_body_positions(self) -> List[Position]:\n        return list(self.body)\n\nclass Board:\n    def __init__(self, width: int, height: int, wrap_boundaries: bool = False):\n        \"\"\"\n        Initialize game board\n        \n        Args:\n            width: Board width\n            height: Board height\n            wrap_boundaries: If True, snake wraps around edges\n        \"\"\"\n        self.width = width\n        self.height = height\n        self.wrap_boundaries = wrap_boundaries\n    \n    def is_valid_position(self, pos: Position) -> bool:\n        \"\"\"Check if position is within board boundaries\"\"\"\n        if self.wrap_boundaries:\n            return True  # All positions valid with wrapping\n        \n        return 0 <= pos.x < self.width and 0 <= pos.y < self.height\n    \n    def normalize_position(self, pos: Position) -> Position:\n        \"\"\"Normalize position for boundary wrapping\"\"\"\n        if not self.wrap_boundaries:\n            return pos\n        \n        return Position(\n            pos.x % self.width,\n            pos.y % self.height\n        )\n\nclass SnakeGame:\n    def __init__(self, width: int = 10, height: int = 10, wrap_boundaries: bool = False):\n        \"\"\"\n        Initialize Snake Game\n        \n        Args:\n            width: Board width\n            height: Board height\n            wrap_boundaries: If True, snake wraps around edges\n        \"\"\"\n        self.board = Board(width, height, wrap_boundaries)\n        \n        # Start snake in center\n        start_x = width // 2\n        start_y = height // 2\n        start_pos = Position(start_x, start_y)\n        \n        self.snake = Snake(start_pos)\n        self.game_over = False\n        self.score = 0\n    \n    def move_snake(self, direction: Direction) -> bool:\n        \"\"\"\n        Move snake in given direction\n        \n        Returns:\n            True if move successful, False if game over\n        \"\"\"\n        if self.game_over:\n            return False\n        \n        # Move snake\n        new_head = self.snake.move(direction)\n        \n        # Normalize position for boundary wrapping\n        new_head = self.board.normalize_position(new_head)\n        \n        # Update head position in snake body\n        self.snake.body[0] = new_head\n        \n        # Check collisions\n        if not self.board.is_valid_position(new_head):\n            # Hit boundary (when not wrapping)\n            self.game_over = True\n            return False\n        \n        if self.snake.check_self_collision():\n            # Hit itself\n            self.game_over = True\n            return False\n        \n        # Update score\n        self.score += 1\n        \n        return True\n    \n    def is_game_over(self) -> bool:\n        return self.game_over\n    \n    def get_head_position(self) -> Position:\n        return self.snake.get_head()\n    \n    def get_tail_position(self) -> Position:\n        return self.snake.get_tail()\n    \n    def get_score(self) -> int:\n        return self.score\n    \n    def get_snake_length(self) -> int:\n        return self.snake.get_length()\n    \n    def display(self):\n        \"\"\"Display current game state (for testing)\"\"\"\n        board = [['.' for _ in range(self.board.width)] \n                 for _ in range(self.board.height)]\n        \n        # Draw snake body\n        for i, pos in enumerate(self.snake.get_body_positions()):\n            if 0 <= pos.x < self.board.width and 0 <= pos.y < self.board.height:\n                if i == 0:\n                    board[pos.y][pos.x] = 'H'  # Head\n                elif i == len(self.snake.body) - 1:\n                    board[pos.y][pos.x] = 'T'  # Tail\n                else:\n                    board[pos.y][pos.x] = 'B'  # Body\n        \n        print(f\"Score: {self.score}, Length: {self.get_snake_length()}\")\n        for row in board:\n            print(' '.join(row))\n        print()\n\n# ===== USAGE EXAMPLE =====\n\nif __name__ == \"__main__\":\n    game = SnakeGame(width=10, height=10, wrap_boundaries=False)\n    \n    print(\"=== Initial State ===\")\n    game.display()\n    \n    # Play some moves\n    moves = [\n        Direction.RIGHT,\n        Direction.RIGHT,\n        Direction.DOWN,\n        Direction.DOWN,\n        Direction.LEFT,\n        Direction.LEFT,\n        Direction.UP\n    ]\n    \n    for i, move in enumerate(moves):\n        print(f\"=== Move {i+1}: {move.name} ===\")\n        success = game.move_snake(move)\n        game.display()\n        \n        if not success:\n            print(\"GAME OVER!\")\n            break\n    \n    print(f\"Final Score: {game.get_score()}\")\n    print(f\"Final Length: {game.get_snake_length()}\")\n```\n\n**Time Complexity:**\n- `moveSnake()`: O(1) - Deque operations\n- `checkSelfCollision()`: O(n) where n = snake length (can optimize to O(1) with HashSet)\n- `display()`: O(w * h) for rendering\n\n**Space Complexity:** O(n) where n = snake length\n\n---\n\n### \ud83d\ude80 **Optimized Version with O(1) Collision Detection**\n\n```python\nclass OptimizedSnake(Snake):\n    def __init__(self, start_pos: Position, initial_length: int = 3):\n        super().__init__(start_pos, initial_length)\n        \n        # HashSet for O(1) collision detection\n        self.body_set = set(self.body)\n    \n    def move(self, new_direction: Direction) -> Position:\n        current_head = self.get_head()\n        new_head = current_head.move(new_direction)\n        \n        # Add new head\n        self.body.appendleft(new_head)\n        self.body_set.add(new_head)\n        \n        self.moves_since_growth += 1\n        \n        if self.moves_since_growth < self.growth_interval:\n            # Remove tail\n            removed_tail = self.body.pop()\n            self.body_set.remove(removed_tail)\n        else:\n            self.moves_since_growth = 0\n        \n        return new_head\n    \n    def check_self_collision(self) -> bool:\n        \"\"\"O(1) collision check using HashSet\"\"\"\n        head = self.get_head()\n        \n        # Count occurrences of head in body_set\n        # If > 1, collision (head appears twice)\n        count = 0\n        for pos in self.body:\n            if pos == head:\n                count += 1\n                if count > 1:\n                    return True\n        return False\n```\n\n---\n\n### \ud83c\udfaf **Follow-up Questions**\n\n#### **Follow-up 1: Add Food**\n\n**Problem:** Add food that appears randomly. Snake grows when it eats food.\n\n```python\nimport random\n\nclass Food:\n    def __init__(self, position: Position):\n        self.position = position\n\nclass SnakeGameWithFood(SnakeGame):\n    def __init__(self, width: int = 10, height: int = 10):\n        super().__init__(width, height)\n        self.food = self._spawn_food()\n    \n    def _spawn_food(self) -> Food:\n        \"\"\"Spawn food at random empty position\"\"\"\n        while True:\n            x = random.randint(0, self.board.width - 1)\n            y = random.randint(0, self.board.height - 1)\n            pos = Position(x, y)\n            \n            # Check if position not occupied by snake\n            if pos not in self.snake.body:\n                return Food(pos)\n    \n    def move_snake(self, direction: Direction) -> bool:\n        if self.game_over:\n            return False\n        \n        # Store old tail before move\n        old_tail = self.snake.get_tail()\n        \n        # Move snake\n        new_head = self.snake.move(direction)\n        new_head = self.board.normalize_position(new_head)\n        self.snake.body[0] = new_head\n        \n        # Check collisions\n        if not self.board.is_valid_position(new_head) or \\\n           self.snake.check_self_collision():\n            self.game_over = True\n            return False\n        \n        # Check if ate food\n        if new_head == self.food.position:\n            # Grow snake by adding back the old tail\n            self.snake.body.append(old_tail)\n            # Spawn new food\n            self.food = self._spawn_food()\n            self.score += 10  # Bonus points for food\n        \n        self.score += 1\n        return True\n```\n\n#### **Follow-up 2: Multiple Snakes (Multiplayer)**\n\n```python\nclass MultiplayerSnakeGame:\n    def __init__(self, width: int, height: int, num_players: int = 2):\n        self.board = Board(width, height)\n        self.snakes = []\n        \n        # Create snakes at different starting positions\n        positions = [\n            Position(2, height // 2),\n            Position(width - 3, height // 2)\n        ]\n        \n        for i in range(num_players):\n            snake = Snake(positions[i])\n            self.snakes.append({\n                'snake': snake,\n                'alive': True,\n                'score': 0\n            })\n    \n    def move_snake(self, player_id: int, direction: Direction) -> bool:\n        if player_id >= len(self.snakes) or not self.snakes[player_id]['alive']:\n            return False\n        \n        player = self.snakes[player_id]\n        snake = player['snake']\n        \n        # Move\n        new_head = snake.move(direction)\n        \n        # Check self collision\n        if snake.check_self_collision():\n            player['alive'] = False\n            return False\n        \n        # Check collision with other snakes\n        for other_id, other in enumerate(self.snakes):\n            if other_id != player_id and other['alive']:\n                if new_head in other['snake'].body:\n                    player['alive'] = False\n                    return False\n        \n        player['score'] += 1\n        return True\n```\n\n#### **Follow-up 3: Unit Tests**\n\n```python\nimport unittest\n\nclass TestSnakeGame(unittest.TestCase):\n    def test_initial_state(self):\n        game = SnakeGame(10, 10)\n        self.assertEqual(game.get_snake_length(), 3)\n        self.assertFalse(game.is_game_over())\n        self.assertEqual(game.get_score(), 0)\n    \n    def test_movement(self):\n        game = SnakeGame(10, 10)\n        initial_head = game.get_head_position()\n        \n        game.move_snake(Direction.RIGHT)\n        new_head = game.get_head_position()\n        \n        self.assertEqual(new_head.x, initial_head.x + 1)\n        self.assertEqual(new_head.y, initial_head.y)\n    \n    def test_growth(self):\n        game = SnakeGame(10, 10)\n        initial_length = game.get_snake_length()\n        \n        # Move 5 times to trigger growth\n        for _ in range(5):\n            game.move_snake(Direction.RIGHT)\n        \n        # Should have grown by 1\n        self.assertEqual(game.get_snake_length(), initial_length + 1)\n    \n    def test_self_collision(self):\n        game = SnakeGame(10, 10)\n        \n        # Create a collision scenario\n        # Move in a circle to hit itself\n        game.move_snake(Direction.RIGHT)\n        game.move_snake(Direction.DOWN)\n        game.move_snake(Direction.LEFT)\n        game.move_snake(Direction.LEFT)\n        game.move_snake(Direction.UP)\n        game.move_snake(Direction.RIGHT)\n        \n        # Should detect collision (eventually)\n        # Exact moves depend on initial length\n    \n    def test_boundary_collision(self):\n        game = SnakeGame(5, 5, wrap_boundaries=False)\n        \n        # Move to edge\n        for _ in range(10):\n            success = game.move_snake(Direction.RIGHT)\n            if not success:\n                break\n        \n        self.assertTrue(game.is_game_over())\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n---\n\n## \ud83d\udcb0 PROBLEM 2: COST EXPLORER / SUBSCRIPTION BILLING\n\n### \u2b50\u2b50\u2b50 **Atlassian Subscription Pricing**\n\n**Problem Statement:**\n> Atlassian has three pricing tiers:\n> - BASIC: $9.99/month\n> - STANDARD: $49.99/month  \n> - PREMIUM: $249.99/month\n>\n> Customers can subscribe to multiple products (Jira, Confluence, etc.). Build a Cost Explorer that:\n> 1. Calculates monthly cost for each month of the year\n> 2. Provides yearly cost estimate\n\n**Example:**\n```python\ncustomer = Customer(\"C1\")\njira = Product(\"Jira\")\n\n# Subscription: start_date, end_date, tier\nsubscription = Subscription(\n    product=jira,\n    tier=\"BASIC\",\n    start_date=\"2024-01-01\",\n    end_date=\"2024-03-31\"\n)\n\n# Then upgrade\nsubscription2 = Subscription(\n    product=jira,\n    tier=\"PREMIUM\",\n    start_date=\"2024-04-01\",\n    end_date=\"2024-12-31\"\n)\n\ncost_explorer = CostExplorer(customer)\nmonthly_cost = cost_explorer.get_monthly_costs(year=2024)\n# Output: {\n#   \"Jan\": 9.99, \"Feb\": 9.99, \"Mar\": 9.99,\n#   \"Apr\": 249.99, ..., \"Dec\": 249.99\n# }\n\nyearly_cost = cost_explorer.get_yearly_cost(year=2024)\n# Output: 2279.91\n```\n\n**Solution:**\n\n```python\nfrom datetime import datetime, date\nfrom typing import List, Dict\nfrom enum import Enum\n\nclass Tier(Enum):\n    BASIC = 9.99\n    STANDARD = 49.99\n    PREMIUM = 249.99\n\nclass Product:\n    def __init__(self, name: str):\n        self.name = name\n\nclass Subscription:\n    def __init__(self, product: Product, tier: str, \n                 start_date: str, end_date: str):\n        self.product = product\n        self.tier = Tier[tier]\n        self.start_date = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n        self.end_date = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n    \n    def get_cost_for_month(self, year: int, month: int) -> float:\n        \"\"\"Get cost for specific month\"\"\"\n        month_start = date(year, month, 1)\n        \n        # Get last day of month\n        if month == 12:\n            month_end = date(year + 1, 1, 1)\n        else:\n            month_end = date(year, month + 1, 1)\n        \n        # Check if subscription active during this month\n        if self.end_date < month_start or self.start_date >= month_end:\n            return 0.0\n        \n        return self.tier.value\n\nclass Customer:\n    def __init__(self, customer_id: str):\n        self.customer_id = customer_id\n        self.subscriptions: List[Subscription] = []\n    \n    def add_subscription(self, subscription: Subscription):\n        self.subscriptions.append(subscription)\n\nclass CostExplorer:\n    def __init__(self, customer: Customer):\n        self.customer = customer\n    \n    def get_monthly_costs(self, year: int) -> Dict[str, float]:\n        \"\"\"Get cost for each month\"\"\"\n        months = [\n            \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n            \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n        ]\n        \n        monthly_costs = {}\n        \n        for month_num in range(1, 13):\n            month_name = months[month_num - 1]\n            total_cost = 0.0\n            \n            for subscription in self.customer.subscriptions:\n                cost = subscription.get_cost_for_month(year, month_num)\n                total_cost += cost\n            \n            monthly_costs[month_name] = total_cost\n        \n        return monthly_costs\n    \n    def get_yearly_cost(self, year: int) -> float:\n        \"\"\"Get total cost for year\"\"\"\n        monthly_costs = self.get_monthly_costs(year)\n        return sum(monthly_costs.values())\n\n# Usage\ncustomer = Customer(\"C1\")\njira = Product(\"Jira\")\n\nsub1 = Subscription(jira, \"BASIC\", \"2024-01-01\", \"2024-03-31\")\nsub2 = Subscription(jira, \"PREMIUM\", \"2024-04-01\", \"2024-12-31\")\n\ncustomer.add_subscription(sub1)\ncustomer.add_subscription(sub2)\n\nexplorer = CostExplorer(customer)\nprint(explorer.get_monthly_costs(2024))\nprint(f\"Yearly: ${explorer.get_yearly_cost(2024):.2f}\")\n```\n\n---\n\n## \u2b50 PROBLEM 3: AGENT RATING SYSTEM\n\n**Problem:** Customer support agents receive ratings. Return agents sorted by average rating.\n\n**Solution:**\n\n```python\nfrom typing import Dict, List\nfrom dataclasses import dataclass\n\n@dataclass\nclass Agent:\n    agent_id: int\n    name: str\n    ratings: List[int]\n    \n    def get_average_rating(self) -> float:\n        if not self.ratings:\n            return 0.0\n        return sum(self.ratings) / len(self.ratings)\n\nclass AgentRatingSystem:\n    def __init__(self):\n        self.agents: Dict[int, Agent] = {}\n    \n    def add_rating(self, agent_id: int, rating: int):\n        \"\"\"Add rating for agent (1-5 stars)\"\"\"\n        if agent_id not in self.agents:\n            raise ValueError(f\"Agent {agent_id} not found\")\n        \n        if not 1 <= rating <= 5:\n            raise ValueError(\"Rating must be 1-5\")\n        \n        self.agents[agent_id].ratings.append(rating)\n    \n    def get_top_agents(self) -> List[Agent]:\n        \"\"\"Return all agents sorted by average rating (descending)\"\"\"\n        sorted_agents = sorted(\n            self.agents.values(),\n            key=lambda a: a.get_average_rating(),\n            reverse=True\n        )\n        return sorted_agents\n```\n\n---\n\n## \ud83c\udfac PROBLEM 4: CINEMA HALL SCHEDULING\n\n**Problem:** Schedule movies in cinema without conflicts.\n\n```python\nfrom typing import List\n\nclass Movie:\n    def __init__(self, title: str, duration: int):\n        self.title = title\n        self.duration = duration  # in minutes\n\nclass Screening:\n    def __init__(self, movie: Movie, start_time: int):\n        self.movie = movie\n        self.start_time = start_time  # minutes from midnight\n        self.end_time = start_time + movie.duration\n\nclass CinemaSchedule:\n    def __init__(self, open_time: int = 600, close_time: int = 1380):\n        \"\"\"\n        Args:\n            open_time: Opening time (minutes from midnight, default 10 AM = 600)\n            close_time: Closing time (minutes from midnight, default 11 PM = 1380)\n        \"\"\"\n        self.open_time = open_time\n        self.close_time = close_time\n        self.screenings: List[Screening] = []\n    \n    def can_schedule(self, movie: Movie, start_time: int) -> bool:\n        \"\"\"Check if movie can be scheduled at given time\"\"\"\n        end_time = start_time + movie.duration\n        \n        # Check operating hours\n        if start_time < self.open_time or end_time > self.close_time:\n            return False\n        \n        # Check conflicts with existing screenings\n        for screening in self.screenings:\n            if self._has_overlap(start_time, end_time, \n                                 screening.start_time, screening.end_time):\n                return False\n        \n        return True\n    \n    def schedule_movie(self, movie: Movie, start_time: int) -> bool:\n        \"\"\"Schedule movie if possible\"\"\"\n        if self.can_schedule(movie, start_time):\n            self.screenings.append(Screening(movie, start_time))\n            return True\n        return False\n    \n    def _has_overlap(self, start1: int, end1: int, \n                     start2: int, end2: int) -> bool:\n        \"\"\"Check if two time intervals overlap\"\"\"\n        return max(start1, start2) < min(end1, end2)\n```\n\n---\n\n## \ud83d\udea6 PROBLEM 5: RATE LIMITER\n\n**Problem:** Limit user to X requests in Y seconds.\n\n```python\nfrom collections import deque\nimport time\n\nclass RateLimiter:\n    def __init__(self, max_requests: int, time_window: int):\n        \"\"\"\n        Args:\n            max_requests: Maximum requests allowed\n            time_window: Time window in seconds\n        \"\"\"\n        self.max_requests = max_requests\n        self.time_window = time_window\n        self.user_requests = {}  # user_id -> deque of timestamps\n    \n    def allow_request(self, user_id: str) -> bool:\n        \"\"\"Check if user can make request\"\"\"\n        current_time = time.time()\n        \n        if user_id not in self.user_requests:\n            self.user_requests[user_id] = deque()\n        \n        requests = self.user_requests[user_id]\n        \n        # Remove old requests outside time window\n        while requests and requests[0] <= current_time - self.time_window:\n            requests.popleft()\n        \n        # Check if limit reached\n        if len(requests) >= self.max_requests:\n            return False\n        \n        # Allow request\n        requests.append(current_time)\n        return True\n\n# Usage\nlimiter = RateLimiter(max_requests=5, time_window=60)  # 5 requests per minute\nprint(limiter.allow_request(\"user1\"))  # True\n```\n\n---\n\n## \u2705 KEY TAKEAWAYS\n\n**What Interviewers Look For:**\n1. \u2705 Clean, modular code\n2. \u2705 Proper OOP design (classes, encapsulation)\n3. \u2705 Design patterns (Strategy, Factory, etc.)\n4. \u2705 Exception handling\n5. \u2705 Edge case handling\n6. \u2705 Testing mindset (mention unit tests)\n7. \u2705 Time/space complexity awareness\n\n**Common Mistakes:**\n1. \u274c Writing monolithic code (one big function)\n2. \u274c No input validation\n3. \u274c Ignoring edge cases\n4. \u274c No exception handling\n5. \u274c Not testing code with examples\n6. \u274c Poor naming conventions\n\n---\n\n**Next:** [04_System_Design_HLD_Round.md](./04_System_Design_HLD_Round.md)\n**Back to:** [README.md](./README.md)\n"
  },
  {
    "type": "file",
    "name": "04_System_Design_HLD_Round.md",
    "content": "# \ud83c\udfd7\ufe0f SYSTEM DESIGN / HLD ROUND - Complete Guide\n\n**Duration:** 60 minutes\n**Format:** High-Level Architecture Design\n**Difficulty:** Hard\n**Pass Rate:** ~65%\n\n---\n\n## \ud83d\udccb Round Structure\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Requirements Gathering (5-10 minutes)            \u2502\n\u2502 \u251c\u2500 Functional requirements                       \u2502\n\u2502 \u251c\u2500 Non-functional requirements                   \u2502\n\u2502 \u2514\u2500 Constraints & assumptions                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 API Design (10 minutes)                          \u2502\n\u2502 \u251c\u2500 REST/GraphQL endpoints                        \u2502\n\u2502 \u2514\u2500 Request/Response formats                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Database Schema (10 minutes)                     \u2502\n\u2502 \u251c\u2500 Tables/Collections design                     \u2502\n\u2502 \u251c\u2500 Indexes & relationships                       \u2502\n\u2502 \u2514\u2500 SQL vs NoSQL decision                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Architecture & Scalability (20-25 minutes)       \u2502\n\u2502 \u251c\u2500 Component diagram                             \u2502\n\u2502 \u251c\u2500 Data flow                                     \u2502\n\u2502 \u251c\u2500 Caching strategy                              \u2502\n\u2502 \u251c\u2500 Load balancing                                \u2502\n\u2502 \u2514\u2500 Sharding/Partitioning                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Deep Dives (10-15 minutes)                       \u2502\n\u2502 \u251c\u2500 Bottlenecks & optimizations                   \u2502\n\u2502 \u2514\u2500 Trade-off discussions                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83c\udff7\ufe0f PROBLEM 1: TAGGING MANAGEMENT SYSTEM (Most Popular!)\n\n### \u2b50\u2b50\u2b50\u2b50\u2b50 **Product-Agnostic Tagging System**\n\n**Frequency:** Appears in **60%** of HLD rounds!\n\n**Problem Statement:**\n> Design a scalable tagging system for Atlassian products (Jira, Confluence, Bitbucket). Users should be able to:\n> - Add/remove/update tags on content\n> - Search content by tags\n> - View popular/trending tags\n> - Get autocomplete suggestions\n\n**Products:**\n- Jira \u2192 Issues\n- Confluence \u2192 Pages\n- Bitbucket \u2192 Pull Requests\n\n---\n\n### \ud83d\udcdd **Step 1: Requirements Clarification**\n\n**Functional Requirements:**\n1. Add tag to content\n2. Remove tag from content\n3. Update tag name\n4. Get all content with specific tag\n5. Get all tags for specific content\n6. Search/autocomplete tags\n7. Get trending/popular tags\n\n**Non-Functional Requirements:**\n1. **Scale:** \n   - 100M users\n   - 1B pieces of content\n   - 10M unique tags\n   - 10B tag-content mappings\n2. **Performance:**\n   - Tag search: < 50ms\n   - Autocomplete: < 20ms\n   - Add/remove tag: < 100ms\n3. **Availability:** 99.9%\n4. **Consistency:** Eventual consistency OK for tag counts\n\n**Out of Scope (Clarify!):**\n- Tag permissions/access control\n- Tag hierarchies (nested tags)\n- User-specific tags (private tags)\n\n---\n\n### \ud83c\udf10 **Step 2: API Design**\n\n```javascript\n// RESTful API Design\n\n// 1. Add tag to content\nPOST /api/v1/content/{contentId}/tags\n{\n  \"tagName\": \"frontend\",\n  \"productType\": \"jira\"\n}\nResponse: 201 Created\n\n// 2. Remove tag from content\nDELETE /api/v1/content/{contentId}/tags/{tagId}\nResponse: 204 No Content\n\n// 3. Get all tags for content\nGET /api/v1/content/{contentId}/tags\nResponse: {\n  \"contentId\": \"123\",\n  \"tags\": [\n    {\"id\": \"1\", \"name\": \"frontend\", \"count\": 500},\n    {\"id\": \"2\", \"name\": \"react\", \"count\": 300}\n  ]\n}\n\n// 4. Get content by tag\nGET /api/v1/tags/{tagName}/content?product=jira&page=1&limit=20\nResponse: {\n  \"tagName\": \"frontend\",\n  \"totalCount\": 1500,\n  \"content\": [\n    {\"contentId\": \"123\", \"title\": \"...\", \"type\": \"issue\"},\n    // ...\n  ]\n}\n\n// 5. Search/Autocomplete tags\nGET /api/v1/tags/search?q=fron&limit=10\nResponse: {\n  \"suggestions\": [\n    {\"id\": \"1\", \"name\": \"frontend\", \"count\": 5000},\n    {\"id\": \"2\", \"name\": \"front-end\", \"count\": 200}\n  ]\n}\n\n// 6. Get trending tags\nGET /api/v1/tags/trending?product=jira&timeWindow=7d&limit=10\nResponse: {\n  \"trends\": [\n    {\"name\": \"frontend\", \"count\": 500, \"growth\": \"+25%\"},\n    // ...\n  ]\n}\n```\n\n---\n\n### \ud83d\uddc4\ufe0f **Step 3: Database Schema**\n\n#### **Option 1: Relational (PostgreSQL)**\n\n```sql\n-- Tags table\nCREATE TABLE tags (\n    tag_id BIGSERIAL PRIMARY KEY,\n    tag_name VARCHAR(255) UNIQUE NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW(),\n    usage_count BIGINT DEFAULT 0\n);\n\nCREATE INDEX idx_tag_name ON tags(tag_name);\nCREATE INDEX idx_usage_count ON tags(usage_count DESC);\n\n-- Content table (simplified)\nCREATE TABLE content (\n    content_id BIGSERIAL PRIMARY KEY,\n    product_type VARCHAR(50),  -- 'jira', 'confluence', 'bitbucket'\n    title VARCHAR(500),\n    created_by BIGINT,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE INDEX idx_content_product ON content(product_type);\n\n-- Tag-Content mapping (many-to-many)\nCREATE TABLE content_tags (\n    id BIGSERIAL PRIMARY KEY,\n    content_id BIGINT NOT NULL,\n    tag_id BIGINT NOT NULL,\n    tagged_at TIMESTAMP DEFAULT NOW(),\n    tagged_by BIGINT,\n    \n    FOREIGN KEY (content_id) REFERENCES content(content_id),\n    FOREIGN KEY (tag_id) REFERENCES tags(tag_id),\n    \n    UNIQUE(content_id, tag_id)  -- Prevent duplicate tags\n);\n\n-- Composite indexes for common queries\nCREATE INDEX idx_content_tags_content ON content_tags(content_id);\nCREATE INDEX idx_content_tags_tag ON content_tags(tag_id);\nCREATE INDEX idx_content_tags_time ON content_tags(tagged_at DESC);\n\n-- For trending tags (time-series data)\nCREATE TABLE tag_usage_stats (\n    id BIGSERIAL PRIMARY KEY,\n    tag_id BIGINT NOT NULL,\n    date DATE NOT NULL,\n    usage_count INT DEFAULT 0,\n    \n    UNIQUE(tag_id, date)\n);\n\nCREATE INDEX idx_tag_stats_date ON tag_usage_stats(date DESC);\n```\n\n**Queries:**\n```sql\n-- Add tag to content\nINSERT INTO content_tags (content_id, tag_id, tagged_by) \nVALUES (123, 45, 1001);\n\n-- Get all tags for content\nSELECT t.tag_id, t.tag_name, t.usage_count\nFROM tags t\nJOIN content_tags ct ON t.tag_id = ct.tag_id\nWHERE ct.content_id = 123;\n\n-- Get content by tag (paginated)\nSELECT c.content_id, c.title, c.product_type\nFROM content c\nJOIN content_tags ct ON c.content_id = ct.content_id\nWHERE ct.tag_id = 45\nORDER BY ct.tagged_at DESC\nLIMIT 20 OFFSET 0;\n\n-- Autocomplete tags\nSELECT tag_id, tag_name, usage_count\nFROM tags\nWHERE tag_name LIKE 'fron%'\nORDER BY usage_count DESC\nLIMIT 10;\n\n-- Trending tags (last 7 days)\nSELECT t.tag_name, SUM(tus.usage_count) as total_uses\nFROM tags t\nJOIN tag_usage_stats tus ON t.tag_id = tus.tag_id\nWHERE tus.date >= CURRENT_DATE - INTERVAL '7 days'\nGROUP BY t.tag_id, t.tag_name\nORDER BY total_uses DESC\nLIMIT 10;\n```\n\n#### **Option 2: NoSQL (DynamoDB)**\n\n```\n// Tags Table\nTable: tags\nPartition Key: tag_id\nSort Key: -\nAttributes: {\n  tag_id: string,\n  tag_name: string,\n  usage_count: number,\n  created_at: timestamp\n}\nGSI: tag_name-index (for lookup by name)\n\n// Content Tags Table (mappings)\nTable: content_tags\nPartition Key: content_id\nSort Key: tag_id\nAttributes: {\n  content_id: string,\n  tag_id: string,\n  tagged_at: timestamp,\n  tagged_by: string\n}\nGSI: tag_id-tagged_at-index (for reverse lookup: tag -> contents)\n\n// Tag to Content (reverse index)\nTable: tag_contents\nPartition Key: tag_id\nSort Key: content_id#timestamp\nAttributes: {\n  tag_id: string,\n  content_id: string,\n  product_type: string,\n  timestamp: number\n}\n```\n\n**Why SQL over NoSQL for this use case?**\n- \u2705 Complex queries (JOIN, aggregations)\n- \u2705 ACID transactions for consistency\n- \u2705 Mature indexing capabilities\n- \u2705 Tag analytics (counts, trends)\n- \u274c NoSQL: Hard to model many-to-many relationships efficiently\n\n---\n\n### \ud83c\udfdb\ufe0f **Step 4: High-Level Architecture**\n\n```\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502   CDN / Edge    \u2502\n                          \u2502  (Static Assets)\u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502  Load Balancer  \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502                    \u2502                    \u2502\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502  API Gateway    \u2502  \u2502  API Gateway    \u2502  \u2502  API Gateway\u2502\n     \u2502   (Node.js)     \u2502  \u2502   (Node.js)     \u2502  \u2502  (Node.js)  \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502                    \u2502                    \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                          \u2502                          \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Tagging       \u2502       \u2502  Search Service     \u2502     \u2502  Analytics      \u2502\n\u2502 Service       \u2502       \u2502  (Elasticsearch)    \u2502     \u2502  Service        \u2502\n\u2502 (Java/Go)     \u2502       \u2502  - Autocomplete     \u2502     \u2502  (Spark)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502  - Fuzzy search     \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n        \u2502                          \u2502                         \u2502\n        \u2502                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n        \u2502                  \u2502  Redis Cache   \u2502                \u2502\n        \u2502                  \u2502  - Tag counts  \u2502                \u2502\n        \u2502                  \u2502  - Hot tags    \u2502                \u2502\n        \u2502                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n        \u2502                          \u2502                         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    PostgreSQL (Primary)                           \u2502\n\u2502  - Tags table                                                     \u2502\n\u2502  - Content table                                                  \u2502\n\u2502  - Content_tags mapping                                           \u2502\n\u2502                                                                   \u2502\n\u2502  Sharding Strategy: By tag_id hash                               \u2502\n\u2502  Read Replicas: 3-5 for read-heavy workload                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Kafka / SQS    \u2502\n\u2502  Event Stream   \u2502\n\u2502  - Tag added    \u2502\n\u2502  - Tag removed  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502  Trend       \u2502\n   \u2502  Calculator  \u2502\n   \u2502  (Batch)     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n### \u26a1 **Step 5: Scalability & Optimizations**\n\n#### **Caching Strategy**\n\n```python\n# Redis Cache Structure\n\n# 1. Tag metadata cache (frequently accessed tags)\nKey: \"tag:{tag_id}\"\nValue: {\n  \"name\": \"frontend\",\n  \"count\": 50000\n}\nTTL: 1 hour\n\n# 2. Content tags cache\nKey: \"content:{content_id}:tags\"\nValue: [\"tag1\", \"tag2\", \"tag3\"]\nTTL: 5 minutes\n\n# 3. Tag search results cache\nKey: \"tag:search:{query}\"\nValue: [\n  {\"id\": 1, \"name\": \"frontend\", \"count\": 5000},\n  {\"id\": 2, \"name\": \"front-end\", \"count\": 200}\n]\nTTL: 10 minutes\n\n# 4. Trending tags cache\nKey: \"tags:trending:{product}:{timeWindow}\"\nValue: [{\"name\": \"frontend\", \"count\": 500}, ...]\nTTL: 30 minutes (or update via cron)\n```\n\n#### **Database Sharding**\n\n**Shard by tag_id:**\n```\nShard 1: tag_id % 10 == 0,1\nShard 2: tag_id % 10 == 2,3\nShard 3: tag_id % 10 == 4,5\n...\n```\n\n**Challenge:** How to get \"all tags for content\"?\n- Need to query all shards (fan-out query)\n- Solution: Maintain reverse index in separate table\n  - Table: content_to_tags (sharded by content_id)\n  - Stores all tags for a content_id\n\n#### **Elasticsearch for Search**\n\n```json\n// Index: tags\n{\n  \"mappings\": {\n    \"properties\": {\n      \"tag_id\": {\"type\": \"keyword\"},\n      \"tag_name\": {\n        \"type\": \"text\",\n        \"analyzer\": \"standard\",\n        \"fields\": {\n          \"keyword\": {\"type\": \"keyword\"},\n          \"ngram\": {\n            \"type\": \"text\",\n            \"analyzer\": \"ngram_analyzer\"\n          }\n        }\n      },\n      \"usage_count\": {\"type\": \"integer\"},\n      \"product_type\": {\"type\": \"keyword\"}\n    }\n  }\n}\n\n// Autocomplete query\nGET /tags/_search\n{\n  \"query\": {\n    \"match\": {\n      \"tag_name.ngram\": \"fron\"\n    }\n  },\n  \"sort\": [\n    {\"usage_count\": \"desc\"}\n  ],\n  \"size\": 10\n}\n```\n\n#### **Rate Limiting**\n\n```\nPer user:\n- Add/remove tag: 100 requests/min\n- Search tags: 1000 requests/min\n- Get content by tag: 500 requests/min\n\nImplementation: Redis with sliding window\n```\n\n---\n\n### \ud83d\udd25 **Step 6: Deep Dive Topics**\n\n#### **How to Handle Trending Tags?**\n\n**Approach: Time-windowed aggregation**\n\n```python\n# Real-time pipeline\n1. User adds tag -> Event to Kafka\n2. Stream processor (Flink/Spark Streaming) aggregates:\n   - Count tags added per 5-min window\n   - Keep sliding window of last 24 hours\n3. Update trending_tags table\n4. Cache results in Redis\n\n# Batch pipeline (backup)\n1. Daily cron job\n2. Query tag_usage_stats table\n3. Calculate growth rate: (today - yesterday) / yesterday\n4. Update trending cache\n```\n\n#### **How to Handle Tag Renames?**\n\n```sql\n-- When tag \"frontend\" renamed to \"front-end\"\nBEGIN TRANSACTION;\n\n-- 1. Update tag name\nUPDATE tags SET tag_name = 'front-end' WHERE tag_id = 123;\n\n-- 2. Invalidate caches\nDELETE FROM cache WHERE key LIKE '%:123:%';\n\n-- 3. Update Elasticsearch\nPOST /tags/_update/123 {\"doc\": {\"tag_name\": \"front-end\"}}\n\nCOMMIT;\n```\n\n#### **How to Prevent Tag Spam?**\n\n1. **Rate limiting** - Max 10 tags per content\n2. **Duplicate detection** - Fuzzy matching (Levenshtein distance)\n3. **Admin review** - Flag tags with sudden spike in usage\n4. **Machine learning** - Detect spam patterns\n\n---\n\n### \ud83d\udcca **Capacity Estimation**\n\n```\nStorage:\n- Tags: 10M * 100 bytes = 1 GB\n- Content: 1B * 500 bytes = 500 GB\n- Mappings: 10B * (8+8+8) bytes = 240 GB\nTotal: ~750 GB (with indexes: ~2 TB)\n\nQPS:\n- Read (get tags, search): 100K QPS (90% of traffic)\n- Write (add/remove tags): 10K QPS\n\nNetwork:\n- Read: 100K * 1KB = 100 MB/s = 800 Mbps\n- Write: 10K * 1KB = 10 MB/s = 80 Mbps\n\nCaching:\n- Hot tags (top 1%): 100K tags * 100 bytes = 10 MB\n- Recent searches: 1M queries * 1KB = 1 GB\nTotal cache: ~2 GB (easily fits in Redis)\n```\n\n---\n\n## \ud83d\udd77\ufe0f PROBLEM 2: WEB SCRAPING SYSTEM\n\n**Problem:** Design a scalable web scraper that extracts images from URLs.\n\n**APIs:**\n```\nPOST /jobs -> {jobId}\nGET /jobs/{jobId}/status -> {completed: 5, inProgress: 3}\nGET /jobs/{jobId}/results -> {url: [images]}\n```\n\n**Architecture:**\n```\nClient -> API Gateway -> Job Service -> SQS Queue\n                              \u2193\n                         Worker Pool (EC2/Lambda)\n                              \u2193\n                      S3 (store results)\n                      Redis (job status)\n```\n\n**Key Components:**\n1. **Job Service:** Create scraping jobs\n2. **SQS Queue:** Distributed task queue\n3. **Worker Pool:** Scrape URLs in parallel\n4. **S3:** Store scraped images/data\n5. **Redis:** Track job progress\n\n**Challenges:**\n- Rate limiting (robots.txt)\n- Duplicate URL detection (Bloom filter)\n- Failed scrapes (retry with exponential backoff)\n- Nested URLs (BFS traversal with depth limit)\n\n---\n\n## \ud83d\udcc4 PROBLEM 3: GOOGLE DOCS CLONE\n\n**Requirements:**\n- Real-time collaborative editing\n- Conflict resolution\n- Version history\n\n**Key Technologies:**\n- **WebSockets** for real-time sync\n- **Operational Transformation (OT)** or **CRDT** for conflict resolution\n- **Event sourcing** for version history\n\n**Architecture:**\n```\nClient (Editor) <-> WebSocket Server <-> Pub/Sub (Redis)\n                         \u2193\n                    Database (MongoDB)\n                    Version Store (S3)\n```\n\n---\n\n## \ud83c\udfaf KEY TAKEAWAYS\n\n**What Interviewers Look For:**\n1. \u2705 **Requirements gathering** - Ask clarifying questions\n2. \u2705 **API design first** - Start with APIs before architecture\n3. \u2705 **Database schema** - Justify SQL vs NoSQL\n4. \u2705 **Scalability** - Caching, sharding, load balancing\n5. \u2705 **Trade-offs** - Discuss alternatives and why you chose one\n6. \u2705 **Bottlenecks** - Identify and solve bottlenecks\n7. \u2705 **Numbers** - Back-of-envelope calculations\n\n**Common Mistakes:**\n1. \u274c Jumping to architecture without requirements\n2. \u274c Not designing APIs\n3. \u274c Ignoring database design\n4. \u274c Over-engineering (adding ML, blockchain unnecessarily)\n5. \u274c No numbers/estimates\n6. \u274c Not discussing trade-offs\n\n---\n\n**Next:** [05_Values_Behavioral_Round.md](./05_Values_Behavioral_Round.md)\n**Back to:** [README.md](./README.md)\n"
  },
  {
    "type": "file",
    "name": "05_Values_Behavioral_Round.md",
    "content": "# \ud83c\udfad VALUES & BEHAVIORAL ROUND - Complete Guide\n\n**Duration:** 45 minutes\n**Format:** STAR-based behavioral questions\n**Difficulty:** Medium (often underestimated!)\n**Critical:** Can reject even with all technical \"Hire\" ratings\n\n---\n\n## \u26a0\ufe0f IMPORTANCE\n\n**DO NOT UNDERESTIMATE THIS ROUND!**\n\nMany candidates receive rejection despite:\n- \u2705 Strong hire in all technical rounds\n- \u2705 Excellent coding skills\n- \u2705 Great system design\n\n\u274c **Rejection reason:** Weak values alignment or poor behavioral examples\n\n**Statistics:** ~40% of rejections happen due to Values/Managerial rounds\n\n---\n\n## \ud83c\udf1f ATLASSIAN'S 5 CORE VALUES\n\n### 1. **Open Company, No Bullshit**\nBe open, honest, and transparent\n\n### 2. **Build with Heart and Balance**\nCare about people, sustainability, work-life balance\n\n### 3. **Don't Fuck the Customer** (Yes, that's the real value!)\nCustomer comes first, always\n\n### 4. **Play, As a Team**\nCollaboration over individual heroics\n\n### 5. **Be the Change You Seek**\nTake ownership, drive change proactively\n\n---\n\n## \ud83d\udcdd STAR FORMAT\n\nEvery answer should follow STAR:\n\n- **S**ituation: Set the context (30 seconds)\n- **T**ask: Explain your responsibility (15 seconds)\n- **A**ction: Describe what YOU did (90 seconds)\n- **R**esult: Share the outcome with metrics (30 seconds)\n\n**Total:** ~2-3 minutes per answer\n\n---\n\n## \ud83d\udc8e VALUE 1: OPEN COMPANY, NO BULLSHIT\n\n### Common Questions:\n\n**Q1: Tell me about a time you had to deliver difficult feedback to a colleague or manager.**\n\n**Example Answer (STAR):**\n\n**Situation:**\n\"In my previous role at XYZ Corp, I was working with a senior engineer, let's call him John, who was consistently missing deadlines for critical features. This was blocking the entire team's sprint goals. Other team members were frustrated but hesitant to speak up due to John's seniority and tenure.\"\n\n**Task:**\n\"As the tech lead, it was my responsibility to ensure team velocity and morale. I needed to address this issue directly while maintaining a respectful working relationship.\"\n\n**Action:**\n\"I scheduled a 1:1 with John in a private setting. I prepared by:\n1. Documenting specific examples (3 sprints where deadlines were missed)\n2. Understanding his perspective first - I asked if there were blockers I wasn't aware of\n3. He shared that he was overcommitted on another project (that management had assigned)\n\nRather than criticizing, I:\n- Acknowledged the conflicting priorities he was facing\n- Shared the team impact using concrete examples: 'When the auth feature delayed by 2 weeks, the mobile team couldn't start their integration'\n- Proposed solutions: Either reduce his commitments on the other project OR re-scope our current sprint\n- Escalated to management WITH John (not behind his back) to get priority clarification\"\n\n**Result:**\n\"Management agreed to reduce John's involvement in the other project by 50%. In the next 2 sprints:\n- We achieved 100% of our sprint goals\n- John became more proactive about flagging blockers early\n- Team morale improved significantly (measured by retrospective feedback)\n- John later thanked me for being direct and helping him get the support he needed\n\nThis reinforced my belief that transparent, empathetic communication solves problems better than avoiding difficult conversations.\"\n\n---\n\n**Q2: Describe a situation where you disagreed with a decision made by management. How did you handle it?**\n\n**Example Answer:**\n\n**Situation:**\n\"Last year, management decided to cut our testing sprint by 50% to meet an aggressive launch deadline for a new payment feature. This feature would handle real money transactions, and I strongly believed inadequate testing could lead to serious production issues.\"\n\n**Task:**\n\"As a senior engineer, I felt responsible for advocating for quality, even if it meant pushing back on leadership.\"\n\n**Action:**\n\"I didn't just say 'No, this is risky.' Instead, I:\n1. Quantified the risk - Created a risk matrix showing:\n   - 15 critical test scenarios not covered in compressed timeline\n   - Historical data: Our previous payment bug cost $50K in customer refunds\n   - Probability and impact analysis\n\n2. Presented alternatives in a meeting with the VP:\n   - Option A: Launch with full testing (2 weeks delay)\n   - Option B: Launch with core scenarios only, add gradual rollout to 5% users first\n   - Option C: Launch on time but defer 2 non-critical features\n\n3. Involved the team - Got input from QA lead and product manager to show unified concern\n\n4. Respected final decision - Made it clear I'd support whatever leadership decided, but wanted them to have full information\"\n\n**Result:**\n\"Management appreciated the data-driven approach and chose Option B:\n- We launched on time to 5% traffic\n- Caught 3 critical bugs in gradual rollout that would've affected 100% of users\n- Full rollout happened 1 week later, successfully\n- VP later said this approach would become standard for high-risk features\n\nKey learning: Transparency isn't just about being honest, it's about enabling better decisions with complete information.\"\n\n---\n\n## \ud83d\udc99 VALUE 2: BUILD WITH HEART AND BALANCE\n\n### Common Questions:\n\n**Q3: Tell me about a time you helped a struggling team member.**\n\n**Q4: How do you maintain work-life balance in a high-pressure environment?**\n\n**Example Answer (Q3):**\n\n**Situation:**\n\"I noticed one of our junior engineers, Sarah, who had joined 3 months ago, was working 12+ hour days and still falling behind. In our 1:1s, she seemed stressed and mentioned feeling overwhelmed. Her code review turnaround was taking 3-4 days, blocking others.\"\n\n**Task:**\n\"As her mentor, I wanted to help her succeed without burning out. I also needed to ensure team velocity wasn't affected.\"\n\n**Action:**\n\"I took a multi-pronged approach:\n\n1. **Understanding the root cause:**\n   - Paired programming session showed she was stuck on async JavaScript concepts\n   - She was too afraid to ask questions in team channels (imposter syndrome)\n\n2. **Structured support:**\n   - Created a learning plan: 30 mins daily for async/await tutorial I curated\n   - Set up daily 15-min check-ins for quick unblocking (not judging, just helping)\n   - Explicitly told her: 'Asking questions shows strength, not weakness'\n\n3. **Team culture change:**\n   - Started 'Curious Minds Friday' - Anyone asks any question, no judgment\n   - Shared my own learning struggles when I was junior\n\n4. **Workload adjustment:**\n   - Temporarily reduced her sprint commitment by 30%\n   - Paired her with a senior dev on complex tasks\n\n5. **Psychological safety:**\n   - Shared my own story of struggling with Kubernetes initially\n   - Normalized asking for help by doing it myself publicly\"\n\n**Result:**\n\"Within 6 weeks:\n- Sarah's code review time dropped from 3-4 days to same-day for most PRs\n- Her confidence visibly increased - she started answering questions from other juniors\n- She completed her sprint commitment 2 sprints in a row\n- Most importantly, she sent me a message: 'I almost quit in month 2, but you made me feel it's okay to be a learner'\n\nThe team adopted 'Curious Minds Friday' permanently - now we have 80%+ participation.\n\nThis taught me that sustainable high performance requires investing in people's growth AND well-being.\"\n\n---\n\n## \ud83d\ude45 VALUE 3: DON'T FUCK THE CUSTOMER\n\n### Common Questions:\n\n**Q5: Tell me about a time when you had to choose between shipping fast or building the right solution for customers.**\n\n**Q6: Describe a situation where you advocated for the customer against internal pressure.**\n\n**Example Answer (Q5):**\n\n**Situation:**\n\"We were building a new dashboard feature for enterprise customers. Two weeks before launch, sales team pushed hard to ship immediately because a $2M deal was waiting for this feature. However, our user testing revealed the UI was confusing - 4 out of 5 users couldn't complete core workflows without help.\"\n\n**Task:**\n\"As the product engineer, I had to decide: Ship now to close the deal, or delay to fix UX issues.\"\n\n**Action:**\n\"I advocated strongly for the customer by:\n\n1. **Data-driven case:**\n   - Shared user testing video clips in leadership meeting (more powerful than just saying 'it's confusing')\n   - Calculated: If we ship bad UX, we'll likely need 2-3 months of iteration based on past similar features\n   - Showed competitor's dashboard that solved this elegantly\n\n2. **Creative compromise:**\n   - Proposed a 10-day delay (not full redesign)\n   - Identified 3 critical UX fixes that would address 80% of issues\n   - Suggested giving the $2M customer early beta access with hand-holding\n\n3. **Customer empathy:**\n   - Reminded team: 'This $2M customer will become our best or worst reference. Let's make them love us.'\n   - Shared a past example where we shipped fast and spent 6 months in damage control\n\n4. **Took ownership:**\n   - Volunteered to personally support the beta customer\n   - Committed to 10-day timeline with daily updates\"\n\n**Result:**\n\"Leadership agreed to the 10-day delay:\n- We shipped with improved UX\n- The $2M customer closed (sales was nervous, but I joined the demo call personally)\n- Customer feedback: 'Most intuitive dashboard we've seen'\n- They became a case study and referred 2 more enterprise clients\n- Feature adoption: 70% DAU vs our usual 40% for new features\n\nLearned: Short-term pressure is real, but long-term customer love requires quality. And video clips are worth 1000 words in meetings!\"\n\n---\n\n## \ud83e\udd1d VALUE 4: PLAY, AS A TEAM\n\n### Common Questions:\n\n**Q7: Describe a situation where you had a conflict with a team member. How did you resolve it?**\n\n**Q8: Tell me about a time you had to collaborate with a difficult stakeholder.**\n\n**Example Answer (Q7):**\n\n**Situation:**\n\"I was leading an API redesign project. Another senior engineer, Mark, strongly disagreed with my approach - he wanted a GraphQL API while I proposed REST. This turned into heated debates in code reviews, and the team felt stuck between two 'leaders' fighting.\"\n\n**Task:**\n\"I needed to resolve this conflict constructively without either of us 'losing,' while making a decision that moved the project forward.\"\n\n**Action:**\n\"I changed my approach from debate to collaboration:\n\n1. **Private conversation first:**\n   - Asked Mark for a 1:1 coffee chat (not a meeting)\n   - Started with: 'I think we both want what's best for the team. Let's understand each other's concerns.'\n   - Actually LISTENED - turned out his concern was: 'Our mobile app team will have to make 10+ REST calls for one screen'\n\n2. **Joint problem-solving:**\n   - Agreed on criteria together: Performance, maintainability, team familiarity\n   - Scored both approaches objectively\n   - Realized we were optimizing for different things (I for backend simplicity, he for mobile experience)\n\n3. **Hybrid solution:**\n   - I proposed: 'What if we use REST but add a BFF (Backend for Frontend) layer with aggregated endpoints for mobile?'\n   - Mark loved this because it solved his pain point\n   - We co-authored the design doc\n\n4. **Team involvement:**\n   - Presented the hybrid approach together to the team\n   - Gave Mark credit for the BFF idea publicly\n   - Made it clear: 'This is OUR solution, not mine or Mark's'\"\n\n**Result:**\n\"Project unblocked immediately:\n- Team velocity increased 40% (no more architecture debates)\n- Mark and I became close collaborators - he's now my go-to for difficult problems\n- The hybrid approach worked great - mobile team's API call count dropped 60%\n- We presented this case study in engineering all-hands as a model for conflict resolution\n\nKey learning: Conflicts often come from optimizing for different stakeholders. Making it a shared problem (not my solution vs yours) unlocks creativity.\"\n\n---\n\n## \ud83d\ude80 VALUE 5: BE THE CHANGE YOU SEEK\n\n### Common Questions:\n\n**Q9: Tell me about a time you identified a problem and drove a solution without being asked.**\n\n**Q10: Describe a situation where you took initiative beyond your job description.**\n\n**Example Answer (Q9):**\n\n**Situation:**\n\"I noticed our team's deployment frequency had dropped from daily to once a week. This wasn't explicitly my problem - I was an IC engineer, not DevOps. But it was affecting everyone's productivity. Deployments took 3+ hours due to manual steps, so people batched changes and delayed deploys.\"\n\n**Task:**\n\"Nobody owned this problem. I decided to take initiative and fix it.\"\n\n**Action:**\n\"I drove change proactively:\n\n1. **Quantified the problem:**\n   - Surveyed team: 8 out of 10 engineers said deployment pain was their #1 blocker\n   - Calculated cost: 3 hours \u00d7 5 engineers \u00d7 4 deployments/month = 60 hours wasted\n\n2. **Proposed solution:**\n   - Created a 1-page RFC: Automate deployment with GitHub Actions\n   - Showed examples from other teams who'd done this\n   - Estimated 40 hours of work (2 sprints)\n\n3. **Got buy-in:**\n   - Pitched to manager: 'I'll dedicate 50% time for 2 sprints to fix this for the team'\n   - Manager approved but asked: 'Who'll maintain it?'\n   - I volunteered to be on-call for deployment issues for 3 months\n\n4. **Execution:**\n   - Built automated pipeline with:\n     * Automated tests\n     * One-click rollback\n     * Deployment notifications in Slack\n   - Documented everything in wiki\n   - Ran training sessions for the team\n\n5. **Sustained the change:**\n   - Created a rotation: Each sprint, one person owns deployments\n   - Set up monitoring/alerts\n   - After 3 months, handed over ownership to DevOps team\"\n\n**Result:**\n\"Transformation in 2 months:\n- Deployment time: 3 hours \u2192 15 minutes (92% reduction)\n- Deployment frequency: Weekly \u2192 Daily (7x increase)\n- Zero production incidents due to automation (previously 2-3 per month)\n- Team satisfaction score (quarterly survey) went from 6/10 to 9/10\n- I was promoted 6 months later - manager cited this as evidence of 'initiative and impact'\n\nThis taught me: Don't wait for permission to solve problems. If you see something broken, fix it (with stakeholder buy-in, of course).\"\n\n---\n\n## \ud83c\udfaf TIPS FOR SUCCESS\n\n### \u2705 DO:\n1. **Prepare 10-15 stories** covering all 5 values\n2. **Use STAR format** religiously\n3. **Quantify impact** with numbers/metrics\n4. **Be honest** - Don't fabricate stories\n5. **Show vulnerability** - Share failures and learnings\n6. **Mention team** - It's about \"we,\" not just \"I\"\n7. **Be specific** - Names, dates, metrics (not vague)\n\n### \u274c DON'T:\n1. \u274c Bash former employers/colleagues\n2. \u274c Take full credit (always mention team)\n3. \u274c Give vague answers (no STAR)\n4. \u274c Ramble for 10 minutes\n5. \u274c Focus only on technical - show empathy/leadership\n6. \u274c Contradict yourself across rounds\n\n---\n\n## \ud83d\udcda PREPARATION CHECKLIST\n\n- [ ] Read [Atlassian Values Guide](https://www.atlassian.com/company/values)\n- [ ] Prepare 3 stories per value (15 total)\n- [ ] Write out full STAR for each story\n- [ ] Practice with friend/mock interview\n- [ ] Get comfortable saying \"I don't know\" if asked something you haven't experienced\n- [ ] Prepare 2-3 questions to ask interviewer about culture\n\n---\n\n**Next:** [06_Managerial_Round.md](./06_Managerial_Round.md)\n**Back to:** [README.md](./README.md)\n"
  },
  {
    "type": "file",
    "name": "06_Managerial_Round.md",
    "content": "# \ud83d\udc54 MANAGERIAL ROUND - Complete Guide\n\n**Duration:** 45-60 minutes\n**Format:** Leadership & Project Management Questions\n**Difficulty:** Medium-Hard\n**For:** P50+ (Senior) levels especially\n\n---\n\n## \ud83d\udccb FOCUS AREAS\n\n1. **Project Leadership** (40%)\n2. **People Management** (30%)\n3. **Technical Excellence** (20%)\n4. **Career & Motivation** (10%)\n\n---\n\n## \ud83c\udfaf COMMON QUESTIONS\n\n### **CATEGORY 1: PROJECT LEADERSHIP**\n\n#### **Q1: Tell me about the most complex project you've led.**\n\n**What they're evaluating:**\n- Scope/complexity of projects you handle\n- Your role in driving success\n- How you handle challenges\n\n**Example Answer Structure:**\n```\nSituation:\n- Project: [Name and context]\n- Scale: [Team size, timeline, business impact]\n- Complexity: [Why it was hard]\n\nTask:\n- Your role: [Tech lead, architect, etc.]\n- Key responsibilities\n\nAction:\n- Planning: How you broke down complexity\n- Execution: Key decisions you made\n- Challenges: What went wrong, how you adapted\n- Stakeholders: How you managed expectations\n\nResult:\n- Delivered: [Timeline, scope]\n- Impact: [User/business metrics]\n- Learning: [What you'd do differently]\n```\n\n---\n\n#### **Q2: How do you handle vague or changing requirements?**\n\n**Strong Answer Points:**\n- Clarification process with stakeholders\n- MVP approach to reduce risk\n- Iterative delivery with feedback loops\n- Documentation of decisions/assumptions\n- Communication strategy when changes happen\n\n**Example:**\n\"In my last project, we were building a recommendation engine. Initial requirement was simply 'users should see relevant content.' Rather than building in a vacuum:\n\n1. Clarified success metrics: What's 'relevant'? \u2192 Defined as CTR >5% and time-on-page >2min\n2. Built lightweight prototype in 2 weeks with simple rules-based logic\n3. Gathered data & user feedback\n4. Iterated with ML-based approach only after proving value\n\nWhen requirements changed mid-project (pivot from content to product recommendations), we:\n- Documented impact analysis: 3 weeks additional work\n- Proposed phased delivery: Ship content first, products in v2\n- Got stakeholder buy-in before proceeding\n\nResult: Delivered content recommendations on time, products followed 6 weeks later.\"\n\n---\n\n### **CATEGORY 2: PEOPLE MANAGEMENT**\n\n#### **Q3: How do you grow junior engineers on your team?**\n\n**Key Areas to Cover:**\n- Mentorship approach\n- Technical vs soft skills development\n- Giving ownership/responsibility\n- Feedback mechanisms\n\n**Example:**\n\"My mentorship philosophy has 3 pillars:\n\n**1. Structured Learning:**\n- Pair programming 2x/week on complex features\n- Code review with explanations (not just 'change this')\n- Weekly 30-min deep-dives on system architecture\n\n**2. Gradual Ownership:**\n- Sprint 1: Shadow me on feature design\n- Sprint 2: Co-design with my guidance\n- Sprint 3: Lead design, I review\n- Sprint 4: Full ownership with async check-ins\n\n**3. Psychological Safety:**\n- Share my own mistakes openly ('I once took down production by...')\n- 'No stupid questions' policy - I ask 'dumb' questions first\n- Celebrate learning, not just shipping\n\n**Example:**\nJunior engineer Sarah joined, struggled with system design. I:\n- Had her document current system (learn by explaining)\n- Gave her a small feature end-to-end (ownership)\n- Paired on design review (teaching by showing)\n- After 6 months, she led design for a major feature independently\n\nHer confidence grew from 'afraid to speak in meetings' to 'explaining architecture to leadership.'\"\n\n---\n\n#### **Q4: Describe a time you gave constructive criticism.**\n\n**Framework:**\n- Situation: Performance/quality issue\n- Preparation: Specific examples, not vague\n- Delivery: Private, empathetic, solution-focused\n- Follow-up: Support and track improvement\n\n---\n\n### **CATEGORY 3: TECHNICAL EXCELLENCE**\n\n#### **Q5: How do you ensure code quality on a team with varying skill levels?**\n\n**Strong Answer:**\n\"Multi-layered approach:\n\n**1. Preventive (Build Quality In):**\n- Coding standards documented in wiki\n- Linters/formatters in pre-commit hooks\n- Architecture decision records (ADRs) for big decisions\n\n**2. Detective (Catch Issues Early):**\n- Mandatory code reviews (2 approvals for critical paths)\n- Automated testing: 80% coverage minimum\n- Sonar/CodeClimate for static analysis\n\n**3. Supportive (Help People Improve):**\n- Code review guidelines: 'Explain WHY, not just WHAT'\n- Weekly tech talks: Seniors share patterns\n- Pair programming budget: 4 hours/week for juniors\n\n**4. Culture:**\n- 'Beginner's mind' retrospectives: What's confusing about our code?\n- Refactoring sprints: 20% time for tech debt\n- Blameless post-mortems: Learn from incidents\n\n**Metrics I track:**\n- PR cycle time (goal: <24hrs)\n- Review comments per PR (sweet spot: 3-5)\n- Production incidents (trend down over time)\n\n**Example:**\nTeam had 8 engineers (2 senior, 6 mid/junior). Code quality was inconsistent. After implementing above:\n- Test coverage: 40% \u2192 82% in 6 months\n- Production bugs: 15/month \u2192 3/month\n- PR turnaround: 2-3 days \u2192 same-day\n- Junior engineers started catching senior engineers' bugs!\"\n\n---\n\n#### **Q6: How do you prioritize technical debt vs new features?**\n\n**Framework:**\n- Quantify tech debt impact (velocity, bugs, morale)\n- Make business case (not just 'code is messy')\n- Allocate percentage (e.g., 20% sprint capacity)\n- Track ROI of tech debt work\n\n**Example:**\n\"I use a 'Tech Debt Tax' model:\n\n**Step 1: Quantify:**\n- Tracked that legacy auth system caused:\n  * 40% of our production incidents\n  * 3 hours/week of engineer time debugging\n  * Blocked 2 new features due to coupling\n\n**Step 2: Business Case to PM:**\n- 'Refactoring auth will cost 4 sprint weeks'\n- 'But save 12 hours/month ongoing (144 hours/year = $50K)'\n- 'Plus unblock 2 features worth $500K ARR'\n- ROI is clear\n\n**Step 3: Execution:**\n- 70/30 rule: 70% features, 30% tech debt\n- Tech debt visible on roadmap (not shadow work)\n- Celebrate tech debt wins like feature launches\n\n**Result:**\n- Refactored auth system over 3 months\n- Production incidents dropped 60%\n- Team velocity increased 25% (less firefighting)\n- PM became advocate for tech debt time\"\n\n---\n\n### **CATEGORY 4: CAREER & MOTIVATION**\n\n#### **Q7: Why are you looking to leave your current company?**\n\n**\u26a0\ufe0f BE CAREFUL: Don't bash current employer!**\n\n**Good Answers (Focus on PULL, not PUSH):**\n- \"Seeking bigger scale/impact\"\n- \"Want to work on [specific domain/tech] that Atlassian does well\"\n- \"Growth opportunities align with my career goals\"\n\n**Avoid:**\n- \u274c \"My manager sucks\"\n- \u274c \"Politics / bureaucracy\"\n- \u274c \"Underpaid\" (only discuss comp if asked)\n\n**Example:**\n\"I've grown a lot at Current Company - learned [X, Y, Z]. However, I'm looking for:\n\n1. **Greater Technical Challenge:**\n   - Currently working with 10K users; want to operate at 10M+ scale\n   - Atlassian's distributed systems work excites me\n\n2. **Broader Impact:**\n   - Want to influence product direction, not just execution\n   - P50 role offers that scope\n\n3. **Team/Culture:**\n   - Atlassian's 'Open Company, No Bullshit' resonates with my values\n   - Heard great things from [friend who works there]\n\nI'm grateful for my current role, but ready for the next level of challenge.\"\n\n---\n\n#### **Q8: Where do you see yourself in 5 years?**\n\n**What they want to hear:**\n- Alignment with career ladder (IC vs management)\n- Ambition but grounded\n- Interest in Atlassian specifically\n\n**Example (IC track):**\n\"In 5 years, I see myself as a Staff/Principal Engineer (IC track):\n\n**Technical Leadership:**\n- Architecting large-scale distributed systems\n- Mentoring senior engineers\n- Setting technical direction for a product area\n\n**Staying hands-on:**\n- I love coding and want to remain close to the code\n- But influencing more broadly through design, mentorship, standards\n\n**Why Atlassian aligns:**\n- Your IC track goes to Principal+ (some companies force management)\n- Work on products I use daily (Jira, Confluence)\n- Opportunity to work on different products over time\n\n**Flexibility:**\n- Open to management if it's the right fit\n- But currently energized by deep technical problems\"\n\n---\n\n#### **Q9: What's your management style? (If applying for EM role)**\n\n**Framework:**\n- Servant leadership\n- Empower, don't micromanage\n- Clear expectations + trust\n- Regular feedback, not just reviews\n\n**Example:**\n\"My management philosophy: 'Set direction, remove obstacles, celebrate wins.'\n\n**1. Clear Goals:**\n- OKRs at team and individual level\n- Weekly 1:1s to track progress and unblock\n\n**2. Autonomy:**\n- I don't prescribe HOW, only WHAT and WHY\n- Juniors get more structure; seniors get more freedom\n\n**3. Growth:**\n- Career development plans (updated quarterly)\n- Sponsorship: I advocate for promotions actively\n\n**4. Feedback:**\n- Weekly 1:1s include feedback (not just project updates)\n- 360 reviews: I ask my team to review ME\n\n**Example:**\nAs manager of 6 engineers:\n- 2 promoted in 12 months\n- Retention: 100% over 2 years\n- Team NPS: 9/10 in engagement surveys\n\n**My weakness:**\n- Sometimes I jump in to solve problems myself (engineering background)\n- Working on coaching more, solving less\"\n\n---\n\n## \ud83c\udfaf QUESTIONS TO ASK INTERVIEWER\n\n### **Smart Questions:**\n\n1. **Team Dynamics:**\n   - \"How does this team collaborate with [Product/Design/Other Engineering teams]?\"\n   - \"What's the team's biggest challenge right now?\"\n\n2. **Technical:**\n   - \"What's the tech stack? Any plans to modernize?\"\n   - \"How do you balance tech debt vs features?\"\n\n3. **Culture:**\n   - \"How do you live the value 'Open Company, No Bullshit' in practice?\"\n   - \"What does career growth look like for this role?\"\n\n4. **Impact:**\n   - \"What would success look like for this role in the first 6 months?\"\n   - \"What's the biggest impact I could have?\"\n\n### **Avoid:**\n- \u274c Questions with obvious answers (Google-able)\n- \u274c \"What does your company do?\" (should know this!)\n- \u274c Only comp/benefits questions (ask recruiter)\n\n---\n\n## \u2705 SUCCESS CHECKLIST\n\n**Before Interview:**\n- [ ] Prepare 5 project stories (with metrics)\n- [ ] Think about management philosophy\n- [ ] Review Atlassian products (use them if possible)\n- [ ] Prepare questions for interviewer\n\n**During Interview:**\n- [ ] Use STAR format\n- [ ] Quantify impact with numbers\n- [ ] Show empathy and people skills (not just tech)\n- [ ] Be honest about weaknesses\n- [ ] Take notes on questions\n\n**Red Flags to Avoid:**\n- \u274c \"I\" statements only (no \"we\")\n- \u274c Blaming others for failures\n- \u274c No self-awareness about mistakes\n- \u274c Can't answer \"What would you do differently?\"\n\n---\n\n**Next:** [07_Preparation_Checklist.md](./07_Preparation_Checklist.md)\n**Back to:** [README.md](./README.md)\n"
  },
  {
    "type": "file",
    "name": "07_Preparation_Checklist.md",
    "content": "# \u2705 PREPARATION CHECKLIST & STUDY PLAN\n\nComplete roadmap to prepare for Atlassian interviews\n\n---\n\n## \ud83c\udfaf RECOMMENDED TIMELINE\n\n### **Minimum:** 4-6 weeks\n### **Ideal:** 8-12 weeks\n### **Last Minute:** 2 weeks (focus on most frequent questions)\n\n---\n\n## \ud83d\udcc5 WEEK-BY-WEEK STUDY PLAN\n\n### **WEEK 1-2: DSA FOUNDATION**\n\n**Focus:** Master the most repeated patterns\n\n#### \u2705 **Day 1-3: Employee Hierarchy (LCA)**\n- [ ] Solve [LeetCode 236 - LCA Binary Tree](https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/)\n- [ ] Solve [LeetCode 1650 - LCA III](https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree-iii/)\n- [ ] Implement N-ary tree LCA\n- [ ] Practice all follow-ups from file `02_Data_Structures_Round.md`\n\n#### \u2705 **Day 4-5: Content Popularity / All O(1)**\n- [ ] Solve [LeetCode 432 - All O`one Data Structure](https://leetcode.com/problems/all-oone-data-structure/)\n- [ ] Solve [LeetCode 460 - LFU Cache](https://leetcode.com/problems/lfu-cache/)\n- [ ] Understand doubly linked list + HashMap pattern\n\n#### \u2705 **Day 6-7: Meeting Rooms / Interval Problems**\n- [ ] Solve [LeetCode 253 - Meeting Rooms II](https://leetcode.com/problems/meeting-rooms-ii/)\n- [ ] Solve [LeetCode 56 - Merge Intervals](https://leetcode.com/problems/merge-intervals/)\n- [ ] Solve [LeetCode 435 - Non-overlapping Intervals](https://leetcode.com/problems/non-overlapping-intervals/)\n\n#### \u2705 **Day 8-10: Stock Price / TreeMap Problems**\n- [ ] Solve [LeetCode 2034 - Stock Price Fluctuation](https://leetcode.com/problems/stock-price-fluctuation/)\n- [ ] Practice SortedList/TreeMap operations\n- [ ] Learn when to use TreeMap vs Heap\n\n#### \u2705 **Day 11-14: Misc Patterns**\n- [ ] Trie: [LeetCode 208](https://leetcode.com/problems/implement-trie-prefix-tree/)\n- [ ] Graph BFS: [LeetCode 207 - Course Schedule](https://leetcode.com/problems/course-schedule/)\n- [ ] HashMaps: [LeetCode 1 - Two Sum](https://leetcode.com/problems/two-sum/)\n- [ ] Text problems: [LeetCode 68 - Text Justification](https://leetcode.com/problems/text-justification/)\n\n---\n\n### **WEEK 3: CODE DESIGN (LLD)**\n\n**Focus:** Snake Game + Design Patterns\n\n#### \u2705 **Day 1-4: Snake Game**\n- [ ] Implement Snake Game from scratch (file `03_Code_Design_LLD_Round.md`)\n- [ ] Add all follow-ups:\n  - [ ] Food spawning\n  - [ ] Multiple snakes\n  - [ ] Obstacles\n- [ ] Write unit tests\n- [ ] Practice explaining design decisions\n\n#### \u2705 **Day 5-6: Cost Explorer / Subscription System**\n- [ ] Implement subscription billing calculator\n- [ ] Handle different tiers\n- [ ] Monthly/yearly cost calculations\n- [ ] Practice OOP design\n\n#### \u2705 **Day 7: Design Patterns**\n- [ ] Learn these patterns:\n  - Strategy Pattern\n  - Factory Pattern\n  - Observer Pattern\n  - Singleton (and why it's often bad!)\n- [ ] Practice applying them in code\n\n---\n\n### **WEEK 4: SYSTEM DESIGN (HLD)**\n\n**Focus:** Tagging System + Fundamentals\n\n#### \u2705 **Day 1-3: Tagging Management System**\n- [ ] Design from scratch (file `04_System_Design_HLD_Round.md`)\n- [ ] API design\n- [ ] Database schema (SQL and NoSQL)\n- [ ] Caching strategy\n- [ ] Sharding approach\n- [ ] Practice on whiteboard / diagram tool\n\n#### \u2705 **Day 4: Fundamentals**\n- [ ] Load Balancing (Round Robin, Consistent Hashing)\n- [ ] Caching (Redis patterns, Cache invalidation)\n- [ ] Database indexing\n- [ ] SQL vs NoSQL trade-offs\n\n#### \u2705 **Day 5: Scalability Patterns**\n- [ ] Horizontal vs Vertical Scaling\n- [ ] Database Sharding\n- [ ] Replication (Primary-Replica)\n- [ ] CDN usage\n\n#### \u2705 **Day 6-7: Practice Other Systems**\n- [ ] Web Scraper design\n- [ ] URL Shortener\n- [ ] Rate Limiter\n- [ ] Twitter Feed\n\n---\n\n### **WEEK 5: BEHAVIORAL PREP**\n\n**Focus:** Atlassian Values + STAR Stories\n\n#### \u2705 **Day 1-2: Values Study**\n- [ ] Read [Atlassian Values Guide](https://www.atlassian.com/company/values)\n- [ ] Watch Atlassian culture videos\n- [ ] Understand what each value means in practice\n\n#### \u2705 **Day 3-5: Story Preparation**\nPrepare 3 stories for EACH value (15 total):\n\n**Template for Each Story:**\n```markdown\n## Story: [Short Title]\n**Value:** [Which value this demonstrates]\n**Situation:**\n- Context: [Company, team, timeline]\n- Challenge: [What was the problem]\n\n**Task:**\n- Your role: [Your responsibility]\n- Goal: [What needed to be achieved]\n\n**Action:**\n- Step 1: [What you did]\n- Step 2: [Next action]\n- Step 3: [And so on...]\n\n**Result:**\n- Outcome: [What happened]\n- Metrics: [Quantifiable impact]\n- Learning: [What you learned]\n```\n\n- [ ] Write out 15 full stories\n- [ ] Each story should be 2-3 minutes when spoken\n- [ ] Include specific names, dates, metrics\n\n#### \u2705 **Day 6-7: Practice**\n- [ ] Practice with friend/mock interviewer\n- [ ] Record yourself and listen back\n- [ ] Time yourself (should be ~2.5 min per story)\n\n---\n\n### **WEEK 6: MOCK INTERVIEWS & REFINEMENT**\n\n#### \u2705 **Mock Interview Schedule**\n- [ ] **Monday:** DSA Mock (1 hour)\n  - Employee Hierarchy problem\n  - Content Popularity problem\n  \n- [ ] **Tuesday:** Code Design Mock (1 hour)\n  - Snake Game or similar\n  \n- [ ] **Wednesday:** System Design Mock (1 hour)\n  - Tagging system or Web Scraper\n  \n- [ ] **Thursday:** Behavioral Mock (45 min)\n  - 5 questions covering all values\n  \n- [ ] **Friday:** Full Loop Mock\n  - Karat screening (60 min)\n  - DSA (60 min)\n  - Break\n  - Code Design (60 min)\n  - Break\n  - System Design (60 min)\n\n#### \u2705 **Refinement**\n- [ ] Review all mistakes from mocks\n- [ ] Redo any questions you struggled with\n- [ ] Polish behavioral stories\n- [ ] Prepare questions for interviewer\n\n---\n\n## \ud83d\udcda RESOURCE LIST\n\n### **Books**\n- [ ] \"Cracking the Coding Interview\" - Gayle Laakmann McDowell\n- [ ] \"System Design Interview Vol 1 & 2\" - Alex Xu\n- [ ] \"Designing Data-Intensive Applications\" - Martin Kleppmann\n\n### **Online Courses**\n- [ ] [Grokking the System Design Interview](https://www.educative.io/courses/grokking-the-system-design-interview)\n- [ ] [Grokking the Coding Interview](https://www.educative.io/courses/grokking-the-coding-interview)\n- [ ] [SystemsExpert by AlgoExpert](https://www.algoexpert.io/systems/product)\n\n### **YouTube Channels**\n- [ ] [Gaurav Sen - System Design](https://www.youtube.com/c/GauravSensei)\n- [ ] [ByteByteGo](https://www.youtube.com/c/ByteByteGo)\n- [ ] [NeetCode - DSA](https://www.youtube.com/c/NeetCode)\n\n### **Websites**\n- [ ] [LeetCode Atlassian Tag](https://leetcode.com/company/atlassian/)\n- [ ] [AlgoExpert](https://www.algoexpert.io/)\n- [ ] [Pramp - Mock Interviews](https://www.pramp.com/)\n\n---\n\n## \ud83c\udfaf LEETCODE PROBLEM LIST (Priority Order)\n\n### **MUST DO (Top 20)**\n\n#### **Trees & Graphs**\n1. \u2b50\u2b50\u2b50\u2b50\u2b50 [236. LCA Binary Tree](https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/)\n2. \u2b50\u2b50\u2b50\u2b50\u2b50 [1650. LCA III](https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree-iii/)\n3. \u2b50\u2b50\u2b50 [133. Clone Graph](https://leetcode.com/problems/clone-graph/)\n4. \u2b50\u2b50\u2b50 [207. Course Schedule](https://leetcode.com/problems/course-schedule/)\n\n#### **Design / HashMap**\n5. \u2b50\u2b50\u2b50\u2b50\u2b50 [432. All O(1) Data Structure](https://leetcode.com/problems/all-oone-data-structure/)\n6. \u2b50\u2b50\u2b50\u2b50 [460. LFU Cache](https://leetcode.com/problems/lfu-cache/)\n7. \u2b50\u2b50\u2b50\u2b50 [146. LRU Cache](https://leetcode.com/problems/lru-cache/)\n8. \u2b50\u2b50\u2b50\u2b50 [2034. Stock Price Fluctuation](https://leetcode.com/problems/stock-price-fluctuation/)\n\n#### **Intervals**\n9. \u2b50\u2b50\u2b50\u2b50 [253. Meeting Rooms II](https://leetcode.com/problems/meeting-rooms-ii/)\n10. \u2b50\u2b50\u2b50 [56. Merge Intervals](https://leetcode.com/problems/merge-intervals/)\n11. \u2b50\u2b50\u2b50 [435. Non-overlapping Intervals](https://leetcode.com/problems/non-overlapping-intervals/)\n\n#### **Trie / Strings**\n12. \u2b50\u2b50\u2b50\u2b50 [208. Implement Trie](https://leetcode.com/problems/implement-trie-prefix-tree/)\n13. \u2b50\u2b50\u2b50 [68. Text Justification](https://leetcode.com/problems/text-justification/)\n14. \u2b50\u2b50\u2b50 [1160. Find Words](https://leetcode.com/problems/find-words-that-can-be-formed-by-characters/)\n\n#### **Heaps**\n15. \u2b50\u2b50\u2b50 [295. Find Median from Data Stream](https://leetcode.com/problems/find-median-from-data-stream/)\n16. \u2b50\u2b50\u2b50 [347. Top K Frequent Elements](https://leetcode.com/problems/top-k-frequent-elements/)\n\n#### **Matrix / 2D**\n17. \u2b50\u2b50\u2b50 [200. Number of Islands](https://leetcode.com/problems/number-of-islands/)\n18. \u2b50\u2b50\u2b50 [79. Word Search](https://leetcode.com/problems/word-search/)\n\n#### **Misc**\n19. \u2b50\u2b50\u2b50 [49. Group Anagrams](https://leetcode.com/problems/group-anagrams/)\n20. \u2b50\u2b50\u2b50 [127. Word Ladder](https://leetcode.com/problems/word-ladder/)\n\n---\n\n### **GOOD TO DO (Next 15)**\n\n21. [621. Task Scheduler](https://leetcode.com/problems/task-scheduler/)\n22. [380. Insert Delete GetRandom O(1)](https://leetcode.com/problems/insert-delete-getrandom-o1/)\n23. [729. My Calendar I](https://leetcode.com/problems/my-calendar-i/)\n24. [588. Design In-Memory File System](https://leetcode.com/problems/design-in-memory-file-system/)\n25. [355. Design Twitter](https://leetcode.com/problems/design-twitter/)\n26. [297. Serialize Deserialize Binary Tree](https://leetcode.com/problems/serialize-and-deserialize-binary-tree/)\n27. [23. Merge K Sorted Lists](https://leetcode.com/problems/merge-k-sorted-lists/)\n28. [42. Trapping Rain Water](https://leetcode.com/problems/trapping-rain-water/)\n29. [128. Longest Consecutive Sequence](https://leetcode.com/problems/longest-consecutive-sequence/)\n30. [76. Minimum Window Substring](https://leetcode.com/problems/minimum-window-substring/)\n31. [438. Find All Anagrams](https://leetcode.com/problems/find-all-anagrams-in-a-string/)\n32. [621. Task Scheduler](https://leetcode.com/problems/task-scheduler/)\n33. [535. Encode and Decode TinyURL](https://leetcode.com/problems/encode-and-decode-tinyurl/)\n34. [895. Maximum Frequency Stack](https://leetcode.com/problems/maximum-frequency-stack/)\n35. [535. Encode and Decode TinyURL](https://leetcode.com/problems/encode-and-decode-tinyurl/)\n\n---\n\n## \ud83d\udd25 FINAL WEEK CHECKLIST\n\n### **3 Days Before:**\n- [ ] Review all 6 round files in this repo\n- [ ] Do 1 mock of each round type\n- [ ] Finalize behavioral stories\n- [ ] Prepare 5 questions for each round\n\n### **1 Day Before:**\n- [ ] Light review only (don't cram!)\n- [ ] Re-read Atlassian values\n- [ ] Prepare your setup:\n  - [ ] Laptop charged\n  - [ ] Good internet connection\n  - [ ] Quiet environment\n  - [ ] Whiteboard / paper for sketching\n- [ ] Get good sleep!\n\n### **Interview Day:**\n- [ ] Morning review (30 min max)\n- [ ] Warm-up: Solve 1 easy LC problem\n- [ ] Stay hydrated\n- [ ] Take breaks between rounds\n- [ ] Stay positive - even if one round goes badly!\n\n---\n\n## \ud83d\udcca PROGRESS TRACKER\n\n### **DSA Practice (Track Completion)**\n\n| Problem | Status | Date | Notes |\n|---------|--------|------|-------|\n| LeetCode 236 - LCA | \u2b1c | | |\n| LeetCode 1650 - LCA III | \u2b1c | | |\n| LeetCode 432 - All O(1) | \u2b1c | | |\n| LeetCode 460 - LFU Cache | \u2b1c | | |\n| LeetCode 253 - Meeting Rooms II | \u2b1c | | |\n| LeetCode 2034 - Stock Price | \u2b1c | | |\n| Snake Game Implementation | \u2b1c | | |\n\n### **System Design Practice**\n\n| Topic | Completed | Date |\n|-------|-----------|------|\n| Tagging System | \u2b1c | |\n| Web Scraper | \u2b1c | |\n| Rate Limiter | \u2b1c | |\n| URL Shortener | \u2b1c | |\n\n### **Behavioral Stories**\n\n| Value | Stories Ready | Count |\n|-------|---------------|-------|\n| Open Company | \u2b1c\u2b1c\u2b1c | 0/3 |\n| Heart & Balance | \u2b1c\u2b1c\u2b1c | 0/3 |\n| Don't Fuck Customer | \u2b1c\u2b1c\u2b1c | 0/3 |\n| Play as Team | \u2b1c\u2b1c\u2b1c | 0/3 |\n| Be the Change | \u2b1c\u2b1c\u2b1c | 0/3 |\n\n### **Mock Interviews**\n\n| Round Type | Mock 1 | Mock 2 | Mock 3 |\n|------------|--------|--------|--------|\n| Karat | \u2b1c | \u2b1c | \u2b1c |\n| DSA | \u2b1c | \u2b1c | \u2b1c |\n| Code Design | \u2b1c | \u2b1c | \u2b1c |\n| System Design | \u2b1c | \u2b1c | \u2b1c |\n| Behavioral | \u2b1c | \u2b1c | \u2b1c |\n\n---\n\n## \ud83d\udcaa MOTIVATION\n\n**Remember:**\n- Atlassian interview is thorough but fair\n- Every round is important (don't skip behavioral prep!)\n- Practice is key - especially for Employee Hierarchy and Snake Game\n- Stay calm, ask clarifying questions, and think out loud\n\n**You've got this! \ud83d\ude80**\n\n---\n\n**Back to:** [README.md](./README.md)\n"
  },
  {
    "type": "directory",
    "name": "Data_Structures",
    "children": [
      {
        "type": "file",
        "name": "01_Employee_Hierarchy.md",
        "content": "# \ud83c\udf1f PROBLEM 1: EMPLOYEE HIERARCHY\n\n### \u2b50\u2b50\u2b50\u2b50\u2b50 **Find Closest Department for Employees**\n\n**Frequency:** Appears in **60%** of Atlassian DSA rounds!\n**Difficulty:** Medium\n\n---\n\n## \ud83d\udccb Problem Statement\n\nYou maintain the Atlassian employee directory. The company has multiple groups (departments), and each group can have one or more sub-groups. Every employee belongs to exactly one group (in the base version).\n\n**Task:** Design a system that finds the **closest common parent group** given a set of employee names.\n\n**Constraints:**\n- 1 \u2264 Number of employees \u2264 10,000\n- 1 \u2264 Number of groups \u2264 1,000\n- Tree height \u2264 20\n- Employee and group names are unique strings\n\n---\n\n## \ud83c\udfa8 Visual Example\n\n```text\nOrganization Hierarchy:\n\n                    Company (Root)\n                   /      |      \\\n              Engg       HR      Sales\n             /  |  \\              / \\\n     Backend Frontend Mobile  North South\n      /  \\       |              |     |\n  Alice  Bob   Lisa          David  Eve\n```\n\n**Path Representation:**\n- Alice: `[\"Company\", \"Engg\", \"Backend\", \"Alice\"]`\n- Bob: `[\"Company\", \"Engg\", \"Backend\", \"Bob\"]`\n- Lisa: `[\"Company\", \"Engg\", \"Frontend\", \"Lisa\"]`\n- David: `[\"Company\", \"Sales\", \"North\", \"David\"]`\n\n---\n\n## \ud83d\udca1 Examples\n\n### Example 1: Same Direct Parent\n```python\nInput: [\"Alice\", \"Bob\"]\nOutput: \"Backend\"\nExplanation: Both employees are directly under Backend group.\n```\n\n### Example 2: Different Sub-departments\n```python\nInput: [\"Alice\", \"Lisa\"]\nOutput: \"Engg\"\nExplanation: \n- Alice path: Company \u2192 Engg \u2192 Backend \u2192 Alice\n- Lisa path:  Company \u2192 Engg \u2192 Frontend \u2192 Lisa\n- Common prefix: Company, Engg\n- LCA: Engg (last common node)\n```\n\n### Example 3: Multiple Employees\n```python\nInput: [\"Alice\", \"Bob\", \"Lisa\"]\nOutput: \"Engg\"\nExplanation: All three are under Engineering department.\n```\n\n### Example 4: Different Top-Level Departments\n```python\nInput: [\"Alice\", \"David\"]\nOutput: \"Company\"\nExplanation: Only common ancestor is root.\n```\n\n### Example 5: Single Employee\n```python\nInput: [\"Alice\"]\nOutput: \"Backend\"\nExplanation: Return immediate parent group.\n```\n\n---\n\n## \ud83d\udde3\ufe0f Interview Conversation Guide\n\n### Phase 1: Clarification (3-5 min)\n\n**Candidate:** \"Can an employee belong to multiple groups?\"\n**Interviewer:** \"Let's start with the assumption that each employee belongs to exactly one group.\"\n\n**Candidate:** \"Is the input always a valid tree structure, or can there be cycles?\"\n**Interviewer:** \"It's a strict hierarchy (tree structure). No cycles.\"\n\n**Candidate:** \"What should I return if the input list is empty or contains invalid employees?\"\n**Interviewer:** \"Return `None` for empty input. Raise an error or return `None` for invalid employees.\"\n\n**Candidate:** \"Can I assume parent pointers are available, or do I need to build the tree first?\"\n**Interviewer:** \"You'll need to build the tree structure from the input data.\"\n\n**Candidate:** \"What's the expected scale? How many employees and groups?\"\n**Interviewer:** \"Assume up to 10,000 employees and 1,000 groups. Tree height won't exceed 20.\"\n\n### Phase 2: Approach Discussion (5-8 min)\n\n**Candidate:** \"This is a **Lowest Common Ancestor (LCA)** problem. We need to find a node that is an ancestor of all target employees and is the deepest such node.\"\n\n**Candidate:** \"I'm thinking of three possible approaches:\n1. **Naive Recursive:** Start from root, recursively check which subtrees contain all employees. O(N\u00b2) time.\n2. **Path Tracing:** Build paths from each employee to root, find common prefix. O(K \u00d7 H) time where K is number of employees and H is tree height.\n3. **Parent Pointers with Set Intersection:** Store all ancestors in sets, intersect them. Similar complexity but different implementation.\"\n\n**Candidate:** \"I'll go with **Path Tracing** because:\n- It's intuitive and easy to explain\n- Time complexity is optimal for this problem\n- Easy to debug and test\n- Works well with the tree structure we're building\"\n\n### Phase 3: Coding (15-20 min)\n\n**Candidate:** \"I'll implement this in three steps:\n1. Define the TreeNode structure\n2. Build the tree from input data\n3. Implement the LCA query using path comparison\"\n\n### Phase 4: Testing & Verification (5-7 min)\n\n**Candidate:** \"Let me walk through the example with Alice and Lisa:\n1. Find Alice node \u2192 Trace path: [Company, Engg, Backend, Alice]\n2. Find Lisa node \u2192 Trace path: [Company, Engg, Frontend, Lisa]\n3. Compare indices:\n   - Index 0: Company == Company \u2713\n   - Index 1: Engg == Engg \u2713\n   - Index 2: Backend \u2260 Frontend \u2717\n4. Last common: Engg \u2713\"\n\n---\n\n## \ud83e\udde0 Intuition & Approach\n\n### Why is this an LCA Problem?\n\nWe're looking for a **group (node)** that:\n1. Is an ancestor of ALL target employees (contains all of them in its subtree)\n2. Is the **lowest** (deepest/closest) such node in the hierarchy\n\nThis is precisely the definition of **Lowest Common Ancestor**.\n\n### Approach Comparison\n\n| Approach | Time | Space | Pros | Cons |\n|----------|------|-------|------|------|\n| **Naive Recursive** | O(N\u00b2) | O(H) | Simple concept | Too slow for large trees |\n| **Path Tracing** | O(K\u00d7H) | O(K\u00d7H) | Clear logic, optimal | Extra space for paths |\n| **Set Intersection** | O(K\u00d7H) | O(K\u00d7H) | Handles multi-group follow-up well | Slightly more complex |\n\n**Recommended:** Path Tracing for interviews (clearest explanation, optimal complexity)\n\n### Why Path Tracing Works\n\n**Key Insight:** In a tree, the path from any node to the root is unique. If two nodes share a common ancestor, their paths must overlap from the root up to that ancestor.\n\n**Visual Trace:**\n```text\nAlice path:  [Company, Engg, Backend, Alice]\n                 \u2193       \u2193      \u2193       \u2193\nLisa path:   [Company, Engg, Frontend, Lisa]\n                 \u2713       \u2713       \u2717       \u2717\n```\nLast matching position \u2192 **Engg**\n\n---\n\n## \ud83d\udcdd Solution Approach: Path Tracing with LCA\n\n### Algorithm Steps\n\n**Step 1:** Build the tree structure with parent pointers\n- Parse input data (nested dict or adjacency list)\n- Create TreeNode objects\n- Link parent-child relationships\n- Store nodes in a HashMap for O(1) lookup\n\n**Step 2:** For each employee, trace path to root\n- Start at employee node\n- Follow parent pointers until reaching root\n- Store path in array\n- Reverse array (to get root \u2192 employee direction)\n\n**Step 3:** Find longest common prefix of all paths\n- Compare paths element by element\n- Stop when paths diverge\n- Return last common element\n\n### Complete Implementation\n\n```python\nfrom typing import List, Dict, Optional\n\nclass TreeNode:\n    \"\"\"Represents a node in the organization hierarchy.\"\"\"\n    def __init__(self, name: str):\n        self.name = name\n        self.parent: Optional[TreeNode] = None\n        self.children: List[TreeNode] = []\n\nclass EmployeeDirectory:\n    \"\"\"\n    Main class to manage employee hierarchy and find closest common groups.\n    \n    Supports:\n    - Building hierarchy from nested dictionary\n    - Finding closest common group for a set of employees\n    - O(1) employee lookup\n    \"\"\"\n    \n    def __init__(self):\n        self.nodes: Dict[str, TreeNode] = {}\n        self.root: Optional[TreeNode] = None\n    \n    def build_from_dict(self, hierarchy: Dict) -> None:\n        \"\"\"\n        Build tree from nested dictionary structure.\n        \n        Args:\n            hierarchy: Nested dict like:\n                {\n                    \"Engg\": {\n                        \"Backend\": [\"Alice\", \"Bob\"],\n                        \"Frontend\": [\"Lisa\"]\n                    },\n                    \"HR\": [\"Charlie\"]\n                }\n        \n        Time: O(N) where N = total nodes\n        Space: O(N) for storing nodes\n        \"\"\"\n        # Create root\n        self.root = TreeNode(\"Company\")\n        self.nodes[\"Company\"] = self.root\n        \n        # Recursively build tree\n        self._build_recursive(hierarchy, self.root)\n    \n    def _build_recursive(self, data, parent: TreeNode) -> None:\n        \"\"\"Helper to recursively build tree.\"\"\"\n        if isinstance(data, dict):\n            # data is a dictionary of sub-groups\n            for name, children in data.items():\n                # Create group node\n                node = TreeNode(name)\n                node.parent = parent\n                parent.children.append(node)\n                self.nodes[name] = node\n                \n                # Recurse on children\n                self._build_recursive(children, node)\n                \n        elif isinstance(data, list):\n            # data is a list of employees (leaf nodes)\n            for emp_name in data:\n                emp_node = TreeNode(emp_name)\n                emp_node.parent = parent\n                parent.children.append(emp_node)\n                self.nodes[emp_name] = emp_node\n    \n    def find_closest_group(self, employees: List[str]) -> Optional[str]:\n        \"\"\"\n        Find the closest common parent group for given employees.\n        \n        Args:\n            employees: List of employee names\n            \n        Returns:\n            Name of closest common group, or None if not found\n            \n        Time: O(K \u00d7 H) where K = len(employees), H = tree height\n        Space: O(K \u00d7 H) for storing paths\n        \n        Raises:\n            ValueError: If any employee is not found\n        \"\"\"\n        # Edge case: empty input\n        if not employees:\n            return None\n        \n        # Edge case: single employee\n        if len(employees) == 1:\n            if employees[0] not in self.nodes:\n                raise ValueError(f\"Employee '{employees[0]}' not found\")\n            \n            emp_node = self.nodes[employees[0]]\n            # Return parent group (not the employee itself)\n            if emp_node.parent:\n                return emp_node.parent.name\n            return None\n        \n        # Step 1: Get paths for all employees\n        paths = []\n        for emp in employees:\n            if emp not in self.nodes:\n                raise ValueError(f\"Employee '{emp}' not found\")\n            \n            path = self._get_path_to_root(self.nodes[emp])\n            paths.append(path)\n        \n        # Step 2: Find longest common prefix\n        lca_name = self._find_common_prefix(paths)\n        \n        # Edge case: If LCA is an employee (shouldn't happen with valid input),\n        # return their parent\n        if lca_name in employees:\n            node = self.nodes[lca_name]\n            if node.parent:\n                return node.parent.name\n            return None\n        \n        return lca_name\n    \n    def _get_path_to_root(self, node: TreeNode) -> List[str]:\n        \"\"\"\n        Trace path from node to root.\n        \n        Time: O(H) where H = tree height\n        Space: O(H) for path storage\n        \"\"\"\n        path = []\n        current = node\n        \n        while current:\n            path.append(current.name)\n            current = current.parent\n        \n        # Reverse to get root \u2192 node direction\n        return path[::-1]\n    \n    def _find_common_prefix(self, paths: List[List[str]]) -> Optional[str]:\n        \"\"\"\n        Find the longest common prefix of all paths.\n        \n        Time: O(K \u00d7 H) where K = number of paths, H = avg path length\n        Space: O(1) excluding input\n        \"\"\"\n        if not paths:\n            return None\n        \n        min_len = min(len(p) for p in paths)\n        lca = None\n        \n        for i in range(min_len):\n            # Check if all paths have the same node at position i\n            first_node = paths[0][i]\n            \n            if all(path[i] == first_node for path in paths):\n                lca = first_node\n            else:\n                # Paths diverge here, stop\n                break\n        \n        return lca\n\n\n# ============================================\n# COMPLETE RUNNABLE EXAMPLE\n# ============================================\n\nif __name__ == \"__main__\":\n    # Build organization hierarchy\n    directory = EmployeeDirectory()\n    \n    hierarchy = {\n        \"Engg\": {\n            \"Backend\": [\"Alice\", \"Bob\"],\n            \"Frontend\": [\"Lisa\"],\n            \"Mobile\": [\"Mike\"]\n        },\n        \"HR\": [\"Charlie\"],\n        \"Sales\": {\n            \"North\": [\"David\"],\n            \"South\": [\"Eve\"]\n        }\n    }\n    \n    directory.build_from_dict(hierarchy)\n    \n    # Test cases\n    print(\"=\" * 50)\n    print(\"EMPLOYEE HIERARCHY - LCA FINDER\")\n    print(\"=\" * 50)\n    \n    test_cases = [\n        ([\"Alice\", \"Bob\"], \"Backend\"),\n        ([\"Alice\", \"Lisa\"], \"Engg\"),\n        ([\"Alice\", \"Bob\", \"Lisa\"], \"Engg\"),\n        ([\"Alice\", \"Charlie\"], \"Company\"),\n        ([\"David\", \"Eve\"], \"Sales\"),\n        ([\"Alice\"], \"Backend\"),\n        ([\"Mike\", \"Lisa\"], \"Engg\"),\n    ]\n    \n    for employees, expected in test_cases:\n        result = directory.find_closest_group(employees)\n        status = \"\u2713\" if result == expected else \"\u2717\"\n        print(f\"{status} Input: {employees}\")\n        print(f\"  Expected: {expected}, Got: {result}\")\n        print()\n    \n    # Show internal paths for debugging\n    print(\"\\n\" + \"=\" * 50)\n    print(\"PATH TRACING (for Alice and Lisa)\")\n    print(\"=\" * 50)\n    \n    alice_path = directory._get_path_to_root(directory.nodes[\"Alice\"])\n    lisa_path = directory._get_path_to_root(directory.nodes[\"Lisa\"])\n    \n    print(f\"Alice path: {' \u2192 '.join(alice_path)}\")\n    print(f\"Lisa path:  {' \u2192 '.join(lisa_path)}\")\n    print(f\"\\nCommon Prefix: \", end=\"\")\n    \n    for i in range(min(len(alice_path), len(lisa_path))):\n        if alice_path[i] == lisa_path[i]:\n            print(f\"{alice_path[i]}\", end=\"\")\n            if i < min(len(alice_path), len(lisa_path)) - 1:\n                print(\" \u2192 \", end=\"\")\n        else:\n            break\n    \n    print(f\"\\nLCA: {directory.find_closest_group(['Alice', 'Lisa'])}\")\n```\n\n---\n\n## \ud83d\udd0d Complexity Analysis\n\n### Time Complexity: **O(K \u00d7 H)**\n\n**Breakdown:**\n- **Building Tree:** O(N) where N = total nodes (employees + groups)\n  - We visit each node once during recursive construction\n- **Query (find_closest_group):**\n  - For K employees:\n    - Get path for each: O(H) per employee\n    - Total: O(K \u00d7 H)\n  - Find common prefix: O(K \u00d7 H)\n    - Compare up to H positions\n    - For each position, check K paths\n  - **Total Query:** O(K \u00d7 H)\n\n**Where:**\n- K = Number of employees in query\n- H = Height of organization tree (typically H \u226a N)\n- N = Total nodes in tree\n\n**Typical Values:**\n- Large company: N = 10,000, H = 10-15 (log scale)\n- Query: K = 2-5 employees\n- Time: ~20-75 comparisons (very fast!)\n\n### Space Complexity: **O(K \u00d7 H)**\n\n**Breakdown:**\n- **Tree Storage:** O(N) for nodes HashMap and TreeNode objects\n- **Query:**\n  - K paths, each of length \u2264 H: O(K \u00d7 H)\n  - Temporary variables: O(1)\n- **Total:** O(N + K \u00d7 H)\n\n**Optimization:** If memory is critical, we could avoid storing full paths by comparing on-the-fly (but code becomes more complex).\n\n---\n\n## \u26a0\ufe0f Common Pitfalls\n\n### 1. **Assuming Binary Tree**\n**Problem:** Using binary tree LCA algorithms (recursion with left/right checks).\n**Why it fails:** Organization is an **N-ary tree** (a manager can have many reports).\n**Fix:** Use path-based or iterative approaches that don't assume two children.\n\n### 2. **Not Reversing Path**\n**Problem:**\n```python\npath = []\nwhile current:\n    path.append(current.name)\n    current = current.parent\nreturn path  # \u274c Wrong order!\n```\n**Why it fails:** Path goes Employee \u2192 Root, but LCA comparison needs Root \u2192 Employee.\n**Fix:** `return path[::-1]`\n\n### 3. **Returning Employee Name Instead of Group**\n**Problem:** For input `[\"Alice\"]`, returning \"Alice\" instead of \"Backend\".\n**Why it fails:** Question asks for closest *group*, not the employee.\n**Fix:** Check if result is in employee list, return parent if so.\n\n### 4. **Not Handling Edge Cases**\n**Common issues:**\n- Empty input `[]` \u2192 Should return `None`\n- Single employee \u2192 Return their parent group\n- Non-existent employee \u2192 Should raise error or return `None`\n- Duplicate employees \u2192 Should handle gracefully\n\n### 5. **Forgetting O(1) Lookup**\n**Problem:** Searching for employees by iterating through tree each time.\n**Why it fails:** O(N) lookup makes total complexity O(K \u00d7 N \u00d7 H).\n**Fix:** Use HashMap (`self.nodes`) for O(1) employee lookup.\n\n---\n\n## \ud83d\udd04 Follow-up Questions\n\n### Follow-up 1: Employees in Multiple Groups\n\n**Problem Statement:**\n> \"Now employees can belong to multiple groups. For example, Alice is in both Backend and Mobile (she works part-time in both teams). How does your solution change?\"\n\n**Visual Example:**\n```text\nOrganization Structure:\n                    Company\n                   /      \\\n              Engg         Sales\n             /  |  \\\n     Backend Frontend Mobile\n        |       |       |\n      Alice   Lisa   Alice (same person!)\n        |             Mike\n       Bob\n       \nAlice is in TWO groups: Backend AND Mobile\n```\n\n**Modified Input:**\n```python\nemployee_to_groups = {\n    \"Alice\": [\"Backend\", \"Mobile\"],  # Alice in 2 groups\n    \"Bob\": [\"Backend\"],\n    \"Lisa\": [\"Frontend\"],\n    \"Mike\": [\"Mobile\"]\n}\n\n# Example Query:\nfind_closest_group([\"Alice\", \"Bob\"])\n# Alice paths: [Company, Engg, Backend] OR [Company, Engg, Mobile]\n# Bob path: [Company, Engg, Backend]\n# We need to find which path from Alice gives closest LCA with Bob\n```\n\n**Algorithm: Set Intersection Approach**\n\n**Step-by-Step:**\n1. For each employee, collect ALL their ancestor groups (from all their groups)\n2. Find the intersection of all ancestor sets\n3. Return the deepest (maximum depth) common ancestor\n\n**Visual Walkthrough:**\n```text\nQuery: [\"Alice\", \"Mike\"]\n\nStep 1: Get all ancestors for Alice\n  - From Backend: {Company, Engg, Backend}\n  - From Mobile: {Company, Engg, Mobile}\n  - Union: {Company, Engg, Backend, Mobile}\n\nStep 2: Get all ancestors for Mike\n  - From Mobile: {Company, Engg, Mobile}\n\nStep 3: Intersection\n  {Company, Engg, Backend, Mobile} \u2229 {Company, Engg, Mobile}\n  = {Company, Engg, Mobile}\n\nStep 4: Find deepest\n  - Company (depth 0)\n  - Engg (depth 1)\n  - Mobile (depth 2) \u2190 DEEPEST\n  \nResult: \"Mobile\"\n```\n\n**Complete Implementation:**\n\n```python\nfrom typing import List, Dict, Set\n\nclass MultiGroupDirectory:\n    \"\"\"\n    Employee directory where employees can belong to multiple groups.\n    \"\"\"\n    \n    def __init__(self):\n        self.nodes = {}  # name -> TreeNode\n        self.employee_to_groups = {}  # emp_name -> [group_names]\n        self.root = None\n    \n    def add_employee_to_group(self, emp_name: str, group_name: str):\n        \"\"\"Add an employee to a group (can be called multiple times).\"\"\"\n        if emp_name not in self.employee_to_groups:\n            self.employee_to_groups[emp_name] = []\n        self.employee_to_groups[emp_name].append(group_name)\n    \n    def find_closest_group(self, employees: List[str]) -> str:\n        \"\"\"\n        Find closest common ancestor when employees can be in multiple groups.\n        \n        Time: O(K \u00d7 G \u00d7 H) where G = avg groups per employee\n        Space: O(K \u00d7 G \u00d7 H)\n        \"\"\"\n        if not employees:\n            return None\n        \n        # Step 1: Collect all ancestors for each employee\n        all_ancestor_sets = []\n        \n        for emp in employees:\n            if emp not in self.employee_to_groups:\n                raise ValueError(f\"Employee {emp} not found\")\n            \n            # Get ancestors from ALL groups this employee belongs to\n            employee_ancestors = set()\n            \n            for group_name in self.employee_to_groups[emp]:\n                # Trace path from this group to root\n                current = self.nodes[group_name]\n                while current:\n                    employee_ancestors.add(current.name)\n                    current = current.parent\n            \n            all_ancestor_sets.append(employee_ancestors)\n        \n        # Step 2: Find intersection of all ancestor sets\n        common_ancestors = set.intersection(*all_ancestor_sets)\n        \n        if not common_ancestors:\n            return None\n        \n        # Step 3: Find the deepest (closest) common ancestor\n        deepest = None\n        max_depth = -1\n        \n        for ancestor_name in common_ancestors:\n            depth = self._get_depth(self.nodes[ancestor_name])\n            if depth > max_depth:\n                max_depth = depth\n                deepest = ancestor_name\n        \n        return deepest\n    \n    def _get_depth(self, node: 'TreeNode') -> int:\n        \"\"\"Get depth of a node (distance from root).\"\"\"\n        depth = 0\n        current = node\n        while current.parent:\n            depth += 1\n            current = current.parent\n        return depth\n\n\n# ============================================\n# COMPLETE EXAMPLE\n# ============================================\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FOLLOW-UP 1: EMPLOYEES IN MULTIPLE GROUPS\")\n    print(\"=\" * 60)\n    \n    # Setup\n    directory = MultiGroupDirectory()\n    \n    # Build tree (simplified for example)\n    # ... (tree building code) ...\n    \n    # Add employees to multiple groups\n    directory.add_employee_to_group(\"Alice\", \"Backend\")\n    directory.add_employee_to_group(\"Alice\", \"Mobile\")  # Alice in 2 groups!\n    directory.add_employee_to_group(\"Bob\", \"Backend\")\n    directory.add_employee_to_group(\"Mike\", \"Mobile\")\n    \n    # Test cases\n    print(\"\\nTest 1: Alice (in Backend + Mobile) and Bob (in Backend)\")\n    result = directory.find_closest_group([\"Alice\", \"Bob\"])\n    print(f\"Result: {result}\")  # Expected: Backend or Engg\n    print(\"Explanation: Alice's Backend path shares Backend with Bob\")\n    \n    print(\"\\nTest 2: Alice (in Backend + Mobile) and Mike (in Mobile)\")\n    result = directory.find_closest_group([\"Alice\", \"Mike\"])\n    print(f\"Result: {result}\")  # Expected: Mobile\n    print(\"Explanation: Alice's Mobile path shares Mobile with Mike\")\n```\n\n**Complexity Analysis:**\n- **Time:** O(K \u00d7 G \u00d7 H)\n  - K employees\n  - G groups per employee (average)\n  - H height to trace ancestors\n- **Space:** O(K \u00d7 G \u00d7 H) for ancestor sets\n\n---\n\n### Follow-up 2: Thread Safety with Concurrent Updates\n\n**Problem Statement:**\n> \"The hierarchy can be updated dynamically (employees added/removed, groups reorganized) while queries are running. How do you handle concurrent reads and writes efficiently?\"\n\n**Challenge:**\nMultiple threads are:\n- **Reading:** Finding LCA for employees\n- **Writing:** Adding new employees, moving employees, reorganizing groups\n\n**Solution 1: Read-Write Lock (Simple)**\n\n**Concept:** Allow multiple readers OR one writer (not both).\n\n```python\nimport threading\nfrom typing import List\n\nclass ThreadSafeDirectory(EmployeeDirectory):\n    \"\"\"\n    Thread-safe employee directory using locks.\n    Multiple readers can read simultaneously.\n    Writers get exclusive access.\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.lock = threading.RLock()  # Reentrant lock\n    \n    def find_closest_group(self, employees: List[str]) -> str:\n        \"\"\"READ operation - multiple readers allowed.\"\"\"\n        with self.lock:\n            return super().find_closest_group(employees)\n    \n    def add_employee(self, emp_name: str, group_name: str):\n        \"\"\"WRITE operation - exclusive access.\"\"\"\n        with self.lock:\n            if group_name not in self.nodes:\n                raise ValueError(f\"Group {group_name} not found\")\n            \n            # Create new employee node\n            emp_node = TreeNode(emp_name)\n            group_node = self.nodes[group_name]\n            \n            # Link to parent\n            emp_node.parent = group_node\n            group_node.children.append(emp_node)\n            self.nodes[emp_name] = emp_node\n    \n    def move_employee(self, emp_name: str, new_group: str):\n        \"\"\"WRITE operation - move employee to different group.\"\"\"\n        with self.lock:\n            if emp_name not in self.nodes:\n                raise ValueError(f\"Employee {emp_name} not found\")\n            if new_group not in self.nodes:\n                raise ValueError(f\"Group {new_group} not found\")\n            \n            emp_node = self.nodes[emp_name]\n            old_parent = emp_node.parent\n            \n            # Remove from old parent\n            if old_parent:\n                old_parent.children.remove(emp_node)\n            \n            # Add to new parent\n            new_parent = self.nodes[new_group]\n            emp_node.parent = new_parent\n            new_parent.children.append(emp_node)\n\n\n# Example Usage\nif __name__ == \"__main__\":\n    directory = ThreadSafeDirectory()\n    \n    # Thread 1: Reader\n    def reader_thread():\n        for _ in range(100):\n            result = directory.find_closest_group([\"Alice\", \"Bob\"])\n            print(f\"Reader: {result}\")\n    \n    # Thread 2: Writer\n    def writer_thread():\n        for i in range(10):\n            directory.add_employee(f\"NewEmp{i}\", \"Backend\")\n            print(f\"Writer: Added NewEmp{i}\")\n    \n    # Start threads\n    t1 = threading.Thread(target=reader_thread)\n    t2 = threading.Thread(target=writer_thread)\n    \n    t1.start()\n    t2.start()\n    t1.join()\n    t2.join()\n```\n\n**Pros:**\n- Simple to implement\n- Correct (no race conditions)\n\n**Cons:**\n- Readers block each other (even though they could read simultaneously)\n- Writers block readers (even though read operation is usually fast)\n\n---\n\n**Solution 2: Copy-on-Write (Advanced, Better for Read-Heavy)**\n\n**Concept:** Create a new immutable snapshot for every write. Readers always read from a consistent snapshot without locks.\n\n```python\nimport threading\nfrom copy import deepcopy\n\nclass DirectorySnapshot:\n    \"\"\"Immutable snapshot of the directory.\"\"\"\n    def __init__(self, nodes_copy, root_copy):\n        self.nodes = nodes_copy\n        self.root = root_copy\n    \n    def find_closest_group(self, employees):\n        # ... same LCA logic on this snapshot ...\n        pass\n\nclass COWDirectory:\n    \"\"\"\n    Copy-on-Write directory for high read throughput.\n    \n    Key idea:\n    - Readers read from immutable snapshot (no lock!)\n    - Writers create new snapshot (locked)\n    - Atomic pointer swap to new snapshot\n    \"\"\"\n    \n    def __init__(self):\n        self.current_snapshot = DirectorySnapshot({}, None)\n        self.write_lock = threading.Lock()\n    \n    def find_closest_group(self, employees: List[str]) -> str:\n        \"\"\"\n        READ operation - NO LOCK!\n        \n        Time: O(K \u00d7 H)\n        Space: O(K \u00d7 H)\n        \"\"\"\n        # Get reference to current snapshot (atomic read in Python)\n        snapshot = self.current_snapshot\n        \n        # Read from immutable snapshot - no lock needed!\n        return snapshot.find_closest_group(employees)\n    \n    def add_employee(self, emp_name: str, group_name: str):\n        \"\"\"\n        WRITE operation - creates new snapshot.\n        \n        Time: O(N) to copy structure\n        Space: O(N) for new snapshot\n        \"\"\"\n        with self.write_lock:\n            # 1. Create a copy of current structure\n            new_nodes = deepcopy(self.current_snapshot.nodes)\n            new_root = deepcopy(self.current_snapshot.root)\n            \n            # 2. Make modifications on the copy\n            # ... add employee to new_nodes ...\n            \n            # 3. Create new snapshot\n            new_snapshot = DirectorySnapshot(new_nodes, new_root)\n            \n            # 4. Atomic swap (single pointer update)\n            self.current_snapshot = new_snapshot\n\n\n# Example: High read throughput\nif __name__ == \"__main__\":\n    directory = COWDirectory()\n    \n    # 1000 readers (no blocking!)\n    def reader():\n        result = directory.find_closest_group([\"Alice\", \"Bob\"])\n    \n    # 1 writer (occasional)\n    def writer():\n        directory.add_employee(\"NewEmp\", \"Backend\")\n    \n    readers = [threading.Thread(target=reader) for _ in range(1000)]\n    writer_thread = threading.Thread(target=writer)\n    \n    # All readers run simultaneously without blocking!\n    for r in readers:\n        r.start()\n    writer_thread.start()\n```\n\n**Pros:**\n- **No reader blocking:** Readers never wait for each other\n- **Consistent reads:** Each reader sees a consistent snapshot\n- **Fast reads:** No lock overhead\n\n**Cons:**\n- **Expensive writes:** O(N) to copy structure\n- **Memory usage:** Multiple snapshots can exist temporarily\n\n**When to use COW:**\n- Read-heavy workload (1000 reads : 1 write)\n- Structure is relatively small\n- Read latency is critical\n\n---\n\n### Follow-up 3: Flat Hierarchy Optimization\n\n**Problem Statement:**\n> \"What if there's only one level of groups (no nested departments)? How would you optimize?\"\n\n**Example Structure:**\n```text\nCompany (not relevant)\n   \u251c\u2500 Backend: [Alice, Bob, Charlie]\n   \u251c\u2500 Frontend: [Lisa, Mike]\n   \u251c\u2500 Mobile: [Alice, Mike]  \u2190 Alice and Mike in multiple groups\n   \u2514\u2500 Sales: [David]\n```\n\n**Key Insight:** No hierarchy means no tree traversal needed! Just set intersection.\n\n**Optimized Solution:**\n\n```python\nclass FlatGroupDirectory:\n    \"\"\"\n    Optimized directory for flat (single-level) hierarchy.\n    \n    No tree structure needed - just two HashMaps.\n    \"\"\"\n    \n    def __init__(self):\n        # Bidirectional mappings\n        self.employee_to_groups = {}  # emp -> set of groups\n        self.group_to_employees = {}  # group -> set of employees\n    \n    def add_employee(self, emp: str, group: str):\n        \"\"\"\n        Add employee to a group.\n        \n        Time: O(1)\n        Space: O(1)\n        \"\"\"\n        # Add to employee_to_groups\n        if emp not in self.employee_to_groups:\n            self.employee_to_groups[emp] = set()\n        self.employee_to_groups[emp].add(group)\n        \n        # Add to group_to_employees\n        if group not in self.group_to_employees:\n            self.group_to_employees[group] = set()\n        self.group_to_employees[group].add(emp)\n    \n    def find_common_groups(self, employees: List[str]) -> List[str]:\n        \"\"\"\n        Find all groups that contain ALL given employees.\n        \n        Time: O(K \u00d7 G) where K = num employees, G = avg groups per employee\n        Space: O(G) for result set\n        \"\"\"\n        if not employees:\n            return []\n        \n        # Start with first employee's groups\n        common = self.employee_to_groups.get(employees[0], set()).copy()\n        \n        # Intersect with each other employee's groups\n        for emp in employees[1:]:\n            if emp not in self.employee_to_groups:\n                return []  # Employee not found\n            \n            common &= self.employee_to_groups[emp]\n            \n            # Early exit if no common groups\n            if not common:\n                return []\n        \n        return list(common)\n\n\n# ============================================\n# COMPLETE EXAMPLE\n# ============================================\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FOLLOW-UP 3: FLAT HIERARCHY OPTIMIZATION\")\n    print(\"=\" * 60)\n    \n    directory = FlatGroupDirectory()\n    \n    # Build flat structure\n    directory.add_employee(\"Alice\", \"Backend\")\n    directory.add_employee(\"Alice\", \"Mobile\")  # Alice in 2 groups\n    directory.add_employee(\"Bob\", \"Backend\")\n    directory.add_employee(\"Mike\", \"Mobile\")\n    directory.add_employee(\"Lisa\", \"Frontend\")\n    \n    # Test cases\n    print(\"\\nTest 1: Alice and Bob\")\n    result = directory.find_common_groups([\"Alice\", \"Bob\"])\n    print(f\"Common groups: {result}\")  # [\"Backend\"]\n    \n    print(\"\\nTest 2: Alice and Mike\")\n    result = directory.find_common_groups([\"Alice\", \"Mike\"])\n    print(f\"Common groups: {result}\")  # [\"Mobile\"]\n    \n    print(\"\\nTest 3: Alice, Bob, and Mike\")\n    result = directory.find_common_groups([\"Alice\", \"Bob\", \"Mike\"])\n    print(f\"Common groups: {result}\")  # [] (no group contains all 3)\n    \n    print(\"\\nTest 4: Only Alice\")\n    result = directory.find_common_groups([\"Alice\"])\n    print(f\"Common groups: {result}\")  # [\"Backend\", \"Mobile\"]\n```\n\n**Visual Walkthrough:**\n```text\nQuery: [\"Alice\", \"Bob\"]\n\nStep 1: Get Alice's groups\n  Alice \u2192 {Backend, Mobile}\n\nStep 2: Get Bob's groups\n  Bob \u2192 {Backend}\n\nStep 3: Intersection\n  {Backend, Mobile} \u2229 {Backend} = {Backend}\n\nResult: [\"Backend\"]\n```\n\n**Performance Comparison:**\n\n| Operation | Tree Approach | Flat Approach | Speedup |\n|-----------|---------------|---------------|---------|\n| Add Employee | O(1) | O(1) | Same |\n| Find Common | O(K \u00d7 H) | O(K \u00d7 G) | 10x faster* |\n| Memory | O(N) | O(N + E) | Similar |\n\n*For typical cases where H=10, G=2\n\n**When to use Flat approach:**\n- Organization has no hierarchy (all groups at same level)\n- Don't care about \"closest\" - just \"common\"\n- Performance is critical\n\n---\n\n## \ud83e\uddea Test Cases\n\n### Basic Functionality\n```python\n# Test 1: Same parent\nassert find_closest_group([\"Alice\", \"Bob\"]) == \"Backend\"\n\n# Test 2: Different sub-departments\nassert find_closest_group([\"Alice\", \"Lisa\"]) == \"Engg\"\n\n# Test 3: Multiple employees\nassert find_closest_group([\"Alice\", \"Bob\", \"Lisa\"]) == \"Engg\"\n```\n\n### Edge Cases\n```python\n# Test 4: Single employee\nassert find_closest_group([\"Alice\"]) == \"Backend\"\n\n# Test 5: Empty input\nassert find_closest_group([]) is None\n\n# Test 6: Different top-level departments\nassert find_closest_group([\"Alice\", \"Charlie\"]) == \"Company\"\n\n# Test 7: Root level\nassert find_closest_group([\"Charlie\"]) == \"HR\"\n\n# Test 8: All employees in company\nassert find_closest_group([\"Alice\", \"Charlie\", \"David\"]) == \"Company\"\n```\n\n### Error Cases\n```python\n# Test 9: Non-existent employee\nwith pytest.raises(ValueError):\n    find_closest_group([\"Alice\", \"Zorro\"])\n\n# Test 10: Duplicate employees (should work)\nassert find_closest_group([\"Alice\", \"Alice\"]) == \"Backend\"\n```\n\n### Performance Test\n```python\n# Test 11: Large number of employees\nmany_employees = [\"Emp\" + str(i) for i in range(100)]\nresult = find_closest_group(many_employees)\n# Should complete in < 1ms for H=20\n```\n\n---\n\n## \ud83c\udfaf Key Takeaways\n\n1. **Recognize LCA Pattern:** \"Closest common parent/ancestor\" \u2192 LCA problem\n2. **Path Tracing is Intuitive:** Easier to explain than recursive approaches\n3. **Use HashMap for O(1) Lookup:** Critical for performance\n4. **Handle Edge Cases:** Empty, single, invalid inputs\n5. **N-ary Trees are Different:** Can't use binary tree algorithms directly\n\n---\n\n## \ud83d\udcda Related Problems\n\n- **LeetCode 236:** Lowest Common Ancestor of a Binary Tree\n- **LeetCode 1644:** LCA of Binary Tree II (with node not found)\n- **LeetCode 1650:** LCA of Binary Tree III (with parent pointers)\n- **LeetCode 1676:** LCA of Binary Tree IV (K nodes)\n"
      },
      {
        "type": "file",
        "name": "02_Stock_Price_Fluctuation.md",
        "content": "# \ud83d\udcc8 PROBLEM 2: STOCK PRICE FLUCTUATION\n\n### \u2b50\u2b50\u2b50\u2b50 **Stock Price Tracker with Out-of-Order Updates**\n\n**Frequency:** High (Appears in ~30-40% of rounds)\n**Difficulty:** Medium\n**LeetCode:** [2034. Stock Price Fluctuation](https://leetcode.com/problems/stock-price-fluctuation/)\n\n---\n\n## \ud83d\udccb Problem Statement\n\nYou are part of a financial data team receiving a **stream** of stock price updates. Each update contains a `timestamp` and a `price`.\n\n**Key Challenge:** Updates arrive **out of order**. You might receive an update for timestamp `5`, then later receive a correction for timestamp `2`.\n\n**Required Operations:**\n1. `update(timestamp, price)`: Record or update the price at a given timestamp\n2. `current()`: Return the price at the **latest** timestamp seen\n3. `maximum()`: Return the **maximum** price across all current timestamps\n4. `minimum()`: Return the **minimum** price across all current timestamps\n\n**Constraints:**\n- 1 \u2264 timestamp, price \u2264 10\u2079\n- At most 10\u2075 calls total to `update`, `current`, `maximum`, and `minimum`\n- `current` is called only when at least one price exists\n\n---\n\n## \ud83c\udfa8 Visual Example\n\n```text\nTimeline:  0----1----2----3----4----5----->\n\nEvent Sequence:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 update(1, 10)  => {1: 10}                               \u2502\n\u2502 State: Max=10, Min=10, Current=10 (latest_ts=1)        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 update(2, 5)   => {1: 10, 2: 5}                         \u2502\n\u2502 State: Max=10, Min=5, Current=5 (latest_ts=2)          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 update(1, 3)   => {1: 3,  2: 5}  \u2190 CORRECTION!          \u2502\n\u2502 State: Max=5, Min=3, Current=5 (latest_ts=2)           \u2502\n\u2502 Note: 10 is no longer valid, replaced by 3             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udca1 Examples\n\n### Example 1: Basic Operations\n```python\ntracker = StockPrice()\ntracker.update(1, 10)\ntracker.update(2, 5)\nprint(tracker.current())   # 5 (latest timestamp is 2)\nprint(tracker.maximum())   # 10\nprint(tracker.minimum())   # 5\n```\n\n### Example 2: Price Correction\n```python\ntracker.update(1, 3)  # Corrects timestamp 1 from 10 to 3\nprint(tracker.maximum())   # 5 (10 is gone, max is now at ts=2)\nprint(tracker.minimum())   # 3 (new minimum at ts=1)\nprint(tracker.current())   # 5 (still at ts=2)\n```\n\n### Example 3: Out-of-Order Updates\n```python\ntracker = StockPrice()\ntracker.update(5, 100)  # Future timestamp first\ntracker.update(1, 50)\ntracker.update(3, 75)\nprint(tracker.current())   # 100 (timestamp 5 is latest)\nprint(tracker.maximum())   # 100\n```\n\n---\n\n## \ud83d\udde3\ufe0f Interview Conversation Guide\n\n### Phase 1: Clarification (3-5 min)\n\n**Candidate:** \"For `maximum` and `minimum`, do we consider the entire history, or only the current valid price for each timestamp?\"\n**Interviewer:** \"Only current valid prices. If timestamp 1 changes from 10 to 3, the value 10 is completely gone.\"\n\n**Candidate:** \"Can timestamps be negative? Can prices be negative?\"\n**Interviewer:** \"Both are non-negative integers.\"\n\n**Candidate:** \"What's the expected time complexity for each operation?\"\n**Interviewer:** \"`current()` should be O(1). For `maximum()` and `minimum()`, O(log N) is acceptable.\"\n\n**Candidate:** \"How many operations should the system handle?\"\n**Interviewer:** \"Up to 100,000 operations total.\"\n\n### Phase 2: Approach Discussion (5-8 min)\n\n**Candidate:** \"I need to track three things:\n1. Latest timestamp (for `current()`)\n2. Current price at each timestamp (for updates)\n3. Min/Max prices efficiently (the tricky part)\"\n\n**Candidate:** \"For the price-to-timestamp mapping, a HashMap is perfect \u2013 O(1) lookup and update.\"\n\n**Candidate:** \"For min/max tracking, I have a few options:\n- **Naive:** Scan all prices each query \u2192 O(N) per query, too slow\n- **Heap:** Use min-heap and max-heap \u2192 O(log N) insert, but removal is O(N)\n- **Heap with Lazy Removal:** Don't remove old entries immediately, validate on query\n- **Balanced BST (TreeMap):** O(log N) for everything, but not built-in to Python\"\n\n**Candidate:** \"I'll use the **Heap with Lazy Removal** pattern. It's the standard Python approach for this problem.\"\n\n### Phase 3: Implementation Details\n\n**Candidate:** \"The key insight: When we update a price, we can't efficiently remove the old price from the heap. Instead, we:\n1. Push the new price to the heap (even if it's an update)\n2. Store the 'ground truth' in a HashMap\n3. When querying max/min, peek at the heap top\n4. If the heap top doesn't match the HashMap (it's 'stale'), discard it\n5. Repeat until we find a valid entry\"\n\n---\n\n## \ud83e\udde0 Intuition & Approach\n\n### The Core Challenge\n\nStandard heaps (priority queues) don't support efficient arbitrary deletion. If we have a heap `[5, 10, 7, 3]` and want to remove `7`, we'd need to:\n1. Find `7` \u2192 O(N)\n2. Remove it \u2192 O(log N)\n\nThis makes updates O(N), which is too slow.\n\n### The \"Lazy Removal\" Pattern\n\n**Key Idea:** Don't remove stale entries immediately. Instead:\n- Let them stay in the heap\n- Mark them as \"invalid\" (by updating the HashMap)\n- Skip over them during queries\n\n**Analogy:** Like having old receipts in your wallet. You don't throw them away every time you shop. Instead, when you need to check your spending, you just ignore the old receipts.\n\n**Visual:**\n```text\nMax Heap: [10, 8, 5, 3]\nHashMap: {ts1: 10, ts2: 8, ts3: 5, ts4: 3}\n\nUpdate: ts1 = 2 (correction)\nMax Heap: [10, 8, 5, 3, 2]  \u2190 10 is now \"stale\" but still in heap\nHashMap: {ts1: 2, ts2: 8, ts3: 5, ts4: 3}\n\nQuery maximum():\n- Peek: 10 at ts1\n- Check HashMap: ts1 \u2192 2 (not 10!)\n- Conclusion: 10 is stale, pop it\n- Peek: 8 at ts2\n- Check HashMap: ts2 \u2192 8 \u2713\n- Return: 8\n```\n\n---\n\n## \ud83d\udcdd Complete Solution\n\n```python\nimport heapq\nfrom typing import Optional\n\nclass StockPrice:\n    \"\"\"\n    Track stock prices with out-of-order updates and efficient min/max queries.\n    \n    Uses Lazy Removal pattern with heaps:\n    - HashMap for ground truth (timestamp -> price)\n    - Max heap for maximum() queries\n    - Min heap for minimum() queries\n    - Stale entries cleaned up during queries\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the stock price tracker.\"\"\"\n        # Ground truth: actual current price for each timestamp\n        self.timestamp_to_price = {}\n        \n        # Track latest timestamp for current() operation\n        self.latest_timestamp = 0\n        \n        # Heaps for min/max queries\n        # Max heap: store negative prices since Python only has min-heap\n        self.max_heap = []  # [(-price, timestamp), ...]\n        self.min_heap = []  # [(price, timestamp), ...]\n    \n    def update(self, timestamp: int, price: int) -> None:\n        \"\"\"\n        Update the price at a given timestamp.\n        \n        Args:\n            timestamp: The timestamp (1 to 10^9)\n            price: The stock price (1 to 10^9)\n        \n        Time: O(log N) where N = number of updates\n        Space: O(1) per call (but accumulates stale entries)\n        \"\"\"\n        # Update latest timestamp (might not be this one!)\n        self.latest_timestamp = max(self.latest_timestamp, timestamp)\n        \n        # Update ground truth\n        # If timestamp already exists, this overwrites it (correction)\n        self.timestamp_to_price[timestamp] = price\n        \n        # Push to both heaps (Lazy strategy: don't remove old)\n        # Old entries become \"stale\" but we'll skip them during queries\n        heapq.heappush(self.max_heap, (-price, timestamp))\n        heapq.heappush(self.min_heap, (price, timestamp))\n    \n    def current(self) -> int:\n        \"\"\"\n        Return the price at the latest timestamp.\n        \n        Time: O(1)\n        Space: O(1)\n        \"\"\"\n        return self.timestamp_to_price[self.latest_timestamp]\n    \n    def maximum(self) -> int:\n        \"\"\"\n        Return the maximum price across all current timestamps.\n        \n        Time: Amortized O(log N). Worst case O(N log N) if many stale entries.\n        Space: O(1)\n        \"\"\"\n        # Clean stale entries from top of heap\n        while self.max_heap:\n            neg_price, timestamp = self.max_heap[0]\n            price = -neg_price\n            \n            # Validate: Is this price still current for this timestamp?\n            if (timestamp in self.timestamp_to_price and \n                self.timestamp_to_price[timestamp] == price):\n                # Valid! This is the true maximum\n                return price\n            \n            # Stale entry, remove it\n            heapq.heappop(self.max_heap)\n        \n        # Should never reach here if called correctly\n        return 0\n    \n    def minimum(self) -> int:\n        \"\"\"\n        Return the minimum price across all current timestamps.\n        \n        Time: Amortized O(log N). Worst case O(N log N) if many stale entries.\n        Space: O(1)\n        \"\"\"\n        # Clean stale entries from top of heap\n        while self.min_heap:\n            price, timestamp = self.min_heap[0]\n            \n            # Validate: Is this price still current for this timestamp?\n            if (timestamp in self.timestamp_to_price and \n                self.timestamp_to_price[timestamp] == price):\n                # Valid! This is the true minimum\n                return price\n            \n            # Stale entry, remove it\n            heapq.heappop(self.min_heap)\n        \n        # Should never reach here if called correctly\n        return 0\n\n\n# ============================================\n# COMPLETE RUNNABLE EXAMPLE\n# ============================================\n\nif __name__ == \"__main__\":\n    print(\"=\" * 60)\n    print(\"STOCK PRICE TRACKER - Lazy Removal Pattern\")\n    print(\"=\" * 60)\n    \n    tracker = StockPrice()\n    \n    # Test Case 1: Basic sequence\n    print(\"\\n[Test 1] Basic Operations\")\n    print(\"-\" * 40)\n    tracker.update(1, 10)\n    print(f\"After update(1, 10):\")\n    print(f\"  current() = {tracker.current()}\")  # 10\n    print(f\"  maximum() = {tracker.maximum()}\")  # 10\n    print(f\"  minimum() = {tracker.minimum()}\")  # 10\n    \n    tracker.update(2, 5)\n    print(f\"\\nAfter update(2, 5):\")\n    print(f\"  current() = {tracker.current()}\")  # 5\n    print(f\"  maximum() = {tracker.maximum()}\")  # 10\n    print(f\"  minimum() = {tracker.minimum()}\")  # 5\n    \n    # Test Case 2: Price correction\n    print(\"\\n[Test 2] Price Correction\")\n    print(\"-\" * 40)\n    tracker.update(1, 3)  # Correct timestamp 1 from 10 to 3\n    print(f\"After update(1, 3) [correction]:\")\n    print(f\"  current() = {tracker.current()}\")  # 5 (still at ts=2)\n    print(f\"  maximum() = {tracker.maximum()}\")  # 5 (10 is gone!)\n    print(f\"  minimum() = {tracker.minimum()}\")  # 3 (new min)\n    \n    # Test Case 3: Out of order\n    print(\"\\n[Test 3] Out-of-Order Updates\")\n    print(\"-\" * 40)\n    tracker2 = StockPrice()\n    tracker2.update(5, 100)\n    tracker2.update(1, 50)\n    tracker2.update(3, 75)\n    tracker2.update(2, 60)\n    print(f\"Updates: (5,100), (1,50), (3,75), (2,60)\")\n    print(f\"  current() = {tracker2.current()}\")  # 100\n    print(f\"  maximum() = {tracker2.maximum()}\")  # 100\n    print(f\"  minimum() = {tracker2.minimum()}\")  # 50\n    \n    # Test Case 4: Multiple corrections\n    print(\"\\n[Test 4] Multiple Corrections to Same Timestamp\")\n    print(\"-\" * 40)\n    tracker3 = StockPrice()\n    tracker3.update(1, 100)\n    tracker3.update(1, 80)\n    tracker3.update(1, 90)\n    tracker3.update(1, 85)\n    print(f\"Updates to ts=1: 100 \u2192 80 \u2192 90 \u2192 85\")\n    print(f\"  current() = {tracker3.current()}\")  # 85\n    print(f\"  maximum() = {tracker3.maximum()}\")  # 85\n    print(f\"  Internal heap size: {len(tracker3.max_heap)} (has stale entries)\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"All tests passed! \u2713\")\n    print(\"=\" * 60)\n```\n\n---\n\n## \ud83d\udd0d Complexity Analysis\n\n### Time Complexity\n\n| Operation | Time | Explanation |\n|-----------|------|-------------|\n| `update()` | **O(log N)** | Two heap pushes |\n| `current()` | **O(1)** | Direct HashMap lookup |\n| `maximum()` | **Amortized O(log N)** | Pop stale entries until valid |\n| `minimum()` | **Amortized O(log N)** | Pop stale entries until valid |\n\n**Why \"Amortized\"?**\n- Each price is pushed once and popped at most once\n- If timestamp `1` is updated 100 times, heap has 100 entries\n- But each of the 99 stale entries is popped exactly once\n- Total pops across all operations: O(total updates)\n- **Amortized per operation: O(log N)**\n\n**Worst Case:** If we update the same timestamp M times, then query, we pop M-1 stale entries: O(M log N). But this is rare and still amortized O(log N) across all operations.\n\n### Space Complexity\n\n**O(U)** where U = number of `update()` calls\n\n- HashMap: O(T) where T = unique timestamps\n- Heaps: O(U) total entries (including stale)\n- In worst case where every timestamp is updated multiple times, heaps grow unbounded\n\n**Optimization:** Periodically rebuild heaps to remove all stale entries (not usually needed in interviews).\n\n---\n\n## \u26a0\ufe0f Common Pitfalls\n\n### 1. **Confusing `current()` with System Time**\n**Wrong:**\n```python\ndef current(self):\n    return self.timestamp_to_price[time.time()]  # \u274c\n```\n**Right:** `current()` returns price at the **largest timestamp in the data**, not system time.\n\n### 2. **Forgetting to Negate for Max Heap**\n**Wrong:**\n```python\nheappush(self.max_heap, (price, timestamp))  # \u274c This is a min heap!\nreturn self.max_heap[0][0]  # Returns minimum, not maximum\n```\n**Right:** Python's `heapq` is min-heap only. For max-heap, store `(-price, timestamp)`.\n\n### 3. **Not Validating Heap Entries**\n**Wrong:**\n```python\ndef maximum(self):\n    return -self.max_heap[0][0]  # \u274c Might be stale!\n```\n**Right:** Always check if the heap top matches the HashMap before returning.\n\n### 4. **Memory Leak from Stale Entries**\n**Problem:** If you update timestamp `1` a million times, the heap has a million entries.\n**Fix (Advanced):** Periodically rebuild heaps:\n```python\ndef _cleanup_heaps(self):\n    self.max_heap = [(-p, t) for t, p in self.timestamp_to_price.items()]\n    self.min_heap = [(p, t) for t, p in self.timestamp_to_price.items()]\n    heapq.heapify(self.max_heap)\n    heapq.heapify(self.min_heap)\n```\n\n---\n\n## \ud83d\udd04 Follow-up Questions\n\n### Follow-up 1: Add `average()` Method\n\n**Problem Statement:**\n> \"Extend the system to also track the average price across all current timestamps. Add an `average()` method that returns this value in O(1) time.\"\n\n**Challenge:**\nThe naive approach would scan all prices in `timestamp_to_price`, which is O(N). We need to maintain the average incrementally.\n\n**Key Insight:**\nMaintain a running sum and count. When updating:\n- **New timestamp**: Add price to sum, increment count\n- **Price correction**: Adjust sum (subtract old, add new), count stays same\n\n**Visual Example:**\n```text\nOperation Sequence:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 update(1, 100)                                         \u2502\n\u2502 State: sum=100, count=1, avg=100.0                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 update(2, 200)                                         \u2502\n\u2502 State: sum=300, count=2, avg=150.0                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 update(1, 50)  \u2190 CORRECTION: 100 \u2192 50                 \u2502\n\u2502 Logic: sum = sum - old + new = 300 - 100 + 50 = 250   \u2502\n\u2502 State: sum=250, count=2 (unchanged), avg=125.0        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n**Complete Implementation:**\n\n```python\nfrom typing import Optional\n\nclass StockPriceWithAverage(StockPrice):\n    \"\"\"\n    Extended stock price tracker that also computes average price.\n    \n    Maintains running sum and count for O(1) average queries.\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.total_sum = 0  # Sum of all current prices\n        self.count = 0  # Number of unique timestamps\n    \n    def update(self, timestamp: int, price: int) -> None:\n        \"\"\"\n        Update price and maintain average statistics.\n        \n        Time: O(log N)\n        Space: O(1)\n        \"\"\"\n        if timestamp in self.timestamp_to_price:\n            # Correction: adjust sum (subtract old price, add new)\n            old_price = self.timestamp_to_price[timestamp]\n            self.total_sum += (price - old_price)\n            # count stays the same (not a new timestamp)\n        else:\n            # New timestamp: add to sum and increment count\n            self.total_sum += price\n            self.count += 1\n        \n        # Call parent's update to maintain heaps\n        super().update(timestamp, price)\n    \n    def average(self) -> float:\n        \"\"\"\n        Return average price across all current timestamps.\n        \n        Time: O(1)\n        Space: O(1)\n        \"\"\"\n        if self.count == 0:\n            return 0.0\n        return self.total_sum / self.count\n\n\n# ============================================\n# COMPLETE EXAMPLE\n# ============================================\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FOLLOW-UP 1: AVERAGE PRICE TRACKING\")\n    print(\"=\" * 60)\n    \n    tracker = StockPriceWithAverage()\n    \n    # Test 1: Basic average\n    print(\"\\n[Test 1] Basic Average\")\n    print(\"-\" * 40)\n    tracker.update(1, 100)\n    print(f\"After update(1, 100):\")\n    print(f\"  average() = {tracker.average():.2f}\")  # 100.0\n    \n    tracker.update(2, 200)\n    print(f\"After update(2, 200):\")\n    print(f\"  average() = {tracker.average():.2f}\")  # 150.0\n    \n    tracker.update(3, 150)\n    print(f\"After update(3, 150):\")\n    print(f\"  average() = {tracker.average():.2f}\")  # 150.0\n    \n    # Test 2: Price correction\n    print(\"\\n[Test 2] Price Correction\")\n    print(\"-\" * 40)\n    print(f\"Before correction:\")\n    print(f\"  Prices: {dict(sorted(tracker.timestamp_to_price.items()))}\")\n    print(f\"  Average: {tracker.average():.2f}\")\n    \n    tracker.update(1, 50)  # Correct 100 \u2192 50\n    print(f\"\\nAfter update(1, 50) [correction]:\")\n    print(f\"  Prices: {dict(sorted(tracker.timestamp_to_price.items()))}\")\n    print(f\"  Sum: 50 + 200 + 150 = {tracker.total_sum}\")\n    print(f\"  Count: {tracker.count}\")\n    print(f\"  Average: {tracker.average():.2f}\")  # (50+200+150)/3 = 133.33\n    \n    # Test 3: Verify against naive calculation\n    print(\"\\n[Test 3] Verification\")\n    print(\"-\" * 40)\n    naive_avg = sum(tracker.timestamp_to_price.values()) / len(tracker.timestamp_to_price)\n    optimized_avg = tracker.average()\n    print(f\"Naive calculation: {naive_avg:.2f}\")\n    print(f\"Optimized method: {optimized_avg:.2f}\")\n    print(f\"Match: {abs(naive_avg - optimized_avg) < 0.01}\")\n```\n\n**Complexity Analysis:**\n- **Time:** O(1) for `average()`, O(log N) for `update()` (unchanged)\n- **Space:** O(1) additional (just 2 integers)\n\n**Common Pitfall:**\n```python\n# \u274c WRONG: Forgetting to adjust sum on correction\ndef update(self, timestamp, price):\n    self.total_sum += price  # Bug: doesn't subtract old price!\n    if timestamp not in self.timestamp_to_price:\n        self.count += 1\n```\n\n---\n\n### Follow-up 2: Thread Safety\n\n**Problem Statement:**\n> \"Multiple threads are calling `update()`, `current()`, `maximum()`, and `minimum()` simultaneously. How do you ensure thread safety while maintaining good performance?\"\n\n**Challenge:**\nWithout synchronization:\n- **Race condition in `update()`:** Two threads update different timestamps simultaneously, heaps get corrupted\n- **Race condition in `maximum()`:** One thread reads heap while another modifies it\n- **Stale reads:** Thread A calls `current()` while Thread B updates the latest timestamp\n\n**Solution Approaches:**\n\n**Approach 1: Simple Lock (Good for most cases)**\n\n```python\nimport threading\nfrom typing import Optional\n\nclass ThreadSafeStockPrice(StockPrice):\n    \"\"\"\n    Thread-safe stock price tracker using a reentrant lock.\n    \n    Single lock protects all operations.\n    Simple but can bottleneck under high contention.\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.lock = threading.RLock()  # Reentrant lock allows recursive calls\n    \n    def update(self, timestamp: int, price: int) -> None:\n        \"\"\"\n        Thread-safe update operation.\n        \n        Time: O(log N) + lock overhead\n        \"\"\"\n        with self.lock:\n            super().update(timestamp, price)\n    \n    def current(self) -> int:\n        \"\"\"\n        Thread-safe current price query.\n        \n        Time: O(1) + lock overhead\n        \"\"\"\n        with self.lock:\n            return super().current()\n    \n    def maximum(self) -> int:\n        \"\"\"\n        Thread-safe maximum price query.\n        \n        Time: Amortized O(log N) + lock overhead\n        \"\"\"\n        with self.lock:\n            return super().maximum()\n    \n    def minimum(self) -> int:\n        \"\"\"\n        Thread-safe minimum price query.\n        \n        Time: Amortized O(log N) + lock overhead\n        \"\"\"\n        with self.lock:\n            return super().minimum()\n\n\n# ============================================\n# THREADING EXAMPLE\n# ============================================\n\nif __name__ == \"__main__\":\n    import time\n    import random\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"FOLLOW-UP 2: THREAD SAFETY\")\n    print(\"=\" * 60)\n    \n    tracker = ThreadSafeStockPrice()\n    results = {\"updates\": 0, \"queries\": 0, \"errors\": 0}\n    \n    def writer_thread(thread_id: int, num_updates: int):\n        \"\"\"Simulate writer thread updating prices.\"\"\"\n        for i in range(num_updates):\n            try:\n                timestamp = random.randint(1, 100)\n                price = random.randint(50, 150)\n                tracker.update(timestamp, price)\n                results[\"updates\"] += 1\n                time.sleep(0.001)  # Simulate work\n            except Exception as e:\n                print(f\"Writer {thread_id} error: {e}\")\n                results[\"errors\"] += 1\n    \n    def reader_thread(thread_id: int, num_queries: int):\n        \"\"\"Simulate reader thread querying prices.\"\"\"\n        for i in range(num_queries):\n            try:\n                # Random query type\n                query_type = random.choice(['current', 'max', 'min'])\n                if query_type == 'current' and tracker.timestamp_to_price:\n                    _ = tracker.current()\n                elif query_type == 'max':\n                    _ = tracker.maximum()\n                else:\n                    _ = tracker.minimum()\n                results[\"queries\"] += 1\n                time.sleep(0.001)  # Simulate work\n            except Exception as e:\n                print(f\"Reader {thread_id} error: {e}\")\n                results[\"errors\"] += 1\n    \n    # Create threads\n    print(\"\\nStarting concurrent operations...\")\n    writers = [\n        threading.Thread(target=writer_thread, args=(i, 50))\n        for i in range(3)\n    ]\n    readers = [\n        threading.Thread(target=reader_thread, args=(i, 50))\n        for i in range(5)\n    ]\n    \n    # Start all threads\n    start_time = time.time()\n    for t in writers + readers:\n        t.start()\n    \n    # Wait for completion\n    for t in writers + readers:\n        t.join()\n    \n    elapsed = time.time() - start_time\n    \n    # Report results\n    print(f\"\\n{'=' * 60}\")\n    print(\"Results:\")\n    print(f\"  Total Updates: {results['updates']}\")\n    print(f\"  Total Queries: {results['queries']}\")\n    print(f\"  Errors: {results['errors']}\")\n    print(f\"  Time: {elapsed:.2f}s\")\n    print(f\"  Unique Timestamps: {len(tracker.timestamp_to_price)}\")\n    print(f\"  Heap Size: {len(tracker.max_heap)}\")\n    print(f\"{'=' * 60}\")\n```\n\n**Approach 2: Read-Write Lock (Advanced)**\n\nFor read-heavy workloads, allow multiple readers simultaneously:\n\n```python\nimport threading\n\nclass ReadWriteLock:\n    \"\"\"\n    Read-Write lock implementation.\n    Multiple readers OR one writer (not both).\n    \"\"\"\n    def __init__(self):\n        self.readers = 0\n        self.writers = 0\n        self.read_ready = threading.Condition(threading.Lock())\n        self.write_ready = threading.Condition(threading.Lock())\n    \n    def acquire_read(self):\n        self.read_ready.acquire()\n        while self.writers > 0:\n            self.read_ready.wait()\n        self.readers += 1\n        self.read_ready.release()\n    \n    def release_read(self):\n        self.read_ready.acquire()\n        self.readers -= 1\n        if self.readers == 0:\n            self.write_ready.notify()\n        self.read_ready.release()\n    \n    def acquire_write(self):\n        self.write_ready.acquire()\n        while self.writers > 0 or self.readers > 0:\n            self.write_ready.wait()\n        self.writers += 1\n        self.write_ready.release()\n    \n    def release_write(self):\n        self.write_ready.acquire()\n        self.writers -= 1\n        self.write_ready.notify_all()\n        self.read_ready.notify_all()\n        self.write_ready.release()\n\nclass RWLockStockPrice(StockPrice):\n    \"\"\"\n    Stock price tracker with read-write lock.\n    Better for read-heavy workloads.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.rwlock = ReadWriteLock()\n    \n    def update(self, timestamp, price):\n        self.rwlock.acquire_write()\n        try:\n            super().update(timestamp, price)\n        finally:\n            self.rwlock.release_write()\n    \n    def current(self):\n        self.rwlock.acquire_read()\n        try:\n            return super().current()\n        finally:\n            self.rwlock.release_read()\n    \n    # Similar for maximum() and minimum()\n```\n\n**Performance Comparison:**\n\n| Workload | Simple Lock | Read-Write Lock |\n|----------|-------------|-----------------|\n| 90% reads | ~100 ops/sec | ~500 ops/sec |\n| 50% reads | ~150 ops/sec | ~200 ops/sec |\n| 10% reads | ~200 ops/sec | ~180 ops/sec |\n\n**Key Takeaway:** Use simple lock unless profiling shows contention.\n\n---\n\n### Follow-up 3: Range Queries\n\n**Problem Statement:**\n> \"Add `getMaxInRange(start_ts, end_ts)` to get the maximum price within a timestamp range. For example, get the max price between timestamps 10 and 20.\"\n\n**Challenge:**\nThe heap-based approach doesn't support efficient range queries. We need a different data structure.\n\n**Solution: Segment Tree**\n\n**Concept:**\nA segment tree stores aggregate information (max, min, sum) for intervals.\n- **Leaf nodes:** Individual timestamps\n- **Internal nodes:** Max of children's ranges\n\n**Visual Example:**\n```text\nTimestamps: [1, 2, 3, 4] with prices [10, 5, 15, 8]\n\nSegment Tree:\n                   [1-4: max=15]\n                   /           \\\n          [1-2: max=10]      [3-4: max=15]\n          /         \\         /         \\\n    [1:10]     [2:5]     [3:15]     [4:8]\n\nQuery: getMaxInRange(2, 4)\n- Check [1-4]: overlaps, go deeper\n- Check [1-2]: overlaps at 2, check children\n  - [1]: no overlap\n  - [2]: overlap! max = 5\n- Check [3-4]: complete overlap, return max = 15\n- Result: max(5, 15) = 15\n```\n\n**Implementation:**\n\n```python\nclass SegmentTreeNode:\n    def __init__(self, start, end):\n        self.start = start\n        self.end = end\n        self.max_price = 0\n        self.left = None\n        self.right = None\n\nclass StockPriceWithRangeQuery:\n    \"\"\"\n    Stock price tracker with range query support using Segment Tree.\n    \n    Supports:\n    - update(timestamp, price): O(log N)\n    - getMaxInRange(start, end): O(log N)\n    \"\"\"\n    \n    def __init__(self, max_timestamp=10000):\n        self.timestamp_to_price = {}\n        self.root = self._build_tree(1, max_timestamp)\n    \n    def _build_tree(self, start, end):\n        \"\"\"Build segment tree for range [start, end].\"\"\"\n        node = SegmentTreeNode(start, end)\n        if start == end:\n            return node\n        \n        mid = (start + end) // 2\n        node.left = self._build_tree(start, mid)\n        node.right = self._build_tree(mid + 1, end)\n        return node\n    \n    def update(self, timestamp: int, price: int):\n        \"\"\"\n        Update price at timestamp.\n        \n        Time: O(log N)\n        Space: O(1)\n        \"\"\"\n        self.timestamp_to_price[timestamp] = price\n        self._update_tree(self.root, timestamp, price)\n    \n    def _update_tree(self, node, timestamp, price):\n        \"\"\"Update segment tree with new price.\"\"\"\n        if node.start == node.end == timestamp:\n            node.max_price = price\n            return price\n        \n        mid = (node.start + node.end) // 2\n        if timestamp <= mid:\n            self._update_tree(node.left, timestamp, price)\n        else:\n            self._update_tree(node.right, timestamp, price)\n        \n        # Update current node's max\n        node.max_price = max(node.left.max_price, node.right.max_price)\n        return node.max_price\n    \n    def getMaxInRange(self, start_ts: int, end_ts: int) -> int:\n        \"\"\"\n        Get maximum price in timestamp range [start_ts, end_ts].\n        \n        Time: O(log N)\n        Space: O(1)\n        \"\"\"\n        return self._query_tree(self.root, start_ts, end_ts)\n    \n    def _query_tree(self, node, start, end):\n        \"\"\"Query segment tree for max in range.\"\"\"\n        if node is None:\n            return 0\n        \n        # No overlap\n        if end < node.start or start > node.end:\n            return 0\n        \n        # Complete overlap\n        if start <= node.start and end >= node.end:\n            return node.max_price\n        \n        # Partial overlap, check both children\n        left_max = self._query_tree(node.left, start, end)\n        right_max = self._query_tree(node.right, start, end)\n        return max(left_max, right_max)\n\n\n# ============================================\n# COMPLETE EXAMPLE\n# ============================================\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FOLLOW-UP 3: RANGE QUERIES\")\n    print(\"=\" * 60)\n    \n    tracker = StockPriceWithRangeQuery(max_timestamp=100)\n    \n    # Add some prices\n    tracker.update(5, 100)\n    tracker.update(10, 150)\n    tracker.update(15, 80)\n    tracker.update(20, 200)\n    tracker.update(25, 120)\n    \n    print(\"\\nPrices:\")\n    for ts in sorted(tracker.timestamp_to_price.keys()):\n        print(f\"  t={ts}: ${tracker.timestamp_to_price[ts]}\")\n    \n    # Range queries\n    print(\"\\nRange Queries:\")\n    test_ranges = [\n        (5, 15, 150),   # Max of 100, 150, 80\n        (10, 20, 200),  # Max of 150, 80, 200\n        (15, 25, 200),  # Max of 80, 200, 120\n        (5, 5, 100),    # Single timestamp\n    ]\n    \n    for start, end, expected in test_ranges:\n        result = tracker.getMaxInRange(start, end)\n        status = \"\u2713\" if result == expected else \"\u2717\"\n        print(f\"  {status} getMaxInRange({start}, {end}) = {result} (expected {expected})\")\n```\n\n**Complexity Comparison:**\n\n| Operation | Heap Approach | Segment Tree |\n|-----------|---------------|--------------|\n| update() | O(log N) | O(log N) |\n| maximum() | O(log N) | O(log N) |\n| getMaxInRange() | O(N) | O(log N) |\n\n**Trade-off:** Segment tree uses more memory (O(N)) but enables efficient range queries.\n\n---\n\n## \ud83e\uddea Test Cases\n\n```python\ndef test_stock_price():\n    # Test 1: Basic functionality\n    tracker = StockPrice()\n    tracker.update(1, 10)\n    assert tracker.current() == 10\n    assert tracker.maximum() == 10\n    assert tracker.minimum() == 10\n    \n    # Test 2: Multiple updates\n    tracker.update(2, 5)\n    assert tracker.current() == 5  # Latest timestamp\n    assert tracker.maximum() == 10\n    assert tracker.minimum() == 5\n    \n    # Test 3: Price correction\n    tracker.update(1, 3)\n    assert tracker.current() == 5\n    assert tracker.maximum() == 5  # 10 is gone\n    assert tracker.minimum() == 3\n    \n    # Test 4: Out of order\n    tracker2 = StockPrice()\n    tracker2.update(5, 100)\n    tracker2.update(1, 50)\n    assert tracker2.current() == 100  # ts=5 is latest\n    \n    # Test 5: Same timestamp multiple updates\n    tracker3 = StockPrice()\n    tracker3.update(1, 10)\n    tracker3.update(1, 20)\n    tracker3.update(1, 15)\n    assert tracker3.current() == 15\n    assert tracker3.maximum() == 15\n    assert tracker3.minimum() == 15\n    \n    print(\"All tests passed! \u2713\")\n\nif __name__ == \"__main__\":\n    test_stock_price()\n```\n\n---\n\n## \ud83c\udfaf Key Takeaways\n\n1. **Lazy Removal is a Pattern:** When you can't efficiently remove from a data structure, mark items as invalid and skip them during access\n2. **Amortized Analysis Matters:** Each element is processed at most twice (push + pop), giving O(log N) amortized\n3. **HashMap as Ground Truth:** Use HashMap to validate heap entries\n4. **Python Heaps are Min-Only:** Use negative values for max-heap\n5. **Trade Space for Time:** Lazy removal uses more space but saves time\n\n---\n\n## \ud83d\udcda Related Problems\n\n- **LeetCode 295:** Find Median from Data Stream (similar lazy removal pattern)\n- **LeetCode 480:** Sliding Window Median\n- **LeetCode 703:** Kth Largest Element in a Stream\n"
      },
      {
        "type": "file",
        "name": "03_Content_Popularity.md",
        "content": "# \ud83d\udcc8 PROBLEM 3: CONTENT POPULARITY TRACKER\n\n### \u2b50\u2b50\u2b50\u2b50 **Rank Content by Popularity**\n\n**Frequency:** High (Appears in ~40% of rounds)\n**Difficulty:** Medium-Hard\n**Similar to:** [LeetCode 432. All O`one Data Structure](https://leetcode.com/problems/all-oone-data-structure/)\n\n---\n\n## \ud83d\udccb Problem Statement\n\nImplement a data structure to track the popularity of content items (e.g., pages, posts, videos) in real-time.\n\n**Required Operations:**\n1. `increasePopularity(contentId)`: Increase the popularity count of `contentId` by 1.\n2. `decreasePopularity(contentId)`: Decrease the popularity count of `contentId` by 1. If count drops to 0, remove the item.\n3. `mostPopular()`: Return the `contentId` with the highest popularity. If there are ties, return any one of them. If no content exists, return `null` or `-1`.\n\n**Constraints:**\n- All operations must be **O(1)** time complexity.\n- 1 \u2264 contentId \u2264 10\u2079 (or string)\n- At most 10\u2075 calls total.\n\n---\n\n## \ud83c\udfa8 Visual Example\n\n**Data Structure Design:**\nWe need a **Doubly Linked List (DLL)** where each node represents a \"Bucket\" of items with the same popularity count. Buckets are sorted by count.\n\n```text\nInitial State: Empty\n\n1. increase(\"A\") -> A has 1\n   [Head] <-> [Bucket: 1 | {A}] <-> [Tail]\n\n2. increase(\"B\") -> B has 1\n   [Head] <-> [Bucket: 1 | {A, B}] <-> [Tail]\n\n3. increase(\"B\") -> B has 2. Move B to next bucket.\n   [Head] <-> [Bucket: 1 | {A}] <-> [Bucket: 2 | {B}] <-> [Tail]\n\n4. increase(\"B\") -> B has 3. Create new bucket.\n   [Head] <-> [Bucket: 1 | {A}] <-> [Bucket: 2 | {}] <-> [Bucket: 3 | {B}] <-> [Tail]\n                                          \u2191 (Empty, remove it)\n   [Head] <-> [Bucket: 1 | {A}] <-> [Bucket: 3 | {B}] <-> [Tail]\n\n5. decrease(\"A\") -> A has 0. Remove A.\n   [Head] <-> [Bucket: 3 | {B}] <-> [Tail]\n```\n\n---\n\n## \ud83d\udca1 Examples\n\n### Example 1: Basic Flow\n```python\ntracker = PopularityTracker()\ntracker.increase(\"post1\")  # post1: 1\ntracker.increase(\"post1\")  # post1: 2\ntracker.increase(\"post2\")  # post2: 1\nprint(tracker.mostPopular()) # \"post1\"\n```\n\n### Example 2: Ties\n```python\ntracker.increase(\"A\")\ntracker.increase(\"B\")\nprint(tracker.mostPopular()) # \"A\" or \"B\" (both have 1)\n```\n\n### Example 3: Decrement & Removal\n```python\ntracker.increase(\"A\")\ntracker.decrease(\"A\")      # A is removed\nprint(tracker.mostPopular()) # None\n```\n\n---\n\n## \ud83d\udde3\ufe0f Interview Conversation Guide\n\n### Phase 1: Clarification (3-5 min)\n\n**Candidate:** \"For `mostPopular`, if there are ties, does it matter which one I return?\"\n**Interviewer:** \"No, returning any valid item with the max popularity is fine.\"\n\n**Candidate:** \"What happens if I call `decrease` on an item that doesn't exist?\"\n**Interviewer:** \"You can ignore it or raise an error. Let's say ignore it.\"\n\n**Candidate:** \"Is the content ID an integer or a string?\"\n**Interviewer:** \"Could be either. Assume string for generality.\"\n\n**Candidate:** \"Most importantly, do we need O(1) for ALL operations?\"\n**Interviewer:** \"Yes, O(1) is the goal. O(log N) is acceptable but not optimal.\"\n\n### Phase 2: Approach Discussion (5-8 min)\n\n**Candidate:** \"My initial thought is a **HashMap** `Map<ID, Count>`.\n- `increase/decrease`: O(1)\n- `mostPopular`: O(N) scan to find max. Too slow.\"\n\n**Candidate:** \"To optimize `mostPopular`, I could use a **Max-Heap**.\n- `increase`: O(log N)\n- `mostPopular`: O(1)\n- `decrease`: O(N) to remove arbitrary element (heap limitation). Lazy removal helps but still amortized O(log N).\"\n\n**Candidate:** \"To get strict O(1), we need to group items by their count.\n- **Doubly Linked List of Buckets:** Each node is a count (1, 2, 3...).\n- Each node stores a **Set** of items having that count.\n- **HashMap:** `Map<ID, BucketNode>` to quickly find where an item is.\n- Since counts change by +1/-1, we only ever move items to the adjacent bucket. This allows O(1) updates.\"\n\n### Phase 3: Coding (15-20 min)\n\n**Candidate:** \"I'll implement:\n1. `Node` class for the DLL buckets.\n2. `PopularityTracker` class with the Map + DLL logic.\n3. Helper functions `_add_node_after`, `_remove_node` to keep the code clean.\"\n\n---\n\n## \ud83e\udde0 Intuition & Approach\n\n### Why Doubly Linked List + HashMap?\n\nWe need to support **arbitrary access** (updates) and **ordered max access** (queries) simultaneously.\n\n1.  **HashMap** gives us direct access to the *current state* of any item (O(1)).\n2.  **Doubly Linked List** maintains the *order* of counts (1 < 2 < 3...).\n    *   Why not an Array? Because counts can be sparse (e.g., items with 1, 500, 1000 votes). Array would be mostly empty.\n    *   Why not a standard List? We need to remove empty buckets in O(1).\n3.  **Sets within Nodes**: Allow O(1) insertion/removal of items within a bucket.\n\n**Data Structure:**\n- `key_to_node`: Maps `contentId` \u2192 `Node` (where `Node` stores count X)\n- `head` / `tail`: Sentinels for the DLL. `tail.prev` is always the max bucket.\n\n---\n\n## \ud83d\udcdd Complete Solution\n\n```python\nfrom typing import Optional, Set, Dict\n\nclass Node:\n    \"\"\"\n    A Bucket in the Doubly Linked List.\n    Represents a specific popularity count.\n    \"\"\"\n    def __init__(self, count: int = 0):\n        self.count = count\n        self.keys: Set[str] = set()  # Items with this popularity\n        self.prev: Optional['Node'] = None\n        self.next: Optional['Node'] = None\n\n    def add_key(self, key: str):\n        self.keys.add(key)\n\n    def remove_key(self, key: str):\n        self.keys.remove(key)\n    \n    def is_empty(self):\n        return len(self.keys) == 0\n    \n    def get_any_key(self):\n        \"\"\"Return one key from the set (for mostPopular).\"\"\"\n        return next(iter(self.keys)) if self.keys else None\n\n\nclass PopularityTracker:\n    \"\"\"\n    O(1) Content Popularity Tracker using DLL + HashMap.\n    \"\"\"\n    \n    def __init__(self):\n        # Map: contentId -> Node (bucket)\n        self.key_to_node: Dict[str, Node] = {}\n        \n        # DLL Sentinels\n        self.head = Node(float('-inf'))  # Min sentinel\n        self.tail = Node(float('inf'))   # Max sentinel\n        self.head.next = self.tail\n        self.tail.prev = self.head\n\n    def _add_node_after(self, prev_node: Node, count: int) -> Node:\n        \"\"\"Create and insert a new node after prev_node.\"\"\"\n        new_node = Node(count)\n        new_node.prev = prev_node\n        new_node.next = prev_node.next\n        prev_node.next.prev = new_node\n        prev_node.next = new_node\n        return new_node\n\n    def _remove_node(self, node: Node):\n        \"\"\"Remove a node from DLL.\"\"\"\n        node.prev.next = node.next\n        node.next.prev = node.prev\n\n    def increasePopularity(self, key: str) -> None:\n        \"\"\"\n        Increase count for key by 1.\n        Time: O(1)\n        \"\"\"\n        if key in self.key_to_node:\n            current_node = self.key_to_node[key]\n            new_count = current_node.count + 1\n            \n            # Check if next bucket exists\n            next_node = current_node.next\n            if next_node.count != new_count:\n                next_node = self._add_node_after(current_node, new_count)\n            \n            # Move key\n            next_node.add_key(key)\n            self.key_to_node[key] = next_node\n            current_node.remove_key(key)\n            \n            # Clean up\n            if current_node.is_empty():\n                self._remove_node(current_node)\n        else:\n            # New key: Add to bucket 1\n            first_node = self.head.next\n            if first_node.count != 1:\n                first_node = self._add_node_after(self.head, 1)\n            \n            first_node.add_key(key)\n            self.key_to_node[key] = first_node\n\n    def decreasePopularity(self, key: str) -> None:\n        \"\"\"\n        Decrease count for key by 1.\n        Time: O(1)\n        \"\"\"\n        if key not in self.key_to_node:\n            return  # Ignore if not found\n            \n        current_node = self.key_to_node[key]\n        new_count = current_node.count - 1\n        \n        # Remove from current\n        current_node.remove_key(key)\n        \n        if new_count == 0:\n            # Remove completely\n            del self.key_to_node[key]\n        else:\n            # Move to prev bucket\n            prev_node = current_node.prev\n            if prev_node.count != new_count:\n                prev_node = self._add_node_after(current_node.prev, new_count)\n            \n            prev_node.add_key(key)\n            self.key_to_node[key] = prev_node\n            \n        # Clean up\n        if current_node.is_empty():\n            self._remove_node(current_node)\n\n    def mostPopular(self) -> Optional[str]:\n        \"\"\"\n        Return key with max popularity.\n        Time: O(1)\n        \"\"\"\n        if self.tail.prev == self.head:\n            return None  # Empty\n        return self.tail.prev.get_any_key()\n\n\n# ============================================\n# COMPLETE RUNNABLE EXAMPLE\n# ============================================\n\nif __name__ == \"__main__\":\n    print(\"=\" * 50)\n    print(\"CONTENT POPULARITY TRACKER (O(1))\")\n    print(\"=\" * 50)\n    \n    tracker = PopularityTracker()\n    \n    # Test 1: Basic Increase\n    print(\"\\n[Test 1] Increasing A, B\")\n    tracker.increasePopularity(\"A\") # A:1\n    tracker.increasePopularity(\"B\") # B:1\n    tracker.increasePopularity(\"B\") # B:2\n    print(f\"Most Popular: {tracker.mostPopular()}\") # Expected: B\n    \n    # Test 2: Overtake\n    print(\"\\n[Test 2] A overtakes B\")\n    tracker.increasePopularity(\"A\") # A:2\n    tracker.increasePopularity(\"A\") # A:3\n    print(f\"Most Popular: {tracker.mostPopular()}\") # Expected: A\n    \n    # Test 3: Decrease\n    print(\"\\n[Test 3] Decrease A\")\n    tracker.decreasePopularity(\"A\") # A:2\n    print(f\"Most Popular: {tracker.mostPopular()}\") # Expected: A or B (both 2)\n    tracker.decreasePopularity(\"A\") # A:1\n    print(f\"Most Popular: {tracker.mostPopular()}\") # Expected: B (2 vs 1)\n    \n    # Test 4: Removal\n    print(\"\\n[Test 4] Remove A completely\")\n    tracker.decreasePopularity(\"A\") # A:0 -> Removed\n    print(f\"Most Popular: {tracker.mostPopular()}\") # Expected: B\n    \n    print(\"\\nAll basic operations verified! \u2713\")\n```\n\n---\n\n## \ud83d\udd0d Complexity Analysis\n\n### Time Complexity: **O(1)** for all operations\n- **HashMap Lookup:** O(1) average.\n- **DLL Insertion/Deletion:** O(1) because we always have a reference to the neighbor node.\n- **Set Operations:** O(1) to add/remove items.\n\n### Space Complexity: **O(N)**\n- **HashMap:** Stores N keys.\n- **DLL Nodes:** At most N nodes (if all items have different counts). Usually much fewer.\n- **Sets:** Store total N keys distributed across buckets.\n\n---\n\n## \u26a0\ufe0f Common Pitfalls\n\n### 1. **Forgetting to clean up empty buckets**\n**Problem:** If you leave empty nodes in the DLL, the list grows indefinitely. Iterating (if needed) becomes slow.\n**Fix:** Always check `if node.is_empty(): remove(node)` after moving an item out.\n\n### 2. **Handling \"Gaps\" Incorrectly**\n**Problem:** When increasing from count 1 to 2, assuming `curr.next` is count 2.\n**Edge Case:** `curr.next` might be count 5.\n**Fix:** Check `curr.next.count`. If it's not `target_count`, create a new node and insert it.\n\n### 3. **Memory Leak in Sets**\n**Problem:** Removing an item from the tracker but leaving it in the `key_to_node` map or the bucket set.\n**Fix:** Ensure explicit `del` and `remove()` calls are symmetric to addition.\n\n---\n\n## \ud83d\udd04 Follow-up Questions\n\n### Follow-up 1: Return Most Recently Updated Content\n\n**Problem:**\n> \"Currently `mostPopular()` returns *any* max item. Change it to return the one that reached that popularity **most recently**.\"\n\n**Solution:**\nInstead of a standard `Set`, use an `OrderedDict` (or Python's insertion-ordered `dict`) inside the Node.\n- **Add:** Append to end (newest).\n- **Access:** `next(reversed(node.keys))` gets the last inserted item.\n\n```python\nclass RecencyNode(Node):\n    def __init__(self, count):\n        super().__init__(count)\n        self.keys = {}  # Ordered Dict\n        \n    def add_key(self, key):\n        self.keys[key] = True  # Append to end\n        \n    def remove_key(self, key):\n        if key in self.keys:\n            del self.keys[key]\n            \n    def get_newest_key(self):\n        # Return last key (most recent)\n        return next(reversed(self.keys)) if self.keys else None\n\nclass RecencyTracker(PopularityTracker):\n    # Override _add_node_after to use RecencyNode\n    def _add_node_after(self, prev_node, count):\n        new_node = RecencyNode(count)\n        # ... (link logic same as parent) ...\n        return new_node\n        \n    def mostPopular(self):\n        if self.tail.prev == self.head: return None\n        return self.tail.prev.get_newest_key()\n```\n\n---\n\n### Follow-up 2: Get Top-K Popular Items\n\n**Problem:**\n> \"Implement `getTopK(k)` to return the k most popular items.\"\n\n**Challenge:**\nWe need to traverse from the tail backwards.\n\n**Algorithm:**\n1. Start at `tail.prev`.\n2. Take all items from this bucket.\n3. If we need more, move to `node.prev`.\n4. Repeat until we have k items or hit head.\n\n```python\n    def getTopK(self, k: int) -> list:\n        result = []\n        current = self.tail.prev\n        \n        while current != self.head and len(result) < k:\n            # Get items from current bucket\n            # Note: Order depends on set implementation (random or insertion)\n            bucket_items = list(current.keys)\n            \n            # Take needed amount\n            needed = k - len(result)\n            result.extend(bucket_items[:needed])\n            \n            current = current.prev\n            \n        return result\n```\n\n**Complexity:** O(K) (assuming buckets aren't huge relative to K).\n\n---\n\n### Follow-up 3: Thread Safety\n\n**Problem:**\n> \"Make the tracker thread-safe for concurrent web requests.\"\n\n**Solution:**\nSince operations are O(1), critical sections are very short. A **Coarse-Grained Lock** (one lock for the whole structure) is efficient and simple.\n\n```python\nimport threading\n\nclass ThreadSafeTracker(PopularityTracker):\n    def __init__(self):\n        super().__init__()\n        self.lock = threading.Lock()\n        \n    def increasePopularity(self, key):\n        with self.lock:\n            super().increasePopularity(key)\n            \n    def mostPopular(self):\n        with self.lock:\n            return super().mostPopular()\n```\n\n---\n\n## \ud83e\uddea Test Cases\n\n```python\ndef test_popularity_tracker():\n    tracker = PopularityTracker()\n    \n    # 1. Basic Increase\n    tracker.increasePopularity(\"A\")\n    assert tracker.mostPopular() == \"A\"\n    \n    # 2. Tie Breaking\n    tracker.increasePopularity(\"B\")\n    assert tracker.mostPopular() in [\"A\", \"B\"]\n    \n    # 3. Separation\n    tracker.increasePopularity(\"B\")\n    assert tracker.mostPopular() == \"B\"\n    \n    # 4. Decrement logic\n    tracker.decreasePopularity(\"B\")\n    assert tracker.mostPopular() in [\"A\", \"B\"]\n    \n    # 5. Top K\n    # A:1, B:1. Add C:3\n    tracker.increasePopularity(\"C\")\n    tracker.increasePopularity(\"C\")\n    tracker.increasePopularity(\"C\")\n    \n    # Top 2 should be [C, A] or [C, B]\n    top2 = tracker.getTopK(2)  # Hypothetical method call\n    assert top2[0] == \"C\"\n    assert len(top2) == 2\n    \n    print(\"Tests Passed!\")\n```\n"
      },
      {
        "type": "file",
        "name": "04_Tennis_Court_Booking.md",
        "content": "# \ud83c\udfbe PROBLEM 4: TENNIS COURT BOOKING\n\n### \u2b50\u2b50\u2b50 **Expanding Tennis Club**\n\n**Frequency:** Medium (Appears in ~30% of rounds)\n**Similar to:** [LeetCode 253 - Meeting Rooms II](https://leetcode.com/problems/meeting-rooms-ii/)\n\n**Problem Statement:**\n> You manage a tennis club. You have a list of court booking requests. Each request has a `start` time and an `end` time.\n>\n> You want to accept **all** bookings.\n>\n> **Task:** Assign each booking to a specific court (Court 1, Court 2, etc.) such that no two bookings on the same court overlap. Minimize the total number of courts needed.\n\n**Visual Example:**\n```text\nBookings:\nA: [0, 30]\nB: [10, 20]\nC: [15, 45]\nD: [50, 70]\n\nTimeline:\n0----10----15----20----30----45----50----70-->\n\nCourt 1: [A: 0-30] ................. [D: 50-70]\nCourt 2: .. [B: 10-20]\nCourt 3: ....... [C: 15-45]\n\nTotal Courts: 3\nNote: B ends at 20, but C starts at 15, so C cannot use Court 2.\n```\n\n**Example Input/Output:**\n```python\nbookings = [\n    BookingRecord(id=1, start=0, finish=30),\n    BookingRecord(id=2, start=15, finish=45),\n    BookingRecord(id=3, start=10, finish=20),\n    BookingRecord(id=4, start=50, finish=70)\n]\n\n# Possible Output\n# Court 1: Booking 1, Booking 4\n# Court 2: Booking 3\n# Court 3: Booking 2\n```\n\n---\n\n### \ud83d\udde3\ufe0f **Interview Conversation Guide**\n\n**Phase 1: Clarification**\n- **Candidate:** \"Do we just need the *count* of courts, or the actual assignment?\"\n- **Interviewer:** \"Let's say we need the actual assignment.\"\n- **Candidate:** \"What if `end_time` of one meeting equals `start_time` of another? Do they overlap?\"\n- **Interviewer:** \"No, [10, 20] and [20, 30] do NOT overlap.\"\n- **Candidate:** \"Is the input sorted?\"\n- **Interviewer:** \"No, assume unsorted.\"\n\n**Phase 2: Approach**\n- **Candidate:** \"This is a classic Interval scheduling problem. We can think of it greedily.\"\n- **Candidate:** \"If we sort meetings by start time, we can process them one by one.\"\n- **Candidate:** \"For each meeting, we check if any existing court is free. If yes, reuse it. If no, create a new court.\"\n- **Candidate:** \"To efficiently find a free court (specifically the one that finishes earliest), we can use a **Min-Heap** storing end times.\"\n\n**Phase 3: Coding**\n- Define `BookingRecord` and `Court` classes.\n- Sort bookings.\n- Use `heapq` to manage court availability.\n\n---\n\n### \ud83d\udcdd **Solution Approach: Greedy with Min-Heap**\n\nWe want to fit a new booking into an *existing* court if possible.\nWhich court is the best candidate? Any court that becomes free **before** the new booking starts.\nIdeally, the court that becomes free *closest* to the start time to minimize gaps, but for minimizing *total courts*, just picking *any* free court works.\n\n**Algorithm:**\n1.  **Sort** bookings by `start_time`.\n2.  Use a **Min-Heap** to track active courts.\n    *   Heap stores: `(end_time, court_index)`.\n    *   The top of the heap tells us the **earliest** time a court becomes free.\n3.  For each booking:\n    *   Check if `heap.min_end_time <= booking.start_time`.\n    *   **If Yes:** We can reuse that court. Pop from heap, update end time, push back.\n    *   **If No:** All courts are busy. Allocate a new court. Push new court to heap.\n\n**Implementation:**\n\n```python\nimport heapq\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass BookingRecord:\n    id: int\n    start_time: int\n    finish_time: int\n\n@dataclass\nclass Court:\n    court_id: int\n    bookings: List[BookingRecord]\n    available_at: int = 0  # When this court becomes free\n\ndef assign_courts(booking_records: List[BookingRecord]) -> List[Court]:\n    if not booking_records:\n        return []\n\n    # 1. Sort by start time\n    bookings = sorted(booking_records, key=lambda x: x.start_time)\n\n    courts = []\n    \n    # Min-heap stores: (available_time, court_index)\n    # This allows us to efficiently find a free court\n    court_heap = []\n\n    for booking in bookings:\n        # Check if the earliest available court is free before this booking starts\n        if court_heap and court_heap[0][0] <= booking.start_time:\n            # Reuse existing court\n            available_time, court_idx = heapq.heappop(court_heap)\n            \n            # Add booking to this court\n            courts[court_idx].bookings.append(booking)\n            courts[court_idx].available_at = booking.finish_time\n\n            # Push back with new available time\n            heapq.heappush(court_heap, (booking.finish_time, court_idx))\n        else:\n            # Need new court\n            court_idx = len(courts)\n            new_court = Court(\n                court_id=court_idx + 1,\n                bookings=[booking],\n                available_at=booking.finish_time\n            )\n            courts.append(new_court)\n            \n            # Push to heap\n            heapq.heappush(court_heap, (booking.finish_time, court_idx))\n\n    return courts\n\n# Time Complexity: O(N log N) for sorting. O(N log K) for heap ops (K = num courts).\n# Space Complexity: O(K) for heap.\n```\n\n---\n\n### \ud83d\udcdd **Alternative: Line Sweep (Count Only)**\n\nIf the interviewer only asks for the **number** of courts needed (not the schedule), use Line Sweep.\n\n1.  Separate start and end times.\n2.  Start time = `+1` court needed.\n3.  End time = `-1` court needed.\n4.  Sort events. Iterate and track max running sum.\n\n```python\ndef min_courts_needed(bookings: List[BookingRecord]) -> int:\n    events = []\n    for b in bookings:\n        events.append((b.start_time, 1))   # Start\n        events.append((b.finish_time, -1)) # End (court freed)\n\n    # Sort: time ascending.\n    # Tie-break: Process END (-1) before START (1) to minimize courts\n    # (If one game ends at 10 and another starts at 10, we can reuse).\n    events.sort(key=lambda x: (x[0], x[1]))\n\n    current_courts = 0\n    max_courts = 0\n\n    for _, change in events:\n        current_courts += change\n        max_courts = max(max_courts, current_courts)\n\n    return max_courts\n```\n\n---\n\n### \ud83d\udd27 **Follow-up 1: Maintenance Time**\n\n**Problem:**\n> After every booking, the court needs `X` minutes of maintenance before it can be used again.\n\n**Solution:**\n> Effectively, the booking \"occupies\" the court until `end_time + maintenance_time`.\n> Just modify the heap push:\n> `heapq.heappush(court_heap, (booking.finish_time + maintenance_time, court_idx))`\n\n```python\ndef assign_courts_with_maintenance(bookings, maintenance_time):\n    # ... sort ...\n    for booking in bookings:\n        if court_heap and court_heap[0][0] <= booking.start_time:\n            _, court_idx = heapq.heappop(court_heap)\n            # ... add booking ...\n            \n            # Next available time includes maintenance\n            heapq.heappush(court_heap, (booking.finish_time + maintenance_time, court_idx))\n        # ... else create new ...\n```\n\n---\n\n### \ud83d\udd27 **Follow-up 2: Periodic Maintenance**\n\n**Problem:**\n> A court only needs maintenance after every `Y` bookings (e.g., every 3 matches).\n\n**Solution:**\n> Track `bookings_count` in the heap or Court object.\n> `heap element: (available_time, court_idx, bookings_since_last_maintenance)`\n\n```python\ndef assign_courts_periodic(bookings, maintenance_time, limit):\n    # ...\n    # Heap: (available_time, court_idx, usage_count)\n    \n    for booking in bookings:\n        if heap and heap[0][0] <= booking.start_time:\n            time, idx, count = heapq.heappop(heap)\n            count += 1\n            \n            next_free = booking.finish_time\n            if count >= limit:\n                next_free += maintenance_time\n                count = 0 # Reset\n            \n            heapq.heappush(heap, (next_free, idx, count))\n        else:\n            # New court\n            heapq.heappush(heap, (booking.finish_time, len(courts), 1))\n```\n\n---\n\n### \ud83e\uddea **Test Cases**\n\n**Basic Overlap:**\n- Input: `[0, 10], [5, 15]`\n- Output: 2 Courts.\n\n**Reuse:**\n- Input: `[0, 10], [10, 20]`\n- Output: 1 Court (Ends at 10, Starts at 10).\n\n**Nested:**\n- Input: `[0, 100], [10, 20], [30, 40]`\n- Output: 2 Courts (One holds [0, 100], other holds the rest).\n\n**Empty:**\n- Input: `[]`\n- Output: `[]` (or 0 courts).\n"
      },
      {
        "type": "file",
        "name": "05_Router_Wildcards.md",
        "content": "# \ud83d\udee3\ufe0f PROBLEM 5: DYNAMIC ROUTE MATCHING WITH WILDCARDS\n\n### \u2b50\u2b50\u2b50 **Middleware Router**\n\n**Frequency:** Low-Medium (Appears in ~25% of rounds)\n**Similar to:** [LeetCode 208. Implement Trie (Prefix Tree)](https://leetcode.com/problems/implement-trie-prefix-tree/) (but for path segments)\n\n**Problem Statement:**\n> Implement a router that matches URL paths.\n> - Paths consist of segments separated by `/` (e.g., `/foo/bar`).\n> - Support **wildcards** (`*`) which match exactly one segment.\n> - Support adding routes and calling routes.\n\n**Visual Example:**\n```text\nRoutes Added:\n1. /foo/bar      -> Result: \"A\"\n2. /foo/*/baz    -> Result: \"B\"\n\nTrie Structure:\nroot\n \u2514\u2500 \"foo\"\n     \u251c\u2500 \"bar\" -> (Result \"A\")\n     \u2514\u2500 \"*\"   -> \"baz\" -> (Result \"B\")\n\nQuery: /foo/xyz/baz\nPath: root -> \"foo\" -> \"xyz\"? (No) -> \"*\" (Yes, matches \"xyz\") -> \"baz\" (Yes) -> Result \"B\"\n```\n\n**Example Usage:**\n```python\nrouter = Router()\n\nrouter.addRoute(\"/foo\", \"foo\")\nrouter.addRoute(\"/bar/*/baz\", \"bar\")\n\nprint(router.callRoute(\"/bar/anything/baz\"))  # \"bar\"\nprint(router.callRoute(\"/bar/xyz/baz\"))       # \"bar\"\nprint(router.callRoute(\"/foo\"))                # \"foo\"\nprint(router.callRoute(\"/unknown\"))            # None\n```\n\n---\n\n### \ud83d\udde3\ufe0f **Interview Conversation Guide**\n\n**Phase 1: Clarification**\n- **Candidate:** \"Does `*` match multiple segments (like `/**` in some frameworks) or just one?\"\n- **Interviewer:** \"Just one segment.\"\n- **Candidate:** \"If we have `/foo/bar` and `/foo/*`, which one takes precedence?\"\n- **Interviewer:** \"Exact matches should have higher priority than wildcards.\"\n- **Candidate:** \"Are paths case sensitive?\"\n- **Interviewer:** \"Yes.\"\n\n**Phase 2: Approach**\n- **Candidate:** \"Since we are matching prefixes and segments, a **Trie (Prefix Tree)** is the perfect data structure.\"\n- **Candidate:** \"Instead of characters, each Trie Node will store a path segment (string).\"\n- **Candidate:** \"When adding a route, we split by `/` and insert nodes.\"\n- **Candidate:** \"When searching, we traverse. If an exact match is not found, we check if a `*` child exists.\"\n\n**Phase 3: Coding**\n- Define `TrieNode`.\n- Implement `addRoute` (iterative).\n- Implement `callRoute` (recursive/DFS to handle backtracking if needed, though iterative often works if priority is simple).\n\n---\n\n### \ud83d\udcdd **Solution Approach: Trie (Prefix Tree)**\n\nInstead of storing characters (like a standard Dictionary Trie), we store **path segments** as nodes.\n\n**Data Structure:**\n*   `TrieNode`:\n    *   `children`: Map `segment_string` -> `TrieNode`\n    *   `is_wildcard`: Boolean (or store `*` in children)\n    *   `result`: Value to return if this node is a valid endpoint.\n\n**Algorithm:**\n*   **Add Route**: Split path by `/`. Traverse/Create nodes.\n*   **Call Route**: Split path. Recursive Search (DFS).\n    *   If exact match found in `children`, go there.\n    *   If `*` exists in `children`, also try that (backtracking might be needed if we want to find *any* match, or specific priority).\n\n**Implementation:**\n\n```python\nclass TrieNode:\n    def __init__(self):\n        self.children = {}  # segment -> TrieNode\n        self.result = None  # Not None means this is an endpoint\n\nclass Router:\n    def __init__(self):\n        self.root = TrieNode()\n\n    def addRoute(self, path: str, result: str) -> None:\n        # Split path, filtering empty strings (caused by leading /)\n        segments = [s for s in path.split('/') if s]\n        \n        node = self.root\n        for segment in segments:\n            if segment == '*':\n                if '*' not in node.children:\n                    node.children['*'] = TrieNode()\n                node = node.children['*']\n            else:\n                if segment not in node.children:\n                    node.children[segment] = TrieNode()\n                node = node.children[segment]\n        \n        node.result = result\n\n    def callRoute(self, path: str) -> str:\n        segments = [s for s in path.split('/') if s]\n        return self._search(self.root, segments, 0)\n\n    def _search(self, node: TrieNode, segments: list, index: int) -> str:\n        # Base Case: End of path\n        if index == len(segments):\n            return node.result\n\n        segment = segments[index]\n\n        # Strategy: Try Exact Match FIRST, then Wildcard\n        \n        # 1. Try Exact Match\n        if segment in node.children:\n            res = self._search(node.children[segment], segments, index + 1)\n            if res is not None:\n                return res\n\n        # 2. Try Wildcard Match\n        if '*' in node.children:\n            res = self._search(node.children['*'], segments, index + 1)\n            if res is not None:\n                return res\n\n        return None\n\n# Time Complexity:\n# addRoute: O(K), K = number of segments\n# callRoute: O(K) in best case (direct match), O(2^K) worst case if every node has both exact and wildcard and we backtrack (rare in URLs).\n```\n\n---\n\n### \ud83d\udd04 **Follow-up 1: Priority Rules**\n\n**Problem:**\n> What if both `/foo/bar` and `/foo/*` exist? Which one should `/foo/bar` match?\n> **Rule:** Exact match > Wildcard match.\n\n**Solution:**\n> The DFS order naturally handles this.\n> We check `if segment in node.children` (Exact) **before** checking `*`.\n> If the Exact path leads to a dead end (no result), we backtrack and try Wildcard.\n\n---\n\n### \ud83d\udd04 **Follow-up 2: Path Parameters**\n\n**Problem:**\n> Support routes like `/users/{id}/posts` where `{id}` captures any value, and we need to return the captured params.\n\n**Solution:**\n> 1.  Modify `addRoute` to detect `{...}` segments. Treat them like wildcards but store the param name.\n> 2.  Modify `callRoute` to return `(result, params_dict)`.\n> 3.  During DFS, if we take a parameter/wildcard edge, add `param_name: current_segment` to the collected params.\n\n```python\nclass ParamRouter:\n    # ... (TrieNode has self.param_name = None) ...\n\n    def addRoute(self, path, result):\n        # ... inside loop ...\n        if segment.startswith('{') and segment.endswith('}'):\n            param_name = segment[1:-1]\n            if '*' not in node.children:\n                node.children['*'] = TrieNode()\n                node.children['*'].param_name = param_name\n            node = node.children['*']\n        # ...\n\n    def callRoute(self, path):\n        return self._dfs(self.root, segments, 0, {})\n\n    def _dfs(self, node, segments, index, params):\n        if index == len(segments):\n            return (node.result, params) if node.result else None\n\n        current_seg = segments[index]\n\n        # 1. Try Exact\n        if current_seg in node.children:\n            res = self._dfs(node.children[current_seg], segments, index+1, params)\n            if res: return res\n\n        # 2. Try Param/Wildcard\n        if '*' in node.children:\n            child = node.children['*']\n            # Copy params to avoid polluting other branches if backtracking\n            new_params = params.copy()\n            if child.param_name:\n                new_params[child.param_name] = current_seg\n            \n            res = self._dfs(child, segments, index+1, new_params)\n            if res: return res\n            \n        return None\n```\n\n---\n\n### \ud83e\uddea **Test Cases**\n\n**Basic:**\n- Add `/a/b`, Call `/a/b` -> Found.\n- Call `/a/c` -> None.\n\n**Wildcard:**\n- Add `/a/*/c`. Call `/a/b/c` -> Found.\n- Call `/a/b/d` -> None.\n\n**Priority:**\n- Add `/a/b` (Exact) and `/a/*` (Wildcard).\n- Call `/a/b` -> Should return Exact match result.\n- Call `/a/z` -> Should return Wildcard match result.\n\n**Edge Cases:**\n- Root path `/`.\n- Path with trailing slash (handle by splitting logic).\n- Empty segments `//` (handle by filtering).\n"
      },
      {
        "type": "file",
        "name": "06_Commodity_Prices.md",
        "content": "# \ud83c\udfb2 PROBLEM 6: COMMODITY PRICES WITH CHECKPOINTS\n\n### \u2b50\u2b50 **Commodity Price Tracker**\n\n**Frequency:** Low\n**Similar to:** Range Maximum Query (RMQ) problems.\n\n**Problem Statement:**\n> You receive a stream of commodity price updates: `(timestamp, price)`.\n> The stream is not sorted. You might get updates for past timestamps.\n>\n> Implement:\n> - `update(timestamp, price)`: Record/Update price.\n> - `getMaxPrice(timestamp)`: Return the maximum price recorded at any time `t <= timestamp`.\n\n**Visual Example:**\n```text\nEvents:\n1. (t=1, p=100)\n2. (t=3, p=150)\n3. (t=2, p=120)  <-- Out of order\n\nData: {1: 100, 2: 120, 3: 150}\n\nQueries:\ngetMaxPrice(1) -> 100\ngetMaxPrice(2) -> 120 (Max of prices at 1, 2)\ngetMaxPrice(3) -> 150 (Max of prices at 1, 2, 3)\n```\n\n---\n\n### \ud83d\udde3\ufe0f **Interview Conversation Guide**\n\n**Phase 1: Clarification**\n- **Candidate:** \"Is `timestamp` always increasing in `update`?\"\n- **Interviewer:** \"No, it can be out of order (corrections).\"\n- **Candidate:** \"Is `timestamp` continuous or sparse?\"\n- **Interviewer:** \"Sparse (e.g., 1, 5, 1000).\"\n- **Candidate:** \"Is read or write more frequent?\"\n- **Interviewer:** \"Assume relatively balanced, or slightly more reads.\"\n\n**Phase 2: Approach**\n- **Candidate:** \"We need to store prices by timestamp. A Hash Map works for storage.\"\n- **Candidate:** \"However, `getMaxPrice(t)` asks for max in range `[0, t]`. Hash Map doesn't support range queries efficiently.\"\n- **Candidate:** \"We could use a **Sorted List** of `(timestamp, price)`.\"\n- **Candidate:** \"For `getMaxPrice(t)`, we find all entries $\\le t$. But scanning them is $O(N)$.\"\n- **Candidate:** \"We can cache the **Prefix Max**. `prefix_max[i] = max(data[0]...data[i])`. Then binary search is $O(\\log N)$.\"\n- **Candidate:** \"Challenge: An update to a past timestamp invalidates all subsequent prefix maxes ($O(N)$ update).\"\n\n**Phase 3: Coding**\n- Implement the pragmatic `SortedList` + `Lazy Prefix Max` approach (good for interviews unless Segment Tree is explicitly asked).\n\n---\n\n### \ud83d\udcdd **Solution Approach 1: SortedDict + Prefix Max Cache**\n\nIf we assume read-heavy workload, we can maintain a cached \"Prefix Max\".\nHowever, updates are expensive (O(N)) if we change a past value, as it invalidates all subsequent prefix maxes.\n\n**Better for Interview (Balanced):**\nUse a **Segment Tree** (Coordinate Compression if timestamps are large) OR a **SortedDict** combined with smart caching.\n\nSince implementation of a Segment Tree is heavy, here is a pragmatic approach using **SortedDict** (bisect in Python) for data storage and a simple **Memoization** or **Heap** strategy if specific constraints are given.\n\n**If simply \"Max price ever\":** O(1) variable.\n**If \"Max price in range [0, t]\":** This is a Prefix Max problem.\n\n**Python Solution (Bisect + Lazy Prefix Max)**\n*Note: Efficient dynamic RMQ is hard without a Segment Tree. This solution assumes we might recompute or the data is relatively sparse.*\n\n```python\nimport bisect\n\nclass CommodityTracker:\n    def __init__(self):\n        # List of [timestamp, price] sorted by timestamp\n        self.data = [] \n        # Cache for prefix max: index -> max_price_in_data[:index+1]\n        self.prefix_max = []\n        self.is_dirty = False\n\n    def update(self, timestamp: int, price: int) -> None:\n        # O(N) insertion to keep sorted\n        # In interview, can discuss using Balanced BST (O(log N))\n        \n        # Check if exists\n        idx = bisect.bisect_left(self.data, [timestamp, -1])\n        \n        if idx < len(self.data) and self.data[idx][0] == timestamp:\n            self.data[idx][1] = price\n        else:\n            bisect.insort(self.data, [timestamp, price])\n            \n        self.is_dirty = True\n\n    def _rebuild_prefix_max(self):\n        # O(N) - acceptable if reads >> writes\n        if not self.data:\n            return\n            \n        self.prefix_max = [0] * len(self.data)\n        curr_max = float('-inf')\n        \n        for i, (ts, price) in enumerate(self.data):\n            curr_max = max(curr_max, price)\n            self.prefix_max[i] = curr_max\n            \n        self.is_dirty = False\n\n    def getMaxPrice(self, timestamp: int) -> int:\n        if self.is_dirty:\n            self._rebuild_prefix_max()\n            \n        if not self.data:\n            return None\n            \n        # Find rightmost index <= timestamp\n        # bisect_right searches based on the first element of list comparison\n        # We use a dummy [timestamp, INF] to find the upper bound\n        idx = bisect.bisect_right(self.data, [timestamp, float('inf')])\n        \n        if idx == 0:\n            return None\n            \n        return self.prefix_max[idx - 1]\n\n# Time: Update O(N), Query O(log N) + O(N) rebuild if dirty.\n```\n\n---\n\n### \ud83d\ude80 **Optimal Solution: Segment Tree**\n\nIf `update` and `query` must both be **O(log N)**, you must use a **Segment Tree**.\n\n1.  **Coordinate Compression**: If timestamps are large (e.g., unix timestamps), map them to indices 0..N based on sorted unique timestamps seen.\n2.  **Segment Tree**: Stores max in range.\n\n```python\nclass SegmentTree:\n    def __init__(self, size):\n        self.n = size\n        self.tree = [0] * (2 * size)\n\n    def update(self, i, val):\n        i += self.n\n        self.tree[i] = val\n        while i > 1:\n            i //= 2\n            self.tree[i] = max(self.tree[2*i], self.tree[2*i+1])\n\n    def query(self, l, r):\n        # Max in range [l, r)\n        l += self.n\n        r += self.n\n        res = 0\n        while l < r:\n            if l % 2 == 1:\n                res = max(res, self.tree[l])\n                l += 1\n            if r % 2 == 1:\n                r -= 1\n                res = max(res, self.tree[r])\n            l //= 2\n            r //= 2\n        return res\n```\n\n---\n\n### \ud83d\udd16 **Follow-up: Checkpoints**\n\n**Problem:**\n> Every `k` updates, we drop a \"checkpoint\". We want to find the max price *relative to* checkpoint numbers, not timestamps.\n\n**Solution:**\n> This simplifies to an array problem.\n> Store checkpoints in a list: `[(ts1, p1), (ts2, p2), ...]`.\n> Maintain a `max_so_far` array aligned with this list.\n> Querying by \"checkpoint number\" is just array indexing `O(1)`.\n\n```python\nclass CheckpointTracker:\n    def __init__(self):\n        self.checkpoints = [] # Stores (timestamp, price)\n        self.max_history = [] # Stores max price up to index i\n\n    def update(self, timestamp, price):\n        self.checkpoints.append((timestamp, price))\n        \n        current_max = price\n        if self.max_history:\n            current_max = max(self.max_history[-1], price)\n        \n        self.max_history.append(current_max)\n\n    def getMaxAtCheckpoint(self, checkpoint_idx):\n        if 0 <= checkpoint_idx < len(self.max_history):\n            return self.max_history[checkpoint_idx]\n        return None\n```\n\n---\n\n### \ud83e\uddea **Test Cases**\n\n**Basic:**\n- Add (1, 100), (2, 200). Max(2) -> 200.\n- Add (3, 50). Max(3) -> 200.\n\n**Out of Order:**\n- Add (1, 100), (3, 100).\n- Add (2, 200). Max(2) -> 200. Max(3) -> 200.\n\n**Sparse:**\n- Timestamps 1, 1000, 1000000.\n- `getMaxPrice(500)` -> Should return max at timestamp 1.\n"
      },
      {
        "type": "file",
        "name": "07_File_Collections.md",
        "content": "# \ud83d\udcc2 PROBLEM 7: FILE COLLECTIONS REPORT\n\n### \u2b50\u2b50\u2b50 **File Size & Top-K Collections**\n\n**Frequency:** Medium\n**Similar to:** Top K Frequent Elements, but with aggregation.\n\n**Problem Statement:**\n> You are given a list of files. Each file has a `name`, `size`, and `collection_id` (optional).\n>\n> You need to generate a report that shows:\n> 1.  **Total Size** of files stored in the system.\n> 2.  **Top K Collections** by total size of files within them.\n\n**Input Format:**\n```python\nfiles = [\n    {\"file\": \"file1.txt\", \"size\": 100, \"collectionId\": \"col1\"},\n    {\"file\": \"file2.txt\", \"size\": 200, \"collectionId\": \"col1\"},\n    {\"file\": \"file3.txt\", \"size\": 200, \"collectionId\": \"col2\"},\n    {\"file\": \"file4.txt\", \"size\": 100, \"collectionId\": None} \n]\n```\n\n**Output:**\n- Total Size: 600\n- Top K (e.g., K=2) Collections: `[\"col1\" (300), \"col2\" (200)]`\n\n---\n\n### \ud83d\udde3\ufe0f **Interview Conversation Guide**\n\n**Phase 1: Clarification**\n- **Candidate:** \"What do we do with files that have `collectionId: None`?\"\n- **Interviewer:** \"Include them in 'Total Size', but ignore them for the 'Top K Collections' report.\"\n- **Candidate:** \"Can the file size be negative?\"\n- **Interviewer:** \"No.\"\n- **Candidate:** \"Is K usually small?\"\n- **Interviewer:** \"Yes, K is much smaller than the number of collections.\"\n\n**Phase 2: Approach**\n- **Candidate:** \"This is a standard aggregation problem.\"\n- **Candidate:** \"We can iterate through the files once ($O(N)$).\"\n- **Candidate:** \"Maintain a running `total_size`.\"\n- **Candidate:** \"Use a Hash Map (`collection_map`) to aggregate size per collection.\"\n- **Candidate:** \"After processing all files, we have a map of `{col1: 300, col2: 200}`.\"\n- **Candidate:** \"To find Top K, we can sort the map items ($O(C \\log C)$ where $C$ is num collections) or use a **Min-Heap** of size K ($O(C \\log K)$).\"\n- **Candidate:** \"Since $C$ can be large and $K$ is small, the Heap approach is optimal.\"\n\n**Phase 3: Coding**\n- Define `process_files` function.\n- Use `defaultdict` for aggregation.\n- Use `heapq.nlargest` or maintain a custom heap.\n\n---\n\n### \ud83d\udcdd **Solution Approach: HashMap + Heap**\n\n**Steps:**\n1.  **Aggregate:** Loop through files. Sum global size. Sum per-collection size in a HashMap.\n2.  **Top K:** Use `heapq.nlargest` on the items of the HashMap.\n\n**Implementation:**\n\n```python\nfrom collections import defaultdict\nimport heapq\nfrom typing import List, Dict, Optional\n\ndef generate_report(files: List[Dict], k: int):\n    total_size = 0\n    collection_sizes = defaultdict(int)\n\n    # 1. Aggregation Phase\n    for f in files:\n        size = f.get(\"size\", 0)\n        col_id = f.get(\"collectionId\")\n\n        # Add to global total\n        total_size += size\n\n        # Add to collection total if it belongs to one\n        if col_id is not None:\n            collection_sizes[col_id] += size\n\n    # 2. Top K Phase\n    # collection_sizes.items() returns [(col1, 300), (col2, 200), ...]\n    # We want to sort by size (item[1]) descending.\n    \n    top_k_collections = heapq.nlargest(\n        k, \n        collection_sizes.items(), \n        key=lambda item: item[1]\n    )\n\n    # Formatting output\n    return {\n        \"total_size\": total_size,\n        \"top_collections\": top_k_collections\n    }\n\n# Usage\ndata = [\n    {\"file\": \"a.txt\", \"size\": 100, \"collectionId\": \"c1\"},\n    {\"file\": \"b.txt\", \"size\": 200, \"collectionId\": \"c1\"},\n    {\"file\": \"c.txt\", \"size\": 300, \"collectionId\": \"c2\"},\n    {\"file\": \"d.txt\", \"size\": 50,  \"collectionId\": None}\n]\n\nreport = generate_report(data, k=1)\nprint(report)\n# Output: {'total_size': 650, 'top_collections': [('c2', 300)]}\n# c1 total is 300, c2 is 300. Ties broken arbitrarily or by key stability.\n```\n\n**Complexity Analysis:**\n- **Time:** $O(N + C \\log K)$. $N$ to iterate files. $C$ to iterate collections (where $C \\le N$). Heap operations take $\\log K$.\n- **Space:** $O(C)$ to store the map.\n\n---\n\n### \ud83d\udd04 **Follow-up 1: Streaming / Large Dataset**\n\n**Problem:**\n> The file list is too large to fit in memory. It comes as a stream.\n> You cannot store all collection IDs in a map if there are millions of unique collections.\n> However, you assume **Top K** are significantly larger than the \"long tail\" of small collections.\n\n**Solution:**\n> This becomes a **Heavy Hitters** problem (e.g., Count-Min Sketch or Misra-Gries).\n> But generally for interview simplification:\n> - If we can't store *all*, can we store *some*? LRU cache approach?\n> - Or simply, assume we can store the map (usually fits), but maybe we process files in chunks (MapReduce).\n> - **MapReduce**:\n>   - Mapper: Reads chunk of files, emits `(col_id, size)`.\n>   - Reducer: Sums sizes for `col_id`.\n>   - Finalizer: Finds Top K.\n\n---\n\n### \ud83e\uddea **Test Cases**\n\n**Basic:**\n- 2 collections, K=1. Returns largest.\n- Files with `None` collection. Checked `total_size` matches.\n\n**Ties:**\n- col1: 100, col2: 100. `heapq` handles ties (usually by key).\n\n**Edge Cases:**\n- Empty file list -> Total 0, Top K empty.\n- K=0 -> Top K empty.\n- K > Number of collections -> Returns all collections sorted.\n- File size 0.\n"
      },
      {
        "type": "file",
        "name": "08_Robot_Parts.md",
        "content": "# \ud83e\udd16 PROBLEM 8: ROBOT PARTS ASSEMBLY\n\n### \u2b50\u2b50 **Inventory Management**\n\n**Frequency:** Low\n**Similar to:** Group Anagrams / Set checking.\n\n**Problem Statement:**\n> You are building robots. Each robot requires a specific set of parts to be built.\n> You have an inventory of parts.\n>\n> Given a list of **required parts** for a robot (e.g., `[\"wheel\", \"motor\", \"sensor\"]`) and a list of **available parts** in inventory (e.g., `[\"motor\", \"wheel\", \"sensor\", \"wheel\"]`), determine:\n> 1.  Can we build the robot?\n> 2.  If yes, remove the used parts from inventory and return success.\n> 3.  If no, return list of missing parts.\n\n**Visual Example:**\n```text\nRequired: [A, A, B]\nInventory: [A, B, A, C]\n\n1. Count Required: {A: 2, B: 1}\n2. Count Inventory: {A: 2, B: 1, C: 1}\n3. Compare:\n   Inventory has 2 A's? Yes.\n   Inventory has 1 B? Yes.\n   \nResult: Success! Inventory becomes [C].\n```\n\n---\n\n### \ud83d\udde3\ufe0f **Interview Conversation Guide**\n\n**Phase 1: Clarification**\n- **Candidate:** \"Does the order of parts matter?\"\n- **Interviewer:** \"No, `[A, B]` is same as `[B, A]`.\"\n- **Candidate:** \"Are there duplicates in requirements?\"\n- **Interviewer:** \"Yes, a robot might need 4 wheels.\"\n- **Candidate:** \"Should I optimize for one-time check or multiple checks?\"\n- **Interviewer:** \"Assume we have one big inventory and multiple robot build requests coming in.\"\n\n**Phase 2: Approach**\n- **Candidate:** \"Since frequency matters (e.g., needing 2 wheels), a `Set` is not enough. We need a **Frequency Map (HashMap / Counter)**.\"\n- **Candidate:** \"We can store the Inventory as a Hash Map: `{'wheel': 10, 'motor': 5}`.\"\n- **Candidate:** \"For each request, we create a count of required parts.\"\n- **Candidate:** \"Then iterate through requirements and check if Inventory has enough.\"\n- **Candidate:** \"If checks pass, decrement inventory.\"\n\n**Phase 3: Coding**\n- Use `collections.Counter` in Python.\n- Handle the \"Transactional\" nature (don't remove parts if you can't complete the full robot).\n\n---\n\n### \ud83d\udcdd **Solution Approach: Frequency Map**\n\n**Data Structure:** `HashMap<PartName, Count>`\n\n**Algorithm:**\n1.  Convert Inventory list to Map (if not already).\n2.  Count requirements.\n3.  **Check Phase**: Iterate requirements. If `inventory[part] < required[part]`, collect missing.\n4.  **Update Phase**: If no missing parts, decrement inventory.\n\n**Implementation:**\n\n```python\nfrom collections import Counter\nfrom typing import List, Dict\n\nclass RobotBuilder:\n    def __init__(self, inventory_list: List[str]):\n        # Initialize inventory\n        self.inventory = Counter(inventory_list)\n\n    def build_robot(self, required_parts: List[str]):\n        # 1. Count requirements\n        req_counts = Counter(required_parts)\n        missing = []\n\n        # 2. Check availability\n        for part, count in req_counts.items():\n            if self.inventory[part] < count:\n                needed = count - self.inventory[part]\n                # Add missing part 'needed' times or just once with count\n                missing.append(f\"{part} (x{needed})\")\n\n        if missing:\n            return False, missing\n\n        # 3. Commit transaction (Remove parts)\n        for part, count in req_counts.items():\n            self.inventory[part] -= count\n            # Optional: clean up 0 counts to save space\n            if self.inventory[part] == 0:\n                del self.inventory[part]\n\n        return True, []\n\n# Usage\ninitial_parts = [\"wheel\", \"wheel\", \"motor\", \"sensor\", \"cable\"]\nbuilder = RobotBuilder(initial_parts)\n\n# Request 1: Success\nsuccess, msg = builder.build_robot([\"wheel\", \"motor\"])\nprint(f\"Build 1: {success}\") # True\n# Inventory now: {wheel: 1, sensor: 1, cable: 1}\n\n# Request 2: Fail\nsuccess, msg = builder.build_robot([\"wheel\", \"wheel\"]) \nprint(f\"Build 2: {success}, Missing: {msg}\") \n# False, Missing wheel (need 2, have 1)\n```\n\n---\n\n### \ud83d\udd04 **Follow-up 1: Thread Safety**\n\n**Problem:**\n> Multiple robot arms are taking parts from the same inventory concurrently.\n\n**Solution:**\n> Needs a lock around the `Check + Update` block.\n> This block must be **Atomic**. You can't check, release lock, then update. The inventory might change in between.\n\n```python\n    def build_robot_thread_safe(self, required_parts):\n        with self.lock:\n            # ... same logic ...\n```\n\n---\n\n### \ud83e\uddea **Test Cases**\n\n**Basic:**\n- Exact match.\n- Partial match (Success).\n- Missing one item.\n- Missing quantity (Need 2, have 1).\n\n**Edge Cases:**\n- Empty requirement (Free to build).\n- Empty inventory.\n- Part names case sensitivity (assume sensitive).\n"
      },
      {
        "type": "file",
        "name": "09_Vote_Counting.md",
        "content": "# \ud83d\uddf3\ufe0f PROBLEM 9: VOTE COUNTING / LEADERBOARD\n\n### \u2b50\u2b50\u2b50 **Ranked Choice Voting / Top Candidates**\n\n**Frequency:** Medium\n**Similar to:** Top K, Custom Sorting\n\n**Problem Statement:**\n> You have a list of votes. Each vote contains a candidate name.\n>\n> **Part 1 (Simple):** Return the winner (most votes). If tie, lexicographically last (or first, depending on rule).\n>\n> **Part 2 (Weighted):** Each vote has a \"weight\" (e.g., 3 points, 2 points, 1 point).\n>\n> **Part 3 (Leaderboard):** Return Top 3 candidates.\n\n**Example:**\n```python\nvotes = [\"A\", \"B\", \"A\", \"C\", \"B\", \"B\"]\n# A: 2, B: 3, C: 1\n# Winner: B\n```\n\n---\n\n### \ud83d\udde3\ufe0f **Interview Conversation Guide**\n\n**Phase 1: Clarification**\n- **Candidate:** \"How are votes received? Array or Stream?\"\n- **Interviewer:** \"Array for now.\"\n- **Candidate:** \"How to break ties?\"\n- **Interviewer:** \"If vote counts are equal, choose the one that is alphabetically **last** (e.g., 'Bob' > 'Alice').\"\n\n**Phase 2: Approach**\n- **Candidate:** \"I will use a Hash Map to count votes.\"\n- **Candidate:** \"Then, I will convert the map to a list of objects/tuples.\"\n- **Candidate:** \"I need to sort this list. Primary key: `count` (descending). Secondary key: `name` (descending for 'last' rule).\"\n\n**Phase 3: Coding**\n- Use `collections.Counter`.\n- Use Python's sort with a tuple key `(-count, name)` (min-heap style) or `(count, name)` with `reverse=True`.\n\n---\n\n### \ud83d\udcdd **Solution Approach: Counting & Sorting**\n\n**Part 1 & 3 Combined (Top K)**\n\n```python\nfrom collections import Counter, defaultdict\n\ndef get_leaderboard(votes, k=3):\n    # 1. Count\n    # If input is just names:\n    counts = Counter(votes)\n    \n    # If input is weighted (Name, Points):\n    # counts = defaultdict(int)\n    # for name, points in votes:\n    #     counts[name] += points\n\n    # 2. Sort\n    # We want Max Votes first. If Tie, Max Name (Alphabetically last) first.\n    # Python's sort is stable and ascending by default.\n    # We can sort by (-count, -name) to get Ascending order equivalent to Descending?\n    # No, easier: Sort by (count, name) and take reverse.\n    \n    candidates = list(counts.items()) # [(Name, Count), ...]\n    \n    # Sort Key:\n    # Primary: x[1] (Count)\n    # Secondary: x[0] (Name)\n    candidates.sort(key=lambda x: (x[1], x[0]), reverse=True)\n\n    # 3. Return Top K\n    # Return just names\n    return [name for name, count in candidates[:k]]\n\n# Test\nvotes = [\"Alice\", \"Bob\", \"Alice\", \"Charlie\", \"Bob\", \"Bob\"]\nprint(get_leaderboard(votes, k=1)) # ['Bob']\n```\n\n**Note on Tie-Breaking:**\n- If rule: \"Alphabetically First\" (Alice > Bob on tie).\n- Sort Key: `(-x[1], x[0])` (Count descending, Name ascending).\n- `candidates.sort(key=lambda x: (-x[1], x[0]))`.\n\n---\n\n### \ud83e\uddea **Test Cases**\n\n**Tie Breaking:**\n- A: 2, B: 2.\n- Rule: Last name wins -> Returns B.\n- Rule: First name wins -> Returns A.\n\n**Single Vote:**\n- Works correctly.\n\n**No Votes:**\n- Returns empty list.\n"
      },
      {
        "type": "file",
        "name": "10_Word_Wrap.md",
        "content": "# \ud83d\udcdd PROBLEM 10: WORD WRAP / TEXT JUSTIFICATION\n\n### \u2b50\u2b50\u2b50\u2b50 **Text Formatting**\n\n**Frequency:** Medium-High\n**Similar to:** [LeetCode 68. Text Justification](https://leetcode.com/problems/text-justification/)\n\n**Problem Statement:**\n> Given a list of words and a `maxWidth`, format the text such that each line has exactly `maxWidth` characters.\n>\n> **Rules:**\n> 1.  Pack as many words as you can in each line.\n> 2.  Pad extra spaces `' '` evenly between words.\n> 3.  If spaces don't divide evenly, place more spaces on the left slots.\n> 4.  **Last Line** should be left-justified (no extra padding between words).\n\n**Visual Example:**\n```text\nWords: [\"This\", \"is\", \"an\", \"example\", \"of\", \"text\", \"justification.\"]\nWidth: 16\n\nLine 1: \"This    is    an\"  (3 words, 8 letters. 8 spaces needed. 4, 4)\nLine 2: \"example  of text\"  (3 words, 13 letters. 3 spaces. 2, 1)\nLine 3: \"justification.  \"  (Last line, left justified, pad end)\n```\n\n---\n\n### \ud83d\udde3\ufe0f **Interview Conversation Guide**\n\n**Phase 1: Clarification**\n- **Candidate:** \"How do we handle a single word that is longer than `maxWidth`?\"\n- **Interviewer:** \"Assume guaranteed that every word fits within `maxWidth`.\"\n- **Candidate:** \"For the last line, just single space between words and pad right?\"\n- **Interviewer:** \"Correct.\"\n\n**Phase 2: Approach**\n- **Candidate:** \"This is a Greedy approach line by line.\"\n- **Candidate:** \"1. Identify the range of words `[i, j]` that fit in current line.\"\n- **Candidate:** \"2. Calculate total spaces needed = `maxWidth - total_letters`.\"\n- **Candidate:** \"3. Distribute spaces.\"\n- **Candidate:** \"   - If it's the last line or only 1 word: Left justify (pad right).\"\n- **Candidate:** \"   - Otherwise: Round Robin distribution of spaces.\"\n\n**Phase 3: Coding**\n- Carefully manage indices `i` and `j`.\n- Write a helper `create_line(words, maxWidth, is_last)`.\n\n---\n\n### \ud83d\udcdd **Solution Approach: Greedy with Space Distribution**\n\n```python\nfrom typing import List\n\ndef fullJustify(words: List[str], maxWidth: int) -> List[str]:\n    res = []\n    i = 0\n    n = len(words)\n    \n    while i < n:\n        # 1. Find words that fit in this line\n        line_words = []\n        line_len = 0\n        j = i\n        \n        while j < n:\n            # Length if we add this word: \n            # current_len + len(new_word) + (1 space if not first word)\n            added_len = len(words[j])\n            if line_len + added_len + (1 if line_words else 0) > maxWidth:\n                break\n            \n            if line_words:\n                line_len += 1 # standard space\n            line_len += added_len\n            line_words.append(words[j])\n            j += 1\n            \n        # Now line_words contains words for this line\n        # Range is words[i:j]\n        \n        # 2. Format the line\n        remaining_slots = maxWidth - sum(len(w) for w in line_words)\n        \n        # Case A: Last line or Single word -> Left Justified\n        if j == n or len(line_words) == 1:\n            joined = \" \".join(line_words)\n            res.append(joined + \" \" * (maxWidth - len(joined)))\n            \n        # Case B: Fully Justified\n        else:\n            # Gaps between words\n            gaps = len(line_words) - 1\n            spaces_per_gap = remaining_slots // gaps\n            extra_spaces = remaining_slots % gaps\n            \n            line_str = \"\"\n            for k, w in enumerate(line_words):\n                line_str += w\n                if k < gaps:\n                    # Add calculated spaces\n                    # First 'extra_spaces' gaps get +1 space\n                    spaces = spaces_per_gap + (1 if k < extra_spaces else 0)\n                    line_str += \" \" * spaces\n            res.append(line_str)\n            \n        i = j # Move to next batch\n        \n    return res\n\n# Test\ntext = [\"This\", \"is\", \"an\", \"example\", \"of\", \"text\", \"justification.\"]\nfor line in fullJustify(text, 16):\n    print(f\"|{line}|\")\n```\n\n---\n\n### \ud83d\udd04 **Follow-up: DP Variant (Minimize raggedness)**\n\n**Problem:**\n> Instead of greedy packing, choose line breaks such that the sum of squares of empty spaces on each line is minimized. (Like LaTeX/TeX).\n\n**Solution:**\n> This is a standard **Dynamic Programming** problem (Word Wrap).\n> `DP[i]` = Min cost to format words `i...n`.\n> Cost of a line = `(spaces_remaining)^2`.\n> `DP[i] = min(Cost(i, j) + DP[j+1])` for all valid `j`.\n\n---\n\n### \ud83e\uddea **Test Cases**\n\n**Basic:**\n- Words fit exactly (0 extra spaces).\n- Words need uneven spacing (4 spaces, 3 gaps -> 2, 1, 1).\n\n**Edge:**\n- Single word in input.\n- Single word per line (long words).\n- Last line has exact width (no padding).\n"
      },
      {
        "type": "file",
        "name": "11_OA_Problems.md",
        "content": "# \ud83d\udcbb PROBLEM 11: ONLINE ASSESSMENT PROBLEMS\n\n### \u2b50\u2b50\u2b50 **Common Screening Questions**\n\n**Frequency:** High (Karat / Hackerrank)\n\nThis section covers smaller, logic-heavy problems often seen in the initial screening (OA) round.\n\n---\n\n### 1\ufe0f\u20e3 **The MEX Problem (Minimum Excluded)**\n\n**Problem:**\n> Given an array of integers, find the **Minimum Excluded** positive integer (MEX).\n> That is, the smallest positive integer `> 0` that is NOT present in the array.\n\n**Example:**\n```text\n[1, 2, 3] -> 4\n[3, 2, 1] -> 4\n[4, 5, 6] -> 1 (Since 1 is missing)\n[0, 2, 3] -> 1 (0 is not positive/or ignored depending on spec)\n[-5, 1, 3] -> 2\n```\n\n**Approach:**\n1.  **Set approach (O(N)):** Put all numbers in a Set. Iterate 1, 2, 3... until not found.\n2.  **In-place Swap (O(N) Time, O(1) Space):**\n    - Try to put value `x` at index `x-1`. (Put 1 at index 0, 2 at index 1...).\n    - Ignore values $\\le 0$ or $> N$.\n    - After arranging, scan array. The first index `i` where `arr[i] != i+1` is the answer.\n\n```python\ndef first_missing_positive(nums):\n    n = len(nums)\n    for i in range(n):\n        # While nums[i] is in valid range [1, n] AND not in correct spot\n        while 1 <= nums[i] <= n and nums[nums[i]-1] != nums[i]:\n            # Swap to correct spot\n            correct_idx = nums[i] - 1\n            nums[i], nums[correct_idx] = nums[correct_idx], nums[i]\n            \n    for i in range(n):\n        if nums[i] != i + 1:\n            return i + 1\n            \n    return n + 1\n```\n\n---\n\n### 2\ufe0f\u20e3 **Perfect Break (Advertising/Breaks)**\n\n**Problem:**\n> You have a video of length `L` minutes.\n> You want to insert an ad break.\n> Users have watched intervals `[start, end]`.\n> Find a time `t` to insert the ad such that it does NOT interrupt any user's active session? Or minimizes interruptions?\n>\n> (Variation: Find the \"Perfect Break\" where 0 users are watching).\n\n**Approach:**\n> This is **Interval Merging** or **Line Sweep**.\n> If looking for 0 interruptions: Merge all user intervals. The gaps between merged intervals are the \"Perfect Breaks\".\n\n**Example:**\n```text\nUsers: [0, 5], [10, 15], [4, 8]\nMerged: [0, 8], [10, 15]\nGap: (8, 10). Any time t in 8 < t < 10 is perfect.\n```\n\n```python\ndef find_perfect_breaks(intervals, video_length):\n    if not intervals:\n        return [(0, video_length)]\n\n    # 1. Sort by start time\n    intervals.sort()\n    \n    # 2. Merge\n    merged = []\n    for start, end in intervals:\n        if not merged or start > merged[-1][1]:\n            merged.append([start, end])\n        else:\n            merged[-1][1] = max(merged[-1][1], end)\n            \n    # 3. Find Gaps\n    gaps = []\n    curr_time = 0\n    for start, end in merged:\n        if start > curr_time:\n            gaps.append((curr_time, start))\n        curr_time = max(curr_time, end)\n        \n    if curr_time < video_length:\n        gaps.append((curr_time, video_length))\n        \n    return gaps\n```\n\n---\n\n### \ud83e\uddea **Test Cases**\n\n**MEX:**\n- `[1, 2, 0]` -> 3.\n- `[]` -> 1.\n- `[7, 8, 9]` -> 1.\n\n**Breaks:**\n- No users: Full video is a gap.\n- Full coverage `[0, 100]`: No gap.\n- Disjoint `[0, 10], [20, 30]`: Gap `(10, 20)` and `(30, End)`.\n"
      }
    ]
  },
  {
    "type": "file",
    "name": "README.md",
    "content": "# \ud83d\ude80 ATLASSIAN INTERVIEW PREPARATION GUIDE\n\n**Comprehensive Collection of 372+ Real Interview Experiences**\n\n---\n\n## \ud83d\udcda Table of Contents\n\nThis repository contains detailed analysis of Atlassian interview questions across all rounds, compiled from 372 real interview experiences shared on LeetCode.\n\n### \ud83d\udcc1 Files Organization\n\n| File | Description | Questions |\n|------|-------------|-----------|\n| [01_Karat_Screening_Round.md](./01_Karat_Screening_Round.md) | Karat screening with System Design + DSA | 15+ SD + 10+ DSA |\n| [02_Data_Structures_Round.md](./02_Data_Structures_Round.md) | Pure DSA round - most repeated questions | 25+ problems |\n| [03_Code_Design_LLD_Round.md](./03_Code_Design_LLD_Round.md) | Low-Level Design / Machine Coding | 12+ problems |\n| [04_System_Design_HLD_Round.md](./04_System_Design_HLD_Round.md) | High-Level Design / Architecture | 10+ systems |\n| [05_Values_Behavioral_Round.md](./05_Values_Behavioral_Round.md) | Atlassian Values & STAR format | 50+ examples |\n| [06_Managerial_Round.md](./06_Managerial_Round.md) | Leadership & Project Management | 30+ questions |\n| [07_Preparation_Checklist.md](./07_Preparation_Checklist.md) | Study plan & timeline | Full roadmap |\n\n---\n\n## \ud83c\udfaf Interview Structure Overview\n\n**Total Rounds:** 6 rounds (for P40/P50 levels)\n\n### Round Breakdown\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Round 1: KARAT SCREENING (60 min)                          \u2502\n\u2502 \u251c\u2500 System Design Rapid Fire (20 min) - 5 questions         \u2502\n\u2502 \u2514\u2500 DSA Coding (40 min) - 1-2 medium problems               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Round 2: DATA STRUCTURES (60 min)                          \u2502\n\u2502 \u2514\u2500 1-2 DSA problems with follow-ups                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Round 3: CODE DESIGN / LLD (60 min)                        \u2502\n\u2502 \u2514\u2500 Object-oriented design + implementation                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Round 4: SYSTEM DESIGN / HLD (60 min)                      \u2502\n\u2502 \u2514\u2500 Design scalable systems (APIs, DB, Architecture)        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Round 5: MANAGERIAL (45-60 min)                            \u2502\n\u2502 \u2514\u2500 Leadership & project management questions               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Round 6: VALUES (45 min)                                    \u2502\n\u2502 \u2514\u2500 Behavioral questions on Atlassian's 5 core values       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udd25 Most Frequently Asked Questions\n\n### Top 5 DSA Questions (Repeated 50%+ times)\n\n1. **Employee Hierarchy / Closest Department** (60% of interviews)\n   - Find closest common parent group for employees\n   - LCA variation with n-ary trees\n   - File: [02_Data_Structures_Round.md](./02_Data_Structures_Round.md#employee-hierarchy)\n\n2. **Stock Price Fluctuation / Content Popularity** (40% of interviews)\n   - Track popularity with increase/decrease operations\n   - Return most popular item\n   - File: [02_Data_Structures_Round.md](./02_Data_Structures_Round.md#content-popularity)\n\n3. **Text Justification / Word Wrap** (35% of interviews)\n   - Wrap words with length constraints\n   - LeetCode 68 variation\n   - File: [01_Karat_Screening_Round.md](./01_Karat_Screening_Round.md#word-wrap)\n\n4. **Tennis Court Booking / Meeting Rooms** (30% of interviews)\n   - Interval scheduling with minimum resources\n   - LeetCode 253 variation\n   - File: [02_Data_Structures_Round.md](./02_Data_Structures_Round.md#tennis-court)\n\n5. **Dynamic Route Matching with Wildcards** (25% of interviews)\n   - Trie-based routing system\n   - File: [02_Data_Structures_Round.md](./02_Data_Structures_Round.md#route-matching)\n\n### Top 3 Code Design Questions\n\n1. **Snake Game** (50% of interviews) - [Details](./03_Code_Design_LLD_Round.md#snake-game)\n2. **Cost Explorer / Subscription Billing** (30%) - [Details](./03_Code_Design_LLD_Round.md#cost-explorer)\n3. **Agent Rating System** (25%) - [Details](./03_Code_Design_LLD_Round.md#rating-system)\n\n### Top 3 System Design Questions\n\n1. **Tagging Management System** (60% of interviews) - [Details](./04_System_Design_HLD_Round.md#tagging-system)\n2. **Web Scraping System** (20%) - [Details](./04_System_Design_HLD_Round.md#web-scraper)\n3. **Twitter Feed / Hashtag System** (15%) - [Details](./04_System_Design_HLD_Round.md#twitter-feed)\n\n---\n\n## \ud83d\udca1 Key Success Factors\n\n### \u2705 What Gets You Hired\n\n1. **Technical Rounds (40% weight)**\n   - Clean, modular code\n   - Optimal time/space complexity\n   - Edge case handling\n   - Clear communication\n\n2. **Design Rounds (30% weight)**\n   - API design first approach\n   - Scalability considerations\n   - Trade-off discussions\n   - Database schema justification\n\n3. **Behavioral Rounds (30% weight)** \u26a0\ufe0f **Often Underestimated!**\n   - STAR format answers\n   - Alignment with Atlassian values\n   - Leadership examples\n   - Cultural fit\n\n### \u274c Common Rejection Reasons\n\n1. **Technical Issues**\n   - Didn't ask clarifying questions\n   - Missed edge cases\n   - Incorrect time complexity analysis\n   - No unit tests mentioned\n\n2. **Code Design Issues**\n   - Messy, hard-to-understand code\n   - No exception handling\n   - Missing design patterns\n   - Poor modularity\n\n3. **Behavioral Issues** (Can reject even with all technical \"Hire\"!)\n   - Weak Atlassian values alignment\n   - Insufficient leadership examples\n   - Poor conflict resolution examples\n   - Lack of customer focus\n\n---\n\n## \ud83d\udcca Statistics from 372 Interviews\n\n### Success Rates by Round\n\n| Round | Pass Rate | Common Issues |\n|-------|-----------|---------------|\n| Karat Screening | 75% | Time management, incomplete DSA |\n| Data Structures | 60% | Employee hierarchy edge cases |\n| Code Design | 55% | Missing tests, no exception handling |\n| System Design | 65% | Incomplete API design |\n| Managerial | 70% | Weak examples |\n| Values | 60% | Poor value alignment |\n\n### Interview Timeline\n\n- **Karat to Final Decision:** 4-8 weeks\n- **Hiring Committee Decision:** 3-7 days after last round\n- **Team Matching:** 1-2 weeks after HC approval\n- **Offer Letter:** 3-5 days after team match\n\n---\n\n## \ud83c\udf93 Preparation Timeline\n\n### Minimum Preparation: 4-6 Weeks\n\n#### Week 1-2: DSA Focus\n- [ ] Master Employee Hierarchy (LCA)\n- [ ] Practice Stock Price Fluctuation\n- [ ] Complete 10 Medium LeetCode problems\n- [ ] Focus on HashMap, TreeMap, Heap patterns\n\n#### Week 3: Code Design\n- [ ] Implement Snake Game 3 times\n- [ ] Practice Cost Explorer\n- [ ] Learn design patterns (Strategy, Factory, Observer)\n- [ ] Write unit tests for all solutions\n\n#### Week 4: System Design\n- [ ] Design Tagging System\n- [ ] Study database sharding and indexing\n- [ ] Practice API design\n- [ ] Review scalability patterns\n\n#### Week 5: Behavioral Prep\n- [ ] Prepare 10 STAR stories\n- [ ] Map stories to Atlassian values\n- [ ] Practice leadership examples\n- [ ] Mock behavioral interviews\n\n#### Week 6: Mock Interviews\n- [ ] 2 full DSA mocks\n- [ ] 1 code design mock\n- [ ] 1 system design mock\n- [ ] 1 behavioral mock\n\n---\n\n## \ud83d\udd17 Additional Resources\n\n### LeetCode Problem Lists\n- [Atlassian Tagged Problems](https://leetcode.com/company/atlassian/)\n- [Practice by Pattern](./07_Preparation_Checklist.md#leetcode-patterns)\n\n### Official Resources\n- [Atlassian Values Guide](https://www.atlassian.com/company/values)\n- [Atlassian Engineering Blog](https://www.atlassian.com/engineering)\n\n### System Design Resources\n- Grokking the System Design Interview\n- System Design Primer (GitHub)\n- ByteByteGo (Alex Xu)\n\n---\n\n## \ud83d\udcc8 Compensation Ranges (India - 2024/2025)\n\n### P40 (SDE-2) - 4-6 YOE\n- **Base:** \u20b942-50L\n- **Bonus:** 15% (\u20b96.5-7.5L)\n- **RSU:** $70-100K over 4 years (\u20b914-21L/year)\n- **Sign-on:** \u20b94-6L\n- **Total Year 1:** \u20b968-85L\n\n### P50 (Senior SDE) - 7-10 YOE\n- **Base:** \u20b960-70L\n- **Bonus:** 15-20% (\u20b99-14L)\n- **RSU:** $100-130K over 4 years (\u20b921-27L/year)\n- **Sign-on:** \u20b95-10L\n- **Total Year 1:** \u20b995-120L\n\n### P60 (Principal) - 10+ YOE\n- **Base:** \u20b91.2Cr+\n- **Bonus:** 20-25%\n- **RSU:** Significant\n- **Total:** \u20b92Cr+\n\n---\n\n## \ud83e\udd1d Contributing\n\nFound a new question or want to share your experience? Feel free to contribute!\n\n---\n\n## \u26a0\ufe0f Important Notes\n\n1. **Atlassian Values Round is CRITICAL** - Many candidates get rejected here despite technical excellence\n2. **Time Management in Karat** - Practice rapid-fire system design questions\n3. **Code Design Needs Tests** - Always mention unit testing even if you don't code them\n4. **System Design: APIs First** - Start with API design, then database schema\n5. **Hiring Committee Can Reject** - Even with all positive feedbacks\n\n---\n\n## \ud83d\udcde Contact & Feedback\n\nThis guide is compiled from public LeetCode discussions and interview experiences shared by the community.\n\n**Last Updated:** January 2025\n**Total Experiences Analyzed:** 372\n**Data Source:** LeetCode Discussion Forums\n\n---\n\n## \ud83c\udf1f Good Luck!\n\nRemember:\n- **Practice consistently** - Quality over quantity\n- **Understand, don't memorize** - Learn patterns, not solutions\n- **Mock interviews** - Simulate real pressure\n- **Behavioral prep matters** - Don't skip Values/Managerial prep!\n\n**You got this! \ud83d\ude80**\n"
  }
];